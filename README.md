# Llava-llama
# LLaVA 1.5: Training on 150k Dataset

## Overview
This repository explores the LLaVA 1.5 model, a state-of-the-art architecture designed for various natural language processing tasks. This project involves training LLaVA 1.5 on a dataset of 150,000 samples to evaluate its performance and capabilities.

## How LLaVA 1.5 Works
LLaVA 1.5 builds upon its predecessors by enhancing its architecture for improved understanding and generation of natural language. Key features include:
- **Transformer Architecture**: Utilizes a modified transformer model for better handling of contextual information.
- **Attention Mechanisms**: Improved attention layers allow the model to focus on relevant parts of the input data more effectively.
- **Fine-tuning Capabilities**: Pre-trained on diverse datasets, LLaVA 1.5 can be fine-tuned on specific tasks, allowing for tailored performance improvements.

training_script https://github.com/11kartheek/Llava-qwen/blob/main/final_training.ipynb
inference_script 
app_inference_script https://github.com/11kartheek/Llava-qwen/blob/main/app_inference.ipynb

## Dataset
- **Size**: 150,000 samples
- **Format**: [JSON]
- **Source**: [Link to the dataset or describe how to obtain it.](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)
- **Preprocessing**: [to be added]

## Base Model
The base model used for LLaVA 1.5 can be found here: [Link to Base Model on Hugging Face](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)

## Hugging Face Space
Explore the model in action and its capabilities on Hugging Face Spaces: [LLaVA 1.5 Space](https://huggingface.co/spaces/Kartheekb7/llava_chat)

