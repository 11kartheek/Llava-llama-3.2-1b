{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442},{"sourceId":9732867,"sourceType":"datasetVersion","datasetId":5945412},{"sourceId":9734757,"sourceType":"datasetVersion","datasetId":5957788}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T04:09:53.443831Z","iopub.execute_input":"2024-10-28T04:09:53.444602Z","iopub.status.idle":"2024-10-28T04:09:53.457903Z","shell.execute_reply.started":"2024-10-28T04:09:53.444560Z","shell.execute_reply":"2024-10-28T04:09:53.457029Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl\n/kaggle/input/sample-cap/sample.csv\n/kaggle/input/sample-cap/turns_50_sample.csv\n/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl\n/kaggle/input/sub-capstone/turns_60k_sample.csv\n/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip -q install -U bitsandbytes peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install --upgrade huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your .pkl file\nfile_path = '/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl'\n\n# Load the embeddings from the .pkl file\nwith open(file_path, 'rb') as file:\n    embeddings = pickle.load(file)\n\n\n# import pickle\n\n# # Specify the path to your .pkl file\n# file_path = '/kaggle/input/llava-processed/final_clip_embeddings_part_1.pkl'\n\n# # Load the embeddings from the .pkl file\n# with open(file_path, 'rb') as file:\n#     embeddings_1 = pickle.load(file)\nlen(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:07:41.152423Z","iopub.execute_input":"2024-10-28T04:07:41.153104Z","iopub.status.idle":"2024-10-28T04:08:45.086941Z","shell.execute_reply.started":"2024-10-28T04:07:41.153036Z","shell.execute_reply":"2024-10-28T04:08:45.085970Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"30946"},"metadata":{}}]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:08:45.096041Z","iopub.execute_input":"2024-10-28T04:08:45.096414Z","iopub.status.idle":"2024-10-28T04:08:45.108370Z","shell.execute_reply.started":"2024-10-28T04:08:45.096381Z","shell.execute_reply":"2024-10-28T04:08:45.107505Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:08:45.110323Z","iopub.execute_input":"2024-10-28T04:08:45.110643Z","iopub.status.idle":"2024-10-28T04:08:45.176314Z","shell.execute_reply.started":"2024-10-28T04:08:45.110611Z","shell.execute_reply":"2024-10-28T04:08:45.175267Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# pip install -U bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n)\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bnb_config,\n    torch_dtype = torch.float32,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:08:45.177559Z","iopub.execute_input":"2024-10-28T04:08:45.177891Z","iopub.status.idle":"2024-10-28T04:08:51.936725Z","shell.execute_reply.started":"2024-10-28T04:08:45.177847Z","shell.execute_reply":"2024-10-28T04:08:51.935931Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n        self.config = phi2_model.config\n\n    def forward(self, image_embeddings=None,\n                inputs_embeds=None,\n                input_ids=None,\n                attention_mask=None,\n                labels=None,\n                output_attentions=False, \n        output_hidden_states=False, \n        **kwargs):  # Catch any additional arguments):\n        \n        if input_ids is not None:\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        elif inputs_embeds is not None:\n            token_embeddings = inputs_embeds\n        else:\n            raise ValueError(\"You must provide either input_ids or inputs_embeds.\")\n\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings).to(device = token_embeddings.device)\n            \n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            \n            # Combine image and token embeddings\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            \n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), 1, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([label_padding,labels], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels, output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            **kwargs)\n\n        return outputs\n    \n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        # Generate inputs with projections where necessary\n        if image_embeddings is not None:\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)\n            new_attention_mask = torch.cat([torch.ones_like(projected_image_embeddings[..., :1]), attention_mask], dim=1)\n        else:\n            combined_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            new_attention_mask = attention_mask\n\n        return {\n            \"inputs_embeds\": combined_embeddings,\n            \"attention_mask\": new_attention_mask,\n            **kwargs\n        }\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2).to(device)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n\n\nfrom peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\n\nphi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:09:04.729783Z","iopub.execute_input":"2024-10-28T04:09:04.730305Z","iopub.status.idle":"2024-10-28T04:09:05.010664Z","shell.execute_reply.started":"2024-10-28T04:09:04.730268Z","shell.execute_reply":"2024-10-28T04:09:05.009843Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True\n\nphi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:09:06.315659Z","iopub.execute_input":"2024-10-28T04:09:06.316369Z","iopub.status.idle":"2024-10-28T04:09:06.328562Z","shell.execute_reply.started":"2024-10-28T04:09:06.316329Z","shell.execute_reply":"2024-10-28T04:09:06.327620Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable params: 4,587,520 || all params: 1,240,401,920 || trainable%: 0.3698\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import CLIPModel\n\n# # Load CLIP and PHI2\n# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sub-capstone/turns_60k_sample.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:10:22.466108Z","iopub.execute_input":"2024-10-28T04:10:22.466544Z","iopub.status.idle":"2024-10-28T04:10:23.453208Z","shell.execute_reply.started":"2024-10-28T04:10:22.466478Z","shell.execute_reply":"2024-10-28T04:10:23.452354Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256 - 49\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image_embedding = embeddings[self.image_paths[idx]]\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n#         label_encoding = tokenizer(\n#             self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n#             return_tensors='pt',\n#             padding='max_length',\n#             truncation=True,\n#             max_length=self.max_length\n#         )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = input_ids.clone()\n#         label_ids = label_encoding['input_ids'].squeeze(0)\n\n#         label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n#             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:10:46.879110Z","iopub.execute_input":"2024-10-28T04:10:46.879483Z","iopub.status.idle":"2024-10-28T04:10:47.546311Z","shell.execute_reply.started":"2024-10-28T04:10:46.879450Z","shell.execute_reply":"2024-10-28T04:10:47.545216Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:10:49.613615Z","iopub.execute_input":"2024-10-28T04:10:49.613991Z","iopub.status.idle":"2024-10-28T04:10:50.873799Z","shell.execute_reply.started":"2024-10-28T04:10:49.613957Z","shell.execute_reply":"2024-10-28T04:10:50.872900Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79c149dd4760>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:10:54.600948Z","iopub.execute_input":"2024-10-28T04:10:54.601719Z","iopub.status.idle":"2024-10-28T04:10:54.609864Z","shell.execute_reply.started":"2024-10-28T04:10:54.601679Z","shell.execute_reply":"2024-10-28T04:10:54.608960Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 1000\nlogging_steps = 10\nlearning_rate = 1e-4\nmax_grad_norm = 0.3\nwarmup_ratio = 0.03\nlr_scheduler_type = \"cosine\"\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=1,\n    max_steps = 11000,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    lr_scheduler_type=lr_scheduler_type,\n    fp16=False,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T04:10:59.219740Z","iopub.execute_input":"2024-10-28T04:10:59.220421Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6274' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6274/15000 6:34:28 < 9:08:48, 0.26 it/s, Epoch 0.36/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.965000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.463600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.318200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.169700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.055500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.079000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.027600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.985500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.996200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.908000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.919300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.950700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.893200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.926600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.918500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.822200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.904800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.916900</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.762600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.821600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.833500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.841100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.811300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.911700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.825400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.782600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.804900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.826400</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.908900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.839800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.846400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.838700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.828200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.805500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.711600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.840200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.784700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.837500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.904500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.752400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.797500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.815500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.852200</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.806200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.857700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.753900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.844700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.827600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.777600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.719200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.821900</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.799400</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.758100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.739100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.890700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.713300</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.819200</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.820100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.711900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.789000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.867600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.787200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.848600</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.701300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.765800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.861800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.791800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.783600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.878900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.812800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.795900</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.703300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.789100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.766500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.764600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.861500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.777000</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.633900</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.724900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.788000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.707200</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.848400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.720600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.812800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.802800</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.754400</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.787300</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.763300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.775700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.723600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.837700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.684500</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.721600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.714100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.831600</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.791500</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.728800</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.694400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.790700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.785200</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.724800</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.884700</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.771500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.830500</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.848800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.859200</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.686400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.736200</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.731500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.713100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.779800</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.846200</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.717400</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.701100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.778300</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.740200</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.712600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.632300</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.713200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.745700</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.707300</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.835500</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.737100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.824800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.714700</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.791300</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.780500</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.728900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.793300</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.807100</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.678100</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.771700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.792000</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.638500</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.679900</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.798900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.789900</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.700300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.755100</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.798100</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.751800</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.747600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.763900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.826300</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.736000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.683100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.724700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.810000</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.719700</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.721600</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.764600</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.653100</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.704100</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.659300</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.679000</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.824400</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.689900</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.770200</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.693700</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.778000</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.760600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.721700</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.681600</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.712900</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.755200</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.735900</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.738800</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.759800</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.746500</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.684300</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.696800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.723400</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.780000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.786200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.697500</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.677000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.743600</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.693800</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.729400</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.765600</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.703900</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.743000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.813800</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.686100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.716900</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.720800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.776100</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.689500</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.728400</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.755200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.803500</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.717900</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.667000</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.724800</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.618800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.676300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.698500</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.809600</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.696900</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.739400</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.794600</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.760800</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.726700</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.710100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.761000</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.687400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.744400</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.713700</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.762700</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.727000</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.696600</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.782500</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.717800</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.813700</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.752300</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.551600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.717700</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.746700</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.755600</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.776300</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.684600</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.679100</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.710200</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.708000</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.717600</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.610800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.666000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.743500</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.664900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.756000</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.740700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.666200</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.677000</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.730200</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.638300</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.683000</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.839400</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.696300</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.719400</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.688600</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.628100</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.667200</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.609800</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.761200</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.746600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.706400</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.712700</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.709200</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.625600</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.669700</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.755400</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.640200</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.779200</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.793800</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.670300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.678700</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.768500</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.694000</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.734500</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.788600</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.680700</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.641700</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.725500</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.753900</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.735600</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.781200</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.752000</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.757000</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.699900</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.636600</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.806500</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.612800</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.748800</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.700100</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.629500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.689100</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.691300</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.607100</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.671100</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.638900</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.660700</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.708500</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.752900</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.783800</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.782600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.684900</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.742400</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.662500</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.781200</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.726200</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.730700</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.743400</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.732200</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.673400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.746800</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.749300</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.664900</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.596900</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.593800</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.743500</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.587300</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.734200</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.642600</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.789300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.697500</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.584400</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.677000</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.643900</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.795000</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.699600</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.668400</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.730400</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.683600</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.664900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.731000</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.701700</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.668900</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.647600</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.762700</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.719500</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.681800</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.657200</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.611800</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.733800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.680700</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.642900</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.632700</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.639600</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.672900</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.623900</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.696800</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.566800</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.676300</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.743300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.664800</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.673300</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.690600</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.716700</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.736900</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.763600</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.711500</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.669900</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.736000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.719100</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.725700</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.647500</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.705700</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.599900</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.628200</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.789400</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.709800</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.678700</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.813200</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.710400</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.787500</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.772900</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.728200</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.745100</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.761300</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.642800</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.788400</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.646400</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.685000</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.681200</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.696100</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.708800</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.701100</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.791300</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.684400</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.708100</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.693600</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.605600</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.674600</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.606200</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.752200</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.654900</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.811400</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.705900</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.726300</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.682100</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.717300</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.734000</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.683000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.693000</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.634000</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.693400</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.698400</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.715500</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.625700</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.622000</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.642500</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.640300</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.726900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.749200</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.797700</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.644500</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.715300</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.610700</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.711300</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.682000</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.646000</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.712900</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.687000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.730200</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.634000</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.695000</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.737400</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.741900</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.687300</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.739700</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.711200</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.708200</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.707400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.652400</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.611000</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>0.732800</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>0.690200</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>0.674700</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.666500</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>0.662800</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>0.646000</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>0.644100</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>0.683900</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.668600</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>0.631700</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>0.638100</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>0.713900</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>0.739400</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.727100</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>0.743800</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>0.712500</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>0.715900</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.754200</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>0.642200</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>0.802300</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>0.735400</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>0.632800</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.718400</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>0.634200</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>0.668700</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>0.609000</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>0.696700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.664300</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>0.754600</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>0.717600</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>0.756300</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>0.726700</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.762000</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>0.718800</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>0.760400</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>0.744200</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>0.748900</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.647500</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>0.634500</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>0.686600</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>0.619000</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>0.678000</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.663200</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>0.649300</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>0.690200</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>0.670200</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>0.742500</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.660300</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>0.764100</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>0.670700</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>0.680900</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>0.563600</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.679800</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>0.608700</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>0.714000</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>0.625400</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>0.699700</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.666300</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>0.698700</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>0.715100</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>0.622900</td>\n    </tr>\n    <tr>\n      <td>4840</td>\n      <td>0.655900</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.765900</td>\n    </tr>\n    <tr>\n      <td>4860</td>\n      <td>0.726600</td>\n    </tr>\n    <tr>\n      <td>4870</td>\n      <td>0.663400</td>\n    </tr>\n    <tr>\n      <td>4880</td>\n      <td>0.801800</td>\n    </tr>\n    <tr>\n      <td>4890</td>\n      <td>0.686700</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.634700</td>\n    </tr>\n    <tr>\n      <td>4910</td>\n      <td>0.694400</td>\n    </tr>\n    <tr>\n      <td>4920</td>\n      <td>0.675400</td>\n    </tr>\n    <tr>\n      <td>4930</td>\n      <td>0.688900</td>\n    </tr>\n    <tr>\n      <td>4940</td>\n      <td>0.744700</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.614800</td>\n    </tr>\n    <tr>\n      <td>4960</td>\n      <td>0.622700</td>\n    </tr>\n    <tr>\n      <td>4970</td>\n      <td>0.702700</td>\n    </tr>\n    <tr>\n      <td>4980</td>\n      <td>0.689800</td>\n    </tr>\n    <tr>\n      <td>4990</td>\n      <td>0.720600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.694800</td>\n    </tr>\n    <tr>\n      <td>5010</td>\n      <td>0.630800</td>\n    </tr>\n    <tr>\n      <td>5020</td>\n      <td>0.632300</td>\n    </tr>\n    <tr>\n      <td>5030</td>\n      <td>0.728200</td>\n    </tr>\n    <tr>\n      <td>5040</td>\n      <td>0.618000</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.686600</td>\n    </tr>\n    <tr>\n      <td>5060</td>\n      <td>0.630600</td>\n    </tr>\n    <tr>\n      <td>5070</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>5080</td>\n      <td>0.733300</td>\n    </tr>\n    <tr>\n      <td>5090</td>\n      <td>0.690900</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.721100</td>\n    </tr>\n    <tr>\n      <td>5110</td>\n      <td>0.777800</td>\n    </tr>\n    <tr>\n      <td>5120</td>\n      <td>0.686500</td>\n    </tr>\n    <tr>\n      <td>5130</td>\n      <td>0.630900</td>\n    </tr>\n    <tr>\n      <td>5140</td>\n      <td>0.669000</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.699700</td>\n    </tr>\n    <tr>\n      <td>5160</td>\n      <td>0.645700</td>\n    </tr>\n    <tr>\n      <td>5170</td>\n      <td>0.671900</td>\n    </tr>\n    <tr>\n      <td>5180</td>\n      <td>0.621300</td>\n    </tr>\n    <tr>\n      <td>5190</td>\n      <td>0.770700</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.659100</td>\n    </tr>\n    <tr>\n      <td>5210</td>\n      <td>0.681900</td>\n    </tr>\n    <tr>\n      <td>5220</td>\n      <td>0.704200</td>\n    </tr>\n    <tr>\n      <td>5230</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>5240</td>\n      <td>0.696800</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.594600</td>\n    </tr>\n    <tr>\n      <td>5260</td>\n      <td>0.656700</td>\n    </tr>\n    <tr>\n      <td>5270</td>\n      <td>0.639500</td>\n    </tr>\n    <tr>\n      <td>5280</td>\n      <td>0.685700</td>\n    </tr>\n    <tr>\n      <td>5290</td>\n      <td>0.625600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.719700</td>\n    </tr>\n    <tr>\n      <td>5310</td>\n      <td>0.772400</td>\n    </tr>\n    <tr>\n      <td>5320</td>\n      <td>0.689600</td>\n    </tr>\n    <tr>\n      <td>5330</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>5340</td>\n      <td>0.636400</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.683500</td>\n    </tr>\n    <tr>\n      <td>5360</td>\n      <td>0.701200</td>\n    </tr>\n    <tr>\n      <td>5370</td>\n      <td>0.663200</td>\n    </tr>\n    <tr>\n      <td>5380</td>\n      <td>0.759700</td>\n    </tr>\n    <tr>\n      <td>5390</td>\n      <td>0.691300</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.650900</td>\n    </tr>\n    <tr>\n      <td>5410</td>\n      <td>0.722700</td>\n    </tr>\n    <tr>\n      <td>5420</td>\n      <td>0.719100</td>\n    </tr>\n    <tr>\n      <td>5430</td>\n      <td>0.688500</td>\n    </tr>\n    <tr>\n      <td>5440</td>\n      <td>0.806300</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.690100</td>\n    </tr>\n    <tr>\n      <td>5460</td>\n      <td>0.719600</td>\n    </tr>\n    <tr>\n      <td>5470</td>\n      <td>0.767500</td>\n    </tr>\n    <tr>\n      <td>5480</td>\n      <td>0.590000</td>\n    </tr>\n    <tr>\n      <td>5490</td>\n      <td>0.681000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.677700</td>\n    </tr>\n    <tr>\n      <td>5510</td>\n      <td>0.738100</td>\n    </tr>\n    <tr>\n      <td>5520</td>\n      <td>0.694400</td>\n    </tr>\n    <tr>\n      <td>5530</td>\n      <td>0.639800</td>\n    </tr>\n    <tr>\n      <td>5540</td>\n      <td>0.729700</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.785700</td>\n    </tr>\n    <tr>\n      <td>5560</td>\n      <td>0.691500</td>\n    </tr>\n    <tr>\n      <td>5570</td>\n      <td>0.748400</td>\n    </tr>\n    <tr>\n      <td>5580</td>\n      <td>0.745100</td>\n    </tr>\n    <tr>\n      <td>5590</td>\n      <td>0.624900</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.664900</td>\n    </tr>\n    <tr>\n      <td>5610</td>\n      <td>0.674700</td>\n    </tr>\n    <tr>\n      <td>5620</td>\n      <td>0.646000</td>\n    </tr>\n    <tr>\n      <td>5630</td>\n      <td>0.651900</td>\n    </tr>\n    <tr>\n      <td>5640</td>\n      <td>0.642600</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.706900</td>\n    </tr>\n    <tr>\n      <td>5660</td>\n      <td>0.678100</td>\n    </tr>\n    <tr>\n      <td>5670</td>\n      <td>0.755100</td>\n    </tr>\n    <tr>\n      <td>5680</td>\n      <td>0.665700</td>\n    </tr>\n    <tr>\n      <td>5690</td>\n      <td>0.736300</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.686200</td>\n    </tr>\n    <tr>\n      <td>5710</td>\n      <td>0.688200</td>\n    </tr>\n    <tr>\n      <td>5720</td>\n      <td>0.607500</td>\n    </tr>\n    <tr>\n      <td>5730</td>\n      <td>0.684600</td>\n    </tr>\n    <tr>\n      <td>5740</td>\n      <td>0.643600</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.751300</td>\n    </tr>\n    <tr>\n      <td>5760</td>\n      <td>0.728100</td>\n    </tr>\n    <tr>\n      <td>5770</td>\n      <td>0.664600</td>\n    </tr>\n    <tr>\n      <td>5780</td>\n      <td>0.690300</td>\n    </tr>\n    <tr>\n      <td>5790</td>\n      <td>0.720200</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.722700</td>\n    </tr>\n    <tr>\n      <td>5810</td>\n      <td>0.608400</td>\n    </tr>\n    <tr>\n      <td>5820</td>\n      <td>0.743600</td>\n    </tr>\n    <tr>\n      <td>5830</td>\n      <td>0.636800</td>\n    </tr>\n    <tr>\n      <td>5840</td>\n      <td>0.772900</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.689800</td>\n    </tr>\n    <tr>\n      <td>5860</td>\n      <td>0.755900</td>\n    </tr>\n    <tr>\n      <td>5870</td>\n      <td>0.780100</td>\n    </tr>\n    <tr>\n      <td>5880</td>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <td>5890</td>\n      <td>0.604900</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.666900</td>\n    </tr>\n    <tr>\n      <td>5910</td>\n      <td>0.639500</td>\n    </tr>\n    <tr>\n      <td>5920</td>\n      <td>0.730100</td>\n    </tr>\n    <tr>\n      <td>5930</td>\n      <td>0.714400</td>\n    </tr>\n    <tr>\n      <td>5940</td>\n      <td>0.622800</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.627200</td>\n    </tr>\n    <tr>\n      <td>5960</td>\n      <td>0.605900</td>\n    </tr>\n    <tr>\n      <td>5970</td>\n      <td>0.703100</td>\n    </tr>\n    <tr>\n      <td>5980</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>5990</td>\n      <td>0.710400</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.728800</td>\n    </tr>\n    <tr>\n      <td>6010</td>\n      <td>0.682900</td>\n    </tr>\n    <tr>\n      <td>6020</td>\n      <td>0.653000</td>\n    </tr>\n    <tr>\n      <td>6030</td>\n      <td>0.717300</td>\n    </tr>\n    <tr>\n      <td>6040</td>\n      <td>0.643100</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>6060</td>\n      <td>0.756900</td>\n    </tr>\n    <tr>\n      <td>6070</td>\n      <td>0.616800</td>\n    </tr>\n    <tr>\n      <td>6080</td>\n      <td>0.677800</td>\n    </tr>\n    <tr>\n      <td>6090</td>\n      <td>0.582500</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.637300</td>\n    </tr>\n    <tr>\n      <td>6110</td>\n      <td>0.688600</td>\n    </tr>\n    <tr>\n      <td>6120</td>\n      <td>0.681500</td>\n    </tr>\n    <tr>\n      <td>6130</td>\n      <td>0.620500</td>\n    </tr>\n    <tr>\n      <td>6140</td>\n      <td>0.738600</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.686400</td>\n    </tr>\n    <tr>\n      <td>6160</td>\n      <td>0.678200</td>\n    </tr>\n    <tr>\n      <td>6170</td>\n      <td>0.721300</td>\n    </tr>\n    <tr>\n      <td>6180</td>\n      <td>0.593700</td>\n    </tr>\n    <tr>\n      <td>6190</td>\n      <td>0.638000</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.637600</td>\n    </tr>\n    <tr>\n      <td>6210</td>\n      <td>0.687800</td>\n    </tr>\n    <tr>\n      <td>6220</td>\n      <td>0.702600</td>\n    </tr>\n    <tr>\n      <td>6230</td>\n      <td>0.679600</td>\n    </tr>\n    <tr>\n      <td>6240</td>\n      <td>0.706100</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.749300</td>\n    </tr>\n    <tr>\n      <td>6260</td>\n      <td>0.659500</td>\n    </tr>\n    <tr>\n      <td>6270</td>\n      <td>0.707600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"image_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path,text_input,text_label # Example text label (if needed for comparison)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_path = image_paths[0]\ntext_input = text_inputs[0]  # Example text input\ntext_label = text_labels[0]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimage_embedding = embeddings[image_path]\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=None)  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\n# print(f\"Predicted labels: {predicted_labels}\")\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_labels[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 256\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_args,\n    data_collator=data_collator\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}