{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442},{"sourceId":9719446,"sourceType":"datasetVersion","datasetId":5945412}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T11:12:09.085123Z","iopub.execute_input":"2024-10-25T11:12:09.085551Z","iopub.status.idle":"2024-10-25T11:12:09.587098Z","shell.execute_reply.started":"2024-10-25T11:12:09.085496Z","shell.execute_reply":"2024-10-25T11:12:09.585826Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl\n/kaggle/input/sample-cap/sample.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:12:09.588482Z","iopub.execute_input":"2024-10-25T11:12:09.589091Z","iopub.status.idle":"2024-10-25T11:12:24.148139Z","shell.execute_reply.started":"2024-10-25T11:12:09.589040Z","shell.execute_reply":"2024-10-25T11:12:24.146685Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:12:24.150176Z","iopub.execute_input":"2024-10-25T11:12:24.150729Z","iopub.status.idle":"2024-10-25T11:12:39.714994Z","shell.execute_reply.started":"2024-10-25T11:12:24.150605Z","shell.execute_reply":"2024-10-25T11:12:39.713678Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:12:39.716685Z","iopub.execute_input":"2024-10-25T11:12:39.717119Z","iopub.status.idle":"2024-10-25T11:12:39.724736Z","shell.execute_reply.started":"2024-10-25T11:12:39.717074Z","shell.execute_reply":"2024-10-25T11:12:39.723527Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:12:39.727702Z","iopub.execute_input":"2024-10-25T11:12:39.728109Z","iopub.status.idle":"2024-10-25T11:12:40.088662Z","shell.execute_reply.started":"2024-10-25T11:12:39.728067Z","shell.execute_reply":"2024-10-25T11:12:40.087347Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdcd0e4a2964c5b949954a76c2d65bf"}},"metadata":{}}]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:12:53.798506Z","iopub.execute_input":"2024-10-25T11:12:53.798983Z","iopub.status.idle":"2024-10-25T11:14:02.296032Z","shell.execute_reply.started":"2024-10-25T11:12:53.798939Z","shell.execute_reply":"2024-10-25T11:14:02.294340Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b880feec396a45b6951c057dc93a0a4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60886422c77d426684e9f7150b265ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8418f346d7e428ea83cf63069aab57e"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.298533Z","iopub.execute_input":"2024-10-25T11:14:02.299149Z","iopub.status.idle":"2024-10-25T11:14:02.310743Z","shell.execute_reply.started":"2024-10-25T11:14:02.299103Z","shell.execute_reply":"2024-10-25T11:14:02.309541Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n\n    def forward(self, image_embeddings=None, input_ids=None, attention_mask=None, labels=None):\n       \n        # Get token embeddings from PHI2 model\n        token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n\n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            \n            # Combine image and token embeddings\n            if random.random() < 0.5:\n                combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            else:\n                combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=1)\n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), -100, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([labels, label_padding], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels)\n\n        return outputs\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.312623Z","iopub.execute_input":"2024-10-25T11:14:02.313266Z","iopub.status.idle":"2024-10-25T11:14:02.377926Z","shell.execute_reply.started":"2024-10-25T11:14:02.313219Z","shell.execute_reply":"2024-10-25T11:14:02.376528Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\"\n)\n\n# Wrap PHI 2 with QLoRA\n# phi_lora_model = LoraModel(model, lora_config,\"default\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.380846Z","iopub.execute_input":"2024-10-25T11:14:02.381252Z","iopub.status.idle":"2024-10-25T11:14:02.571223Z","shell.execute_reply.started":"2024-10-25T11:14:02.381212Z","shell.execute_reply":"2024-10-25T11:14:02.569894Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"phi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.573006Z","iopub.execute_input":"2024-10-25T11:14:02.573481Z","iopub.status.idle":"2024-10-25T11:14:02.745767Z","shell.execute_reply.started":"2024-10-25T11:14:02.573419Z","shell.execute_reply":"2024-10-25T11:14:02.743853Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.747942Z","iopub.execute_input":"2024-10-25T11:14:02.748535Z","iopub.status.idle":"2024-10-25T11:14:02.766847Z","shell.execute_reply.started":"2024-10-25T11:14:02.748470Z","shell.execute_reply":"2024-10-25T11:14:02.765265Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.768671Z","iopub.execute_input":"2024-10-25T11:14:02.769186Z","iopub.status.idle":"2024-10-25T11:14:02.783250Z","shell.execute_reply.started":"2024-10-25T11:14:02.769128Z","shell.execute_reply":"2024-10-25T11:14:02.781796Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 1,240,401,920 || trainable%: 0.1374\n","output_type":"stream"}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.784753Z","iopub.execute_input":"2024-10-25T11:14:02.785145Z","iopub.status.idle":"2024-10-25T11:14:02.801507Z","shell.execute_reply.started":"2024-10-25T11:14:02.785105Z","shell.execute_reply":"2024-10-25T11:14:02.800199Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 1,240,401,920 || trainable%: 0.1374\n","output_type":"stream"}]},{"cell_type":"markdown","source":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n\ntrainable params: 2,179,072 || all params: 1,547,205,888 || trainable%: 0.1408\n\n\r\n","metadata":{}},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.803210Z","iopub.execute_input":"2024-10-25T11:14:02.803776Z","iopub.status.idle":"2024-10-25T11:14:02.822860Z","shell.execute_reply.started":"2024-10-25T11:14:02.803719Z","shell.execute_reply":"2024-10-25T11:14:02.821335Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.827581Z","iopub.execute_input":"2024-10-25T11:14:02.828037Z","iopub.status.idle":"2024-10-25T11:14:02.840334Z","shell.execute_reply.started":"2024-10-25T11:14:02.827975Z","shell.execute_reply":"2024-10-25T11:14:02.839121Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"trainable params: 4,587,520 || all params: 1,240,401,920 || trainable%: 0.3698\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import CLIPModel\n\n# Load CLIP and PHI2\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:02.841727Z","iopub.execute_input":"2024-10-25T11:14:02.842130Z","iopub.status.idle":"2024-10-25T11:14:06.731933Z","shell.execute_reply.started":"2024-10-25T11:14:02.842067Z","shell.execute_reply":"2024-10-25T11:14:06.730866Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c547bca1f0c42349990ee39dc0f1979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23da2c2f4d1d40ddb4bf76739d44bfe0"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Now the model can be trained, and the optimizer only updates LoRA and projection\noptimizer = torch.optim.AdamW(\n    [p for p in combined_model.parameters() if p.requires_grad], lr=1e-4\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:06.733650Z","iopub.execute_input":"2024-10-25T11:14:06.734068Z","iopub.status.idle":"2024-10-25T11:14:07.565681Z","shell.execute_reply.started":"2024-10-25T11:14:06.734025Z","shell.execute_reply":"2024-10-25T11:14:07.564260Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n# # Training loop\n# def train_model(combined_model, data_loader, optimizer, num_epochs=1, device=\"cuda\"):\n#     combined_model.train()\n#     combined_model = combined_model.to(device)\n    \n#     for epoch in range(num_epochs):\n#         total_loss = 0\n#         for batch in data_loader:\n#             image_embeddings = batch['image_embeddings'].to(device)\n#             input_ids = batch['input_ids'].to(device)\n#             labels = batch['labels'].to(device)\n            \n#             # Forward pass\n#             optimizer.zero_grad()\n#             outputs = combined_model(image_embeddings, input_ids)\n            \n#             # Assume outputs is a tuple where the first element is logits\n#             logits = outputs.logits\n            \n#             # Flatten the logits and labels for cross-entropy loss\n#             logits = logits.view(-1, logits.size(-1))\n#             labels = labels.view(-1)\n            \n#             # Calculate loss (cross-entropy loss for language modeling)\n#             loss = F.cross_entropy(logits, labels)\n#             total_loss += loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             optimizer.step()\n        \n#         avg_loss = total_loss / len(data_loader)\n#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# # Usage\n# data_loader = get_data_loader(batch_size=16)  # Adjust the batch size as needed\n# train_model(combined_model, data_loader, optimizer, num_epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:07.567251Z","iopub.execute_input":"2024-10-25T11:14:07.567886Z","iopub.status.idle":"2024-10-25T11:14:07.574960Z","shell.execute_reply.started":"2024-10-25T11:14:07.567842Z","shell.execute_reply":"2024-10-25T11:14:07.573500Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# for batch in data_loader:\n#     image_embeddings = batch['image_embeddings'].to(device)  # Assuming pre-extracted embeddings\n#     input_ids = batch['input_ids'].to(device)  # Tokenized text input\n#     labels = batch['labels'].to(device)  # Labels for training\n    \n#     # Forward pass through the model\n#     optimizer.zero_grad()\n#     outputs = combined_model(image_embeddings, input_ids)\n    \n#     # Get logits and calculate loss\n#     logits = outputs.logits.view(-1, logits.size(-1))\n#     labels = labels.view(-1)\n#     loss = F.cross_entropy(logits, labels)\n    \n#     # Backward pass and optimization\n#     loss.backward()\n#     optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:07.576714Z","iopub.execute_input":"2024-10-25T11:14:07.577108Z","iopub.status.idle":"2024-10-25T11:14:07.598595Z","shell.execute_reply.started":"2024-10-25T11:14:07.577067Z","shell.execute_reply":"2024-10-25T11:14:07.597020Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:07.600767Z","iopub.execute_input":"2024-10-25T11:14:07.601296Z","iopub.status.idle":"2024-10-25T11:14:07.616946Z","shell.execute_reply.started":"2024-10-25T11:14:07.601239Z","shell.execute_reply":"2024-10-25T11:14:07.615689Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your .pkl file\nfile_path = '/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl'\n\n# Load the embeddings from the .pkl file\nwith open(file_path, 'rb') as file:\n    embeddings = pickle.load(file)\n\n# Now you can use your embeddings\n# print(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T12:01:02.882130Z","iopub.execute_input":"2024-10-25T12:01:02.882729Z","iopub.status.idle":"2024-10-25T12:01:03.042938Z","shell.execute_reply.started":"2024-10-25T12:01:02.882679Z","shell.execute_reply":"2024-10-25T12:01:03.041527Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sample-cap/sample.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:07.991807Z","iopub.execute_input":"2024-10-25T11:14:07.992175Z","iopub.status.idle":"2024-10-25T11:14:08.015344Z","shell.execute_reply.started":"2024-10-25T11:14:07.992135Z","shell.execute_reply":"2024-10-25T11:14:08.014227Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import CLIPProcessor, CLIPModel\n# from transformers import Trainer, TrainingArguments\n# from datasets import Dataset\n# from torch.utils.data import Dataset as TorchDataset\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from PIL import Image\n\n# # Initialize the tokenizer and image model\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\n# class CustomDataset(TorchDataset):\n#     def __init__(self, image_paths, text_inputs, text_labels):\n#         self.image_paths = image_paths\n#         self.text_inputs = text_inputs\n#         self.text_labels = text_labels\n#         self.max_length = 256\n\n#     def __len__(self):\n#         return len(self.text_labels)\n\n#     def __getitem__(self, idx):\n#         image = Image.open(self.image_paths[idx])\n#         inputs = clip_processor(images=image, return_tensors=\"pt\")\n#         image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n#         image_features = feature_select(image_forward_outs)\n#         image_embedding = image_features.squeeze(0).to(device)\n        \n#         # Tokenize text input\n#         input_encoding = tokenizer(\n#             self.text_inputs[idx].replace('<image>',' '),\n#             return_tensors='pt',\n#             padding='max_length',  # Pad to max length\n#             truncation=True,  # Truncate if needed\n#             max_length=self.max_length\n#         )\n        \n#         # Tokenize text label (similar to inputs)\n#         label_encoding = tokenizer(\n#             self.text_labels[idx],\n#             return_tensors='pt',\n#             padding='max_length',\n#             truncation=True,\n#             max_length=self.max_length\n#         )\n\n#         # Extract input_ids and attention_mask for both inputs and labels\n#         input_ids = input_encoding['input_ids'].squeeze(0)\n#         input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n#         label_ids = label_encoding['input_ids'].squeeze(0)\n#         label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n#         # Return the image embeddings, tokenized inputs/labels, and attention masks\n#         return {\n#             'image_embeddings': image_embedding,  # Precomputed image embedding\n#             'input_ids': input_ids,  # Tokenized input\n#             'attention_mask': input_attention_mask,  # Attention mask for input\n#             'labels': label_ids,  # Tokenized label\n# #             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n#         }\n\n# # Create dataset (you will replace this with actual paths and data)\n# image_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\n# text_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\n# text_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# # Instantiate dataset\n# dataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:08.017560Z","iopub.execute_input":"2024-10-25T11:14:08.017952Z","iopub.status.idle":"2024-10-25T11:14:08.026387Z","shell.execute_reply.started":"2024-10-25T11:14:08.017913Z","shell.execute_reply":"2024-10-25T11:14:08.025101Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image_embedding = embeddings[self.image_paths[idx]]\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>',''),\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n#             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:08.027973Z","iopub.execute_input":"2024-10-25T11:14:08.028466Z","iopub.status.idle":"2024-10-25T11:14:32.718822Z","shell.execute_reply.started":"2024-10-25T11:14:08.028399Z","shell.execute_reply":"2024-10-25T11:14:32.717520Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5345ad54b0407fbb054f3898a9ec24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b29d86c71c4283ad6952beb46a0176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed25750342e417fa46bec9a3332f550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb34d288a7f4a7fbfa749112c5f9792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42b0ae45c8a44a178b91be779c2a121f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ae0fc9578549b1a39ad6eb8fea9c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c537a98bb54842ad8b159a75bd14980b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab72965f4d174d689a21fce91e73985c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2b2bf0071f4ec08565a04a05fcedb5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:32.720328Z","iopub.execute_input":"2024-10-25T11:14:32.721136Z","iopub.status.idle":"2024-10-25T11:14:32.742067Z","shell.execute_reply.started":"2024-10-25T11:14:32.721089Z","shell.execute_reply":"2024-10-25T11:14:32.740115Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'image_embeddings': tensor([[ 0.3817,  0.9930,  0.2789,  ...,  0.0197, -0.0689, -0.4641],\n         [ 0.3424,  0.9193,  1.4041,  ..., -0.1432, -0.2891,  0.5565],\n         [-0.3738,  0.6973,  0.2574,  ...,  0.1940,  0.2922,  0.6109],\n         ...,\n         [ 0.3682,  0.4731, -0.4251,  ...,  0.5928,  0.2523, -0.6850],\n         [-0.1267,  0.3708, -0.3366,  ...,  0.1251,  0.5551, -0.3766],\n         [ 0.5355,  1.0992, -0.4195,  ...,  0.3650, -0.2440,  0.3265]]),\n 'input_ids': tensor([128000,    198,   3923,    527,    279,   8146,    315,    279,   5951,\n            304,    279,   2217,     30, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor([128000,    791,   5951,    304,    279,   2217,    374,   4251,    323,\n           2579,     13, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009])}"},"metadata":{}}]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:32.743548Z","iopub.execute_input":"2024-10-25T11:14:32.744087Z","iopub.status.idle":"2024-10-25T11:14:34.977681Z","shell.execute_reply.started":"2024-10-25T11:14:32.744029Z","shell.execute_reply":"2024-10-25T11:14:34.976377Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7e661d20bf10>"},"metadata":{}}]},{"cell_type":"code","source":"# import torch\n# from transformers import DataCollator\n\n# class CustomDataCollator:\n#     def __init__(self, tokenizer):\n#         self.tokenizer = tokenizer\n    \n#     def __call__(self, batch):\n#         # Extract image embeddings\n#         image_embeddings = torch.stack([item['image_embeddings'] for item in batch])\n#         # Extract input_ids and labels\n#         input_ids = [item['input_ids'] for item in batch]\n#         labels = [item['labels'] for item in batch]\n#         print(batch)\n#         # Pad the input_ids and labels\n#         padded_input_ids = self.tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt')['input_ids']\n#         padded_labels = self.tokenizer.pad({'input_ids': labels}, padding=True, return_tensors='pt')['input_ids']\n\n#         # Create attention masks\n#         input_attention_mask = (padded_input_ids != self.tokenizer.pad_token_id).type(torch.float)\n#         label_attention_mask = (padded_labels != self.tokenizer.pad_token_id).type(torch.float)\n        \n#         # Prepare collated inputs\n#         collated_inputs = {\n#             'input_ids': padded_input_ids,\n#             'labels': padded_labels,\n#             'image_embeddings': image_embeddings,\n#             'attention_mask': input_attention_mask\n#         }\n        \n#         return collated_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:34.979170Z","iopub.execute_input":"2024-10-25T11:14:34.980001Z","iopub.status.idle":"2024-10-25T11:14:34.986317Z","shell.execute_reply.started":"2024-10-25T11:14:34.979953Z","shell.execute_reply":"2024-10-25T11:14:34.985214Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:34.988004Z","iopub.execute_input":"2024-10-25T11:14:34.988377Z","iopub.status.idle":"2024-10-25T11:14:35.011621Z","shell.execute_reply.started":"2024-10-25T11:14:34.988335Z","shell.execute_reply":"2024-10-25T11:14:35.010207Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=4,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n# mlp_projection = MLPProjection(input_dim=512, output_dim=768, hidden_dim=1024, num_layers=2)\n# model = PHI2WithMLP(mlp_projection,phi_lora_model = phi_lora_model)\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:14:51.378654Z","iopub.execute_input":"2024-10-25T11:14:51.379590Z","iopub.status.idle":"2024-10-25T11:40:10.364001Z","shell.execute_reply.started":"2024-10-25T11:14:51.379535Z","shell.execute_reply":"2024-10-25T11:40:10.361626Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 24:15, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"torch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\ntorch.Size([4, 256])\n4 256\ntorch.Size([4, 305])\ntorch.Size([4, 49, 2048]) torch.Size([4, 256, 2048])\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25, training_loss=3.9455938720703125, metrics={'train_runtime': 1517.433, 'train_samples_per_second': 0.066, 'train_steps_per_second': 0.016, 'total_flos': 0.0, 'train_loss': 3.9455938720703125, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"image_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:45:58.258057Z","iopub.execute_input":"2024-10-25T11:45:58.258912Z","iopub.status.idle":"2024-10-25T11:45:58.269871Z","shell.execute_reply.started":"2024-10-25T11:45:58.258857Z","shell.execute_reply":"2024-10-25T11:45:58.268580Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_path = image_paths[0]\ntext_input = text_inputs[0]  # Example text input\ntext_label = text_labels[0]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimage_embedding = embeddings[image_path]\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:47:50.375581Z","iopub.execute_input":"2024-10-25T11:47:50.376141Z","iopub.status.idle":"2024-10-25T11:47:50.389393Z","shell.execute_reply.started":"2024-10-25T11:47:50.376093Z","shell.execute_reply":"2024-10-25T11:47:50.387907Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:48:17.313776Z","iopub.execute_input":"2024-10-25T11:48:17.314341Z","iopub.status.idle":"2024-10-25T11:48:17.322224Z","shell.execute_reply.started":"2024-10-25T11:48:17.314294Z","shell.execute_reply":"2024-10-25T11:48:17.320847Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"torch.Size([49, 768])"},"metadata":{}}]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:48:19.691590Z","iopub.execute_input":"2024-10-25T11:48:19.692059Z","iopub.status.idle":"2024-10-25T11:48:19.699709Z","shell.execute_reply.started":"2024-10-25T11:48:19.692013Z","shell.execute_reply":"2024-10-25T11:48:19.698541Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"torch.Size([256])"},"metadata":{}}]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:48:30.225167Z","iopub.execute_input":"2024-10-25T11:48:30.225705Z","iopub.status.idle":"2024-10-25T11:48:30.231739Z","shell.execute_reply.started":"2024-10-25T11:48:30.225654Z","shell.execute_reply":"2024-10-25T11:48:30.230421Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:49:17.036354Z","iopub.execute_input":"2024-10-25T11:49:17.036915Z","iopub.status.idle":"2024-10-25T11:49:17.047384Z","shell.execute_reply.started":"2024-10-25T11:49:17.036867Z","shell.execute_reply":"2024-10-25T11:49:17.045908Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_embedding)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:50:56.266166Z","iopub.execute_input":"2024-10-25T11:50:56.266657Z","iopub.status.idle":"2024-10-25T11:50:56.280950Z","shell.execute_reply.started":"2024-10-25T11:50:56.266605Z","shell.execute_reply":"2024-10-25T11:50:56.279731Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:51:04.452747Z","iopub.execute_input":"2024-10-25T11:51:04.453219Z","iopub.status.idle":"2024-10-25T11:51:04.462161Z","shell.execute_reply.started":"2024-10-25T11:51:04.453177Z","shell.execute_reply":"2024-10-25T11:51:04.460595Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"(torch.Size([49, 768]), torch.Size([256, 2048]), torch.Size([49, 2048]))"},"metadata":{}}]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:51:09.547729Z","iopub.execute_input":"2024-10-25T11:51:09.548197Z","iopub.status.idle":"2024-10-25T11:51:09.558181Z","shell.execute_reply.started":"2024-10-25T11:51:09.548153Z","shell.execute_reply":"2024-10-25T11:51:09.556879Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"torch.Size([305, 2048])"},"metadata":{}}]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:51:14.093951Z","iopub.execute_input":"2024-10-25T11:51:14.094462Z","iopub.status.idle":"2024-10-25T11:51:14.222847Z","shell.execute_reply.started":"2024-10-25T11:51:14.094400Z","shell.execute_reply":"2024-10-25T11:51:14.221635Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\n# print(f\"Predicted labels: {predicted_labels}\")\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_labels[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:54:16.770289Z","iopub.execute_input":"2024-10-25T11:54:16.770811Z","iopub.status.idle":"2024-10-25T11:54:24.296597Z","shell.execute_reply.started":"2024-10-25T11:54:16.770763Z","shell.execute_reply":"2024-10-25T11:54:24.295356Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"torch.Size([1, 256])\n1 256\ntorch.Size([1, 305])\ntorch.Size([1, 49, 2048]) torch.Size([1, 256, 2048])\nPredicted text: ,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:52:07.827070Z","iopub.execute_input":"2024-10-25T11:52:07.827567Z","iopub.status.idle":"2024-10-25T11:52:15.507590Z","shell.execute_reply.started":"2024-10-25T11:52:07.827520Z","shell.execute_reply":"2024-10-25T11:52:15.506372Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"torch.Size([1, 256])\n1 256\ntorch.Size([1, 305])\ntorch.Size([1, 49, 2048]) torch.Size([1, 256, 2048])\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Predicted labels: tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009,     11, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# outputs","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:51:47.694354Z","iopub.execute_input":"2024-10-25T11:51:47.695687Z","iopub.status.idle":"2024-10-25T11:51:47.700758Z","shell.execute_reply.started":"2024-10-25T11:51:47.695630Z","shell.execute_reply":"2024-10-25T11:51:47.699522Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:52:24.707816Z","iopub.execute_input":"2024-10-25T11:52:24.708253Z","iopub.status.idle":"2024-10-25T11:52:24.716353Z","shell.execute_reply.started":"2024-10-25T11:52:24.708213Z","shell.execute_reply":"2024-10-25T11:52:24.715100Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 305, 128256])"},"metadata":{}}]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:52:27.677976Z","iopub.execute_input":"2024-10-25T11:52:27.678422Z","iopub.status.idle":"2024-10-25T11:52:27.735639Z","shell.execute_reply.started":"2024-10-25T11:52:27.678382Z","shell.execute_reply":"2024-10-25T11:52:27.734360Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Predicted text: ,\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:45:03.626424Z","iopub.execute_input":"2024-10-24T16:45:03.627110Z","iopub.status.idle":"2024-10-24T16:45:03.633990Z","shell.execute_reply.started":"2024-10-24T16:45:03.627048Z","shell.execute_reply":"2024-10-24T16:45:03.633058Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"'....Tags..................................................'"},"metadata":{}}]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:48.087127Z","iopub.execute_input":"2024-10-24T16:44:48.087547Z","iopub.status.idle":"2024-10-24T16:44:48.100215Z","shell.execute_reply.started":"2024-10-24T16:44:48.087499Z","shell.execute_reply":"2024-10-24T16:44:48.099332Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 306, 128256])"},"metadata":{}}]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = torch.randn(7484, 1, 1)\n\n# works as we are expanding singleton dimensions\nb = a.expand(-1, 100, 200)\nprint(b.shape)\n# torch.Size([7484, 100, 200])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fails\nb = a.expand(19, 100, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}