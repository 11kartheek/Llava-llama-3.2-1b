{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T16:15:59.165032Z","iopub.execute_input":"2024-10-24T16:15:59.165491Z","iopub.status.idle":"2024-10-24T16:15:59.647576Z","shell.execute_reply.started":"2024-10-24T16:15:59.165439Z","shell.execute_reply":"2024-10-24T16:15:59.646422Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:15:59.650047Z","iopub.execute_input":"2024-10-24T16:15:59.650662Z","iopub.status.idle":"2024-10-24T16:16:21.073199Z","shell.execute_reply.started":"2024-10-24T16:15:59.650609Z","shell.execute_reply":"2024-10-24T16:16:21.071894Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:22:48.612225Z","iopub.execute_input":"2024-10-24T16:22:48.613402Z","iopub.status.idle":"2024-10-24T16:23:02.494129Z","shell.execute_reply.started":"2024-10-24T16:22:48.613349Z","shell.execute_reply":"2024-10-24T16:23:02.492863Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:16:51.219386Z","iopub.execute_input":"2024-10-24T16:16:51.219824Z","iopub.status.idle":"2024-10-24T16:16:51.226301Z","shell.execute_reply.started":"2024-10-24T16:16:51.219782Z","shell.execute_reply":"2024-10-24T16:16:51.225270Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:23:05.018462Z","iopub.execute_input":"2024-10-24T16:23:05.018965Z","iopub.status.idle":"2024-10-24T16:23:05.045258Z","shell.execute_reply.started":"2024-10-24T16:23:05.018902Z","shell.execute_reply":"2024-10-24T16:23:05.044381Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba901f5da154437a256c876421b974d"}},"metadata":{}}]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:23:30.688532Z","iopub.execute_input":"2024-10-24T16:23:30.689273Z","iopub.status.idle":"2024-10-24T16:24:33.977649Z","shell.execute_reply.started":"2024-10-24T16:23:30.689226Z","shell.execute_reply":"2024-10-24T16:24:33.976298Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd049953f5646338780ef92cb0a4393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de1ff765796d455ca664a5e720bf766a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c1f96e9b6740999f3ac3bdfff13d84"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:24:33.980578Z","iopub.execute_input":"2024-10-24T16:24:33.981166Z","iopub.status.idle":"2024-10-24T16:24:33.990618Z","shell.execute_reply.started":"2024-10-24T16:24:33.981125Z","shell.execute_reply":"2024-10-24T16:24:33.989628Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n\n    def forward(self, image_embeddings=None, input_ids=None, attention_mask=None, labels=None):\n       \n        # Get token embeddings from PHI2 model\n        token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n\n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            print(attention_mask.shape)\n            print(batch_size, text_sequence_length)\n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            print(new_attention_mask.shape)\n            print(projected_image_embeddings.shape,token_embeddings.shape)\n            # Combine image and token embeddings\n            if random.random() < 0.5:\n                combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            else:\n                combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=1)\n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), -100, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([labels, label_padding], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels)\n\n        return outputs\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:12.711777Z","iopub.execute_input":"2024-10-24T16:40:12.712905Z","iopub.status.idle":"2024-10-24T16:40:12.767578Z","shell.execute_reply.started":"2024-10-24T16:40:12.712856Z","shell.execute_reply":"2024-10-24T16:40:12.766688Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\"\n)\n\n# Wrap PHI 2 with QLoRA\n# phi_lora_model = LoraModel(model, lora_config,\"default\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:14.237144Z","iopub.execute_input":"2024-10-24T16:40:14.238135Z","iopub.status.idle":"2024-10-24T16:40:14.243685Z","shell.execute_reply.started":"2024-10-24T16:40:14.238081Z","shell.execute_reply":"2024-10-24T16:40:14.242683Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"phi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:15.073020Z","iopub.execute_input":"2024-10-24T16:40:15.073442Z","iopub.status.idle":"2024-10-24T16:40:15.167514Z","shell.execute_reply.started":"2024-10-24T16:40:15.073402Z","shell.execute_reply":"2024-10-24T16:40:15.166771Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:15.834442Z","iopub.execute_input":"2024-10-24T16:40:15.834881Z","iopub.status.idle":"2024-10-24T16:40:15.849548Z","shell.execute_reply.started":"2024-10-24T16:40:15.834839Z","shell.execute_reply":"2024-10-24T16:40:15.848631Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:16.355974Z","iopub.execute_input":"2024-10-24T16:40:16.357088Z","iopub.status.idle":"2024-10-24T16:40:16.365690Z","shell.execute_reply.started":"2024-10-24T16:40:16.357024Z","shell.execute_reply":"2024-10-24T16:40:16.364718Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:17.052141Z","iopub.execute_input":"2024-10-24T16:40:17.052590Z","iopub.status.idle":"2024-10-24T16:40:17.062166Z","shell.execute_reply.started":"2024-10-24T16:40:17.052545Z","shell.execute_reply":"2024-10-24T16:40:17.061308Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n\ntrainable params: 2,179,072 || all params: 1,547,205,888 || trainable%: 0.1408\n\n\r\n","metadata":{}},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:18.288077Z","iopub.execute_input":"2024-10-24T16:40:18.289563Z","iopub.status.idle":"2024-10-24T16:40:18.298909Z","shell.execute_reply.started":"2024-10-24T16:40:18.289495Z","shell.execute_reply":"2024-10-24T16:40:18.298007Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:18.784025Z","iopub.execute_input":"2024-10-24T16:40:18.784467Z","iopub.status.idle":"2024-10-24T16:40:18.793740Z","shell.execute_reply.started":"2024-10-24T16:40:18.784425Z","shell.execute_reply":"2024-10-24T16:40:18.792788Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPModel\n\n# Load CLIP and PHI2\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:19.158147Z","iopub.execute_input":"2024-10-24T16:40:19.158601Z","iopub.status.idle":"2024-10-24T16:40:19.683613Z","shell.execute_reply.started":"2024-10-24T16:40:19.158558Z","shell.execute_reply":"2024-10-24T16:40:19.682659Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"\n# Now the model can be trained, and the optimizer only updates LoRA and projection\noptimizer = torch.optim.AdamW(\n    [p for p in combined_model.parameters() if p.requires_grad], lr=1e-4\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:19.685951Z","iopub.execute_input":"2024-10-24T16:40:19.686337Z","iopub.status.idle":"2024-10-24T16:40:19.695496Z","shell.execute_reply.started":"2024-10-24T16:40:19.686296Z","shell.execute_reply":"2024-10-24T16:40:19.694594Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"\n# # Training loop\n# def train_model(combined_model, data_loader, optimizer, num_epochs=1, device=\"cuda\"):\n#     combined_model.train()\n#     combined_model = combined_model.to(device)\n    \n#     for epoch in range(num_epochs):\n#         total_loss = 0\n#         for batch in data_loader:\n#             image_embeddings = batch['image_embeddings'].to(device)\n#             input_ids = batch['input_ids'].to(device)\n#             labels = batch['labels'].to(device)\n            \n#             # Forward pass\n#             optimizer.zero_grad()\n#             outputs = combined_model(image_embeddings, input_ids)\n            \n#             # Assume outputs is a tuple where the first element is logits\n#             logits = outputs.logits\n            \n#             # Flatten the logits and labels for cross-entropy loss\n#             logits = logits.view(-1, logits.size(-1))\n#             labels = labels.view(-1)\n            \n#             # Calculate loss (cross-entropy loss for language modeling)\n#             loss = F.cross_entropy(logits, labels)\n#             total_loss += loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             optimizer.step()\n        \n#         avg_loss = total_loss / len(data_loader)\n#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# # Usage\n# data_loader = get_data_loader(batch_size=16)  # Adjust the batch size as needed\n# train_model(combined_model, data_loader, optimizer, num_epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:20.670643Z","iopub.execute_input":"2024-10-24T16:40:20.671130Z","iopub.status.idle":"2024-10-24T16:40:20.677298Z","shell.execute_reply.started":"2024-10-24T16:40:20.671083Z","shell.execute_reply":"2024-10-24T16:40:20.676314Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# for batch in data_loader:\n#     image_embeddings = batch['image_embeddings'].to(device)  # Assuming pre-extracted embeddings\n#     input_ids = batch['input_ids'].to(device)  # Tokenized text input\n#     labels = batch['labels'].to(device)  # Labels for training\n    \n#     # Forward pass through the model\n#     optimizer.zero_grad()\n#     outputs = combined_model(image_embeddings, input_ids)\n    \n#     # Get logits and calculate loss\n#     logits = outputs.logits.view(-1, logits.size(-1))\n#     labels = labels.view(-1)\n#     loss = F.cross_entropy(logits, labels)\n    \n#     # Backward pass and optimization\n#     loss.backward()\n#     optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:21.119291Z","iopub.execute_input":"2024-10-24T16:40:21.119790Z","iopub.status.idle":"2024-10-24T16:40:21.124803Z","shell.execute_reply.started":"2024-10-24T16:40:21.119736Z","shell.execute_reply":"2024-10-24T16:40:21.123746Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:21.571177Z","iopub.execute_input":"2024-10-24T16:40:21.571625Z","iopub.status.idle":"2024-10-24T16:40:21.576244Z","shell.execute_reply.started":"2024-10-24T16:40:21.571586Z","shell.execute_reply":"2024-10-24T16:40:21.575384Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        inputs = clip_processor(images=image, return_tensors=\"pt\")\n        image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n        image_features = feature_select(image_forward_outs)\n        image_embedding = image_features.squeeze(0).to(device)\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n#             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\ntext_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:22.031282Z","iopub.execute_input":"2024-10-24T16:40:22.031706Z","iopub.status.idle":"2024-10-24T16:40:24.170262Z","shell.execute_reply.started":"2024-10-24T16:40:22.031663Z","shell.execute_reply":"2024-10-24T16:40:24.168718Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:24.172707Z","iopub.execute_input":"2024-10-24T16:40:24.173465Z","iopub.status.idle":"2024-10-24T16:40:24.213443Z","shell.execute_reply.started":"2024-10-24T16:40:24.173411Z","shell.execute_reply":"2024-10-24T16:40:24.212380Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78ff145ba110>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollator\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        # Extract image embeddings\n        image_embeddings = torch.stack([item['image_embeddings'] for item in batch])\n        # Extract input_ids and labels\n        input_ids = [item['input_ids'] for item in batch]\n        labels = [item['labels'] for item in batch]\n        print(batch)\n        # Pad the input_ids and labels\n        padded_input_ids = self.tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt')['input_ids']\n        padded_labels = self.tokenizer.pad({'input_ids': labels}, padding=True, return_tensors='pt')['input_ids']\n\n        # Create attention masks\n        input_attention_mask = (padded_input_ids != self.tokenizer.pad_token_id).type(torch.float)\n        label_attention_mask = (padded_labels != self.tokenizer.pad_token_id).type(torch.float)\n        \n        # Prepare collated inputs\n        collated_inputs = {\n            'input_ids': padded_input_ids,\n            'labels': padded_labels,\n            'image_embeddings': image_embeddings,\n            'attention_mask': input_attention_mask\n        }\n        \n        return collated_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:24.215229Z","iopub.execute_input":"2024-10-24T16:40:24.215748Z","iopub.status.idle":"2024-10-24T16:40:24.232027Z","shell.execute_reply.started":"2024-10-24T16:40:24.215705Z","shell.execute_reply":"2024-10-24T16:40:24.231018Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:40:24.770653Z","iopub.execute_input":"2024-10-24T16:40:24.771138Z","iopub.status.idle":"2024-10-24T16:40:24.780123Z","shell.execute_reply.started":"2024-10-24T16:40:24.771092Z","shell.execute_reply":"2024-10-24T16:40:24.779090Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n# mlp_projection = MLPProjection(input_dim=512, output_dim=768, hidden_dim=1024, num_layers=2)\n# model = PHI2WithMLP(mlp_projection,phi_lora_model = phi_lora_model)\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:41:37.238327Z","iopub.execute_input":"2024-10-24T16:41:37.239340Z","iopub.status.idle":"2024-10-24T16:44:15.639388Z","shell.execute_reply.started":"2024-10-24T16:41:37.239290Z","shell.execute_reply":"2024-10-24T16:44:15.638352Z"},"trusted":true},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 02:22, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=3.2734024047851564, metrics={'train_runtime': 157.8481, 'train_samples_per_second': 0.063, 'train_steps_per_second': 0.063, 'total_flos': 0.0, 'train_loss': 3.2734024047851564, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_embedding = clip_model.get_image_features(**inputs)\n    images.append(image_embedding)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:26.329061Z","iopub.execute_input":"2024-10-24T16:44:26.329517Z","iopub.status.idle":"2024-10-24T16:44:26.487695Z","shell.execute_reply.started":"2024-10-24T16:44:26.329476Z","shell.execute_reply":"2024-10-24T16:44:26.486906Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:27.262064Z","iopub.execute_input":"2024-10-24T16:44:27.262517Z","iopub.status.idle":"2024-10-24T16:44:27.268914Z","shell.execute_reply.started":"2024-10-24T16:44:27.262476Z","shell.execute_reply":"2024-10-24T16:44:27.268103Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"torch.Size([256])"},"metadata":{}}]},{"cell_type":"code","source":"input_ids.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:28.152080Z","iopub.execute_input":"2024-10-24T16:44:28.152511Z","iopub.status.idle":"2024-10-24T16:44:28.159179Z","shell.execute_reply.started":"2024-10-24T16:44:28.152471Z","shell.execute_reply":"2024-10-24T16:44:28.158267Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"torch.Size([256])"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:28.861851Z","iopub.execute_input":"2024-10-24T16:44:28.862741Z","iopub.status.idle":"2024-10-24T16:44:28.876917Z","shell.execute_reply.started":"2024-10-24T16:44:28.862682Z","shell.execute_reply":"2024-10-24T16:44:28.875974Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:29.530906Z","iopub.execute_input":"2024-10-24T16:44:29.531345Z","iopub.status.idle":"2024-10-24T16:44:29.536593Z","shell.execute_reply.started":"2024-10-24T16:44:29.531305Z","shell.execute_reply":"2024-10-24T16:44:29.535639Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"select_feature = 'cls_patch'","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:30.275896Z","iopub.execute_input":"2024-10-24T16:44:30.276387Z","iopub.status.idle":"2024-10-24T16:44:30.280893Z","shell.execute_reply.started":"2024-10-24T16:44:30.276343Z","shell.execute_reply":"2024-10-24T16:44:30.279973Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:31.134216Z","iopub.execute_input":"2024-10-24T16:44:31.135010Z","iopub.status.idle":"2024-10-24T16:44:31.140882Z","shell.execute_reply.started":"2024-10-24T16:44:31.134962Z","shell.execute_reply":"2024-10-24T16:44:31.140002Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\nimage_features = feature_select(image_forward_outs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:32.212205Z","iopub.execute_input":"2024-10-24T16:44:32.212756Z","iopub.status.idle":"2024-10-24T16:44:32.340972Z","shell.execute_reply.started":"2024-10-24T16:44:32.212699Z","shell.execute_reply":"2024-10-24T16:44:32.340167Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"image_features = image_features.squeeze(0).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:33.593139Z","iopub.execute_input":"2024-10-24T16:44:33.594379Z","iopub.status.idle":"2024-10-24T16:44:33.598883Z","shell.execute_reply.started":"2024-10-24T16:44:33.594329Z","shell.execute_reply":"2024-10-24T16:44:33.598001Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:34.830975Z","iopub.execute_input":"2024-10-24T16:44:34.831842Z","iopub.status.idle":"2024-10-24T16:44:34.838566Z","shell.execute_reply.started":"2024-10-24T16:44:34.831793Z","shell.execute_reply":"2024-10-24T16:44:34.837659Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:35.791113Z","iopub.execute_input":"2024-10-24T16:44:35.791598Z","iopub.status.idle":"2024-10-24T16:44:35.801958Z","shell.execute_reply.started":"2024-10-24T16:44:35.791543Z","shell.execute_reply":"2024-10-24T16:44:35.800898Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape,image_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:36.633143Z","iopub.execute_input":"2024-10-24T16:44:36.634257Z","iopub.status.idle":"2024-10-24T16:44:36.640885Z","shell.execute_reply.started":"2024-10-24T16:44:36.634207Z","shell.execute_reply":"2024-10-24T16:44:36.639959Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"(torch.Size([512]),\n torch.Size([256, 2048]),\n torch.Size([50, 2048]),\n torch.Size([50, 768]))"},"metadata":{}}]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:37.398910Z","iopub.execute_input":"2024-10-24T16:44:37.399412Z","iopub.status.idle":"2024-10-24T16:44:37.407004Z","shell.execute_reply.started":"2024-10-24T16:44:37.399366Z","shell.execute_reply":"2024-10-24T16:44:37.405674Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"torch.Size([306, 2048])"},"metadata":{}}]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:38.233838Z","iopub.execute_input":"2024-10-24T16:44:38.234801Z","iopub.status.idle":"2024-10-24T16:44:38.349717Z","shell.execute_reply.started":"2024-10-24T16:44:38.234749Z","shell.execute_reply":"2024-10-24T16:44:38.348762Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_features.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:40.038833Z","iopub.execute_input":"2024-10-24T16:44:40.039335Z","iopub.status.idle":"2024-10-24T16:44:47.829994Z","shell.execute_reply.started":"2024-10-24T16:44:40.039291Z","shell.execute_reply":"2024-10-24T16:44:47.828955Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:47.832251Z","iopub.execute_input":"2024-10-24T16:44:47.832631Z","iopub.status.idle":"2024-10-24T16:44:48.001217Z","shell.execute_reply.started":"2024-10-24T16:44:47.832591Z","shell.execute_reply":"2024-10-24T16:44:48.000191Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"CausalLMOutputWithPast(loss=None, logits=tensor([[[ 8.9182, 12.9890,  5.6337,  ...,  0.6924,  0.6912,  0.6914],\n         [ 5.5880,  7.9247,  1.2343,  ..., -0.4657, -0.4664, -0.4659],\n         [ 6.2376, 10.5711,  3.1060,  ..., -0.4147, -0.4155, -0.4151],\n         ...,\n         [ 0.6510,  2.8897, -0.2204,  ..., -0.8497, -0.8505, -0.8501],\n         [ 0.6649,  2.9125, -0.2187,  ..., -0.8565, -0.8572, -0.8569],\n         [ 0.6741,  2.9140, -0.1904,  ..., -0.8339, -0.8347, -0.8343]]]), past_key_values=((tensor([[[[ 2.2605e-01, -1.7809e-01,  2.5449e-01,  ...,  1.4157e-01,\n            1.8603e-01, -5.4988e-01],\n          [-9.9465e-02,  2.7529e-01, -3.3938e-01,  ...,  5.2675e-01,\n            2.2873e-01, -6.1204e-01],\n          [-1.1712e+00,  2.2214e-01, -2.2749e-01,  ...,  1.6170e-01,\n            5.1338e-01, -3.1485e-01],\n          ...,\n          [-2.0244e+00,  3.1323e+00, -3.5241e+00,  ...,  1.8625e+00,\n           -9.6013e-01, -1.6563e+00],\n          [-6.5572e+00,  1.1339e+00, -3.0932e+00,  ...,  1.8625e+00,\n           -9.6013e-01, -1.6563e+00],\n          [-5.0613e+00, -1.3459e+00, -2.0722e+00,  ...,  1.8625e+00,\n           -9.6013e-01, -1.6563e+00]],\n\n         [[-9.4037e-02, -1.8375e-01, -2.3978e-01,  ..., -3.5959e-01,\n            5.4894e-01, -5.2750e-01],\n          [ 1.3794e+00, -1.6444e+00,  4.7954e-01,  ...,  5.9912e-01,\n            1.4199e+00, -5.0706e-02],\n          [ 1.4032e+00, -1.9532e+00,  1.4705e-01,  ...,  7.8767e-01,\n            3.5934e-01, -4.2084e-01],\n          ...,\n          [ 3.6009e+00, -2.2452e+00,  3.0134e+00,  ...,  6.9038e-01,\n            2.8364e+00, -1.7789e+00],\n          [ 1.6722e+00, -2.3983e+00,  2.0906e+00,  ...,  6.9037e-01,\n            2.8364e+00, -1.7789e+00],\n          [-1.7939e+00, -1.5333e+00,  7.6885e-01,  ...,  6.9037e-01,\n            2.8364e+00, -1.7789e+00]],\n\n         [[-4.8812e-01,  3.1721e-01,  1.2999e-01,  ..., -2.6310e-01,\n            1.5657e+00, -7.1581e-01],\n          [ 1.2818e-01, -4.8718e-03, -2.9523e-01,  ...,  5.5462e-01,\n           -2.2545e-01, -1.9078e+00],\n          [ 6.3936e-01,  1.2463e-01, -7.6625e-03,  ..., -3.1216e-01,\n           -1.4942e-01, -6.2907e-01],\n          ...,\n          [-3.8268e-01, -2.3285e+00, -2.8333e+00,  ...,  3.3899e-01,\n            6.7066e-02, -5.5943e-01],\n          [-4.3743e+00, -2.0506e+00, -2.2440e+00,  ...,  3.3899e-01,\n            6.7066e-02, -5.5943e-01],\n          [-4.3442e+00, -9.0228e-01, -1.2266e+00,  ...,  3.3899e-01,\n            6.7067e-02, -5.5943e-01]],\n\n         ...,\n\n         [[ 1.6537e-01,  3.2111e-02, -1.8622e-01,  ...,  2.2131e-01,\n           -9.1171e-02,  2.1132e-01],\n          [-1.0264e+00,  2.9092e-01, -9.8559e-02,  ...,  1.2998e-01,\n            4.3894e-01, -4.0298e-01],\n          [-6.7691e-01,  3.8891e-01,  2.3159e-01,  ...,  2.1758e-01,\n            6.3313e-01,  7.5066e-02],\n          ...,\n          [-1.9763e-01,  5.7006e-01, -7.6190e-01,  ..., -5.5814e-01,\n           -5.2295e-01, -8.7590e-01],\n          [ 3.4198e-01,  3.7359e-03, -4.7146e-01,  ..., -5.5814e-01,\n           -5.2295e-01, -8.7590e-01],\n          [ 5.6717e-01, -5.6417e-01, -9.1070e-02,  ..., -5.5814e-01,\n           -5.2295e-01, -8.7590e-01]],\n\n         [[-6.5120e-02,  3.7857e-01, -3.9153e-01,  ...,  1.3155e+00,\n           -1.6840e+00,  2.7316e+00],\n          [ 1.5903e+00,  1.1451e+00,  1.4167e-01,  ..., -2.8398e-01,\n           -4.3006e-01,  1.2516e+00],\n          [ 2.0362e+00,  9.4912e-01,  2.4619e-01,  ..., -4.4715e-01,\n            3.3578e-01, -1.6107e+00],\n          ...,\n          [ 3.8360e+00,  3.5581e+00, -1.8470e+00,  ..., -8.6860e-01,\n            4.0980e-01, -1.5264e+00],\n          [ 1.3693e+00,  4.1334e+00, -3.2630e+00,  ..., -8.6860e-01,\n            4.0980e-01, -1.5264e+00],\n          [-2.3563e+00,  2.9543e+00, -4.0563e+00,  ..., -8.6860e-01,\n            4.0980e-01, -1.5264e+00]],\n\n         [[ 1.4899e-01, -1.5012e-01, -1.8356e-01,  ..., -2.0144e-01,\n            3.1387e-01, -2.5599e-02],\n          [-7.8053e-02,  6.9981e-01, -3.4844e-01,  ..., -5.3150e-01,\n           -2.3388e-01,  1.5135e-01],\n          [-6.6539e-01,  8.6918e-01, -6.5639e-02,  ..., -2.7116e-01,\n           -1.2550e-01,  3.7909e-02],\n          ...,\n          [-3.1006e+00,  2.7139e+00, -6.6674e-01,  ...,  5.1224e-01,\n           -1.7775e+00,  4.9628e-01],\n          [-2.3644e+00,  2.6259e+00, -5.8576e-01,  ...,  5.1224e-01,\n           -1.7775e+00,  4.9628e-01],\n          [ 5.4561e-01,  1.4233e+00, -3.9302e-01,  ...,  5.1224e-01,\n           -1.7775e+00,  4.9628e-01]]]]), tensor([[[[-9.1872e-02, -2.3381e-02,  8.9057e-03,  ...,  3.3569e-02,\n           -6.9674e-03, -9.2872e-02],\n          [-2.6816e-02, -5.7255e-02,  1.5932e-02,  ...,  1.0343e-02,\n            1.2997e-01, -1.0058e-02],\n          [-1.3583e-02,  7.9979e-03,  3.2020e-02,  ..., -4.5965e-02,\n            5.9505e-02, -4.7670e-02],\n          ...,\n          [ 6.1138e-02, -1.5685e-03,  4.6727e-02,  ...,  1.2562e-02,\n            5.7220e-03,  2.3513e-02],\n          [ 6.1138e-02, -1.5685e-03,  4.6727e-02,  ...,  1.2562e-02,\n            5.7220e-03,  2.3513e-02],\n          [ 6.1138e-02, -1.5685e-03,  4.6727e-02,  ...,  1.2562e-02,\n            5.7220e-03,  2.3513e-02]],\n\n         [[-1.1430e-01, -1.2152e-01, -9.4119e-02,  ..., -1.2358e-02,\n            1.9062e-02,  5.2745e-02],\n          [ 7.1548e-02,  4.5771e-02, -1.5688e-01,  ...,  7.7164e-03,\n            1.2496e-01,  4.5758e-02],\n          [-2.6648e-03, -3.4318e-02, -1.8734e-01,  ...,  3.5861e-02,\n            1.2360e-01,  7.4675e-02],\n          ...,\n          [-9.3609e-02,  2.7464e-02, -7.1667e-03,  ...,  1.6957e-02,\n           -5.7031e-02, -1.1698e-01],\n          [-9.3609e-02,  2.7464e-02, -7.1667e-03,  ...,  1.6957e-02,\n           -5.7031e-02, -1.1698e-01],\n          [-9.3609e-02,  2.7464e-02, -7.1667e-03,  ...,  1.6957e-02,\n           -5.7031e-02, -1.1698e-01]],\n\n         [[-3.1032e-02,  1.3596e-01,  8.3697e-02,  ...,  2.0070e-02,\n            4.0816e-02,  4.3427e-02],\n          [-3.2720e-02,  1.2521e-03,  2.6443e-02,  ...,  1.9562e-02,\n            5.5931e-02, -4.9993e-02],\n          [ 3.1072e-02,  3.7663e-03, -1.7901e-02,  ...,  1.7185e-02,\n            2.3487e-02, -4.1326e-03],\n          ...,\n          [-3.5401e-02, -2.2929e-02, -1.2671e-02,  ...,  8.8547e-02,\n            5.5377e-02, -4.8470e-02],\n          [-3.5401e-02, -2.2929e-02, -1.2671e-02,  ...,  8.8547e-02,\n            5.5377e-02, -4.8470e-02],\n          [-3.5401e-02, -2.2929e-02, -1.2671e-02,  ...,  8.8547e-02,\n            5.5377e-02, -4.8470e-02]],\n\n         ...,\n\n         [[ 2.3251e-02, -1.2684e-02,  9.0667e-03,  ...,  3.2000e-02,\n           -3.7886e-02,  1.7431e-02],\n          [ 6.6060e-02, -1.1239e-02, -1.6413e-02,  ..., -2.9338e-02,\n            3.9318e-02, -4.4915e-02],\n          [ 2.4273e-02, -5.6786e-02, -5.4624e-02,  ..., -3.0333e-02,\n            3.5213e-02, -6.8525e-03],\n          ...,\n          [-6.0798e-02,  8.2078e-02,  1.0836e-02,  ...,  1.4497e-02,\n            7.6514e-03, -1.3565e-04],\n          [-6.0798e-02,  8.2078e-02,  1.0836e-02,  ...,  1.4497e-02,\n            7.6514e-03, -1.3565e-04],\n          [-6.0798e-02,  8.2078e-02,  1.0836e-02,  ...,  1.4497e-02,\n            7.6514e-03, -1.3565e-04]],\n\n         [[-1.4531e-01, -8.8731e-02, -8.4762e-02,  ..., -4.3840e-02,\n           -1.7932e-01, -1.5386e-01],\n          [ 7.4551e-02, -1.2226e-01,  7.0286e-02,  ..., -5.9452e-02,\n           -3.7554e-02, -2.4681e-03],\n          [-7.3292e-02, -1.9725e-01,  1.4955e-01,  ..., -1.0296e-02,\n           -1.4494e-02, -9.1767e-02],\n          ...,\n          [ 1.1535e-01, -7.5705e-02,  4.6340e-02,  ...,  1.4725e-02,\n           -1.3349e-02, -4.5793e-02],\n          [ 1.1535e-01, -7.5705e-02,  4.6340e-02,  ...,  1.4725e-02,\n           -1.3349e-02, -4.5793e-02],\n          [ 1.1535e-01, -7.5705e-02,  4.6340e-02,  ...,  1.4725e-02,\n           -1.3349e-02, -4.5793e-02]],\n\n         [[-3.1155e-03, -1.1627e-01, -1.1876e-01,  ...,  4.7236e-02,\n           -2.8417e-02,  2.8249e-02],\n          [-4.6176e-02, -1.3779e-01, -1.6158e-02,  ...,  4.5749e-03,\n           -5.2209e-03,  6.3803e-03],\n          [-1.5080e-02, -3.1876e-03, -1.8879e-02,  ...,  5.2839e-03,\n           -4.6661e-02, -2.7635e-02],\n          ...,\n          [ 2.5300e-02,  1.1243e-01,  4.8090e-02,  ..., -6.2857e-02,\n            4.3669e-02, -1.5433e-01],\n          [ 2.5300e-02,  1.1243e-01,  4.8090e-02,  ..., -6.2857e-02,\n            4.3669e-02, -1.5433e-01],\n          [ 2.5300e-02,  1.1243e-01,  4.8090e-02,  ..., -6.2857e-02,\n            4.3669e-02, -1.5433e-01]]]])), (tensor([[[[ 1.0441e+00, -1.0372e+00,  3.3279e-01,  ...,  1.9729e-02,\n            1.5515e+00, -1.5549e-01],\n          [ 7.6488e-01, -7.6058e-01, -1.5921e-02,  ...,  2.7567e-01,\n           -1.4876e-01, -9.3604e-01],\n          [-5.5268e-01, -1.5346e-01, -7.2048e-01,  ..., -1.2232e-01,\n            1.3606e-01, -1.4330e+00],\n          ...,\n          [ 6.1472e-01, -2.4488e+00, -1.6213e+00,  ...,  4.0061e-02,\n            8.6723e-01, -2.9511e+00],\n          [-3.6972e+00, -1.2902e+00, -1.0959e+00,  ...,  3.1575e-02,\n            8.5689e-01, -2.9742e+00],\n          [-4.6358e+00,  4.4195e-01, -3.4086e-01,  ...,  2.9215e-02,\n            8.4459e-01, -2.9985e+00]],\n\n         [[ 9.9964e-01,  7.9556e-01, -3.8318e-01,  ..., -9.5811e-01,\n            4.4879e-01,  2.3309e-01],\n          [ 1.4825e+00, -9.1158e-01, -8.0924e-01,  ..., -1.1143e+00,\n           -1.4697e-01,  5.8990e-01],\n          [-8.9206e-01, -1.5394e+00, -8.5257e-01,  ..., -1.7866e+00,\n           -6.3710e-01,  1.1406e+00],\n          ...,\n          [-1.1820e+00, -1.0105e+00, -2.3528e+00,  ..., -2.2515e+00,\n           -3.3301e-01,  2.3313e+00],\n          [-5.3676e+00, -2.8326e+00, -2.2643e+00,  ..., -2.2690e+00,\n           -3.3376e-01,  2.3481e+00],\n          [-4.6450e+00, -3.4710e+00, -1.7424e+00,  ..., -2.2877e+00,\n           -3.2894e-01,  2.3651e+00]],\n\n         [[-1.4590e-01, -8.6060e-02, -5.4420e-01,  ...,  1.0860e-01,\n           -7.1318e-01, -1.1373e-01],\n          [-1.1757e+00, -6.0645e-01, -1.1626e+00,  ..., -3.9020e-01,\n            1.4191e-01, -4.5251e-02],\n          [-5.7037e-02, -2.9959e-01, -7.6254e-01,  ..., -4.2564e-01,\n           -5.2407e-01,  1.0799e-02],\n          ...,\n          [-1.7423e+00, -8.5183e-01, -6.3310e-01,  ..., -2.3956e+00,\n            1.0618e+00,  2.1043e-01],\n          [-1.8433e-01, -1.5628e+00, -8.9917e-01,  ..., -2.3842e+00,\n            1.0615e+00,  2.0969e-01],\n          [ 1.5676e+00, -1.6301e+00, -9.8560e-01,  ..., -2.3788e+00,\n            1.0600e+00,  2.0954e-01]],\n\n         ...,\n\n         [[ 8.0000e-01, -5.0776e-01,  5.0923e-02,  ..., -1.5282e-01,\n           -2.8960e+00, -1.2289e+00],\n          [ 4.2028e-01, -7.4463e-01,  6.8813e-01,  ...,  1.5801e+00,\n           -2.5778e+00, -9.1546e-01],\n          [-7.6106e-01,  3.0462e-01,  3.0614e-01,  ...,  8.6851e-01,\n           -2.7453e+00, -6.5844e-01],\n          ...,\n          [-4.8320e-01, -2.0547e+00,  2.6747e-01,  ..., -7.4369e-01,\n           -1.5551e+00, -3.3011e+00],\n          [-3.5310e+00, -7.0276e-01, -3.1610e-01,  ..., -7.5267e-01,\n           -1.5604e+00, -3.3293e+00],\n          [-3.3481e+00,  9.6395e-01, -8.5990e-01,  ..., -7.5762e-01,\n           -1.5644e+00, -3.3605e+00]],\n\n         [[ 2.2807e+00, -9.9714e-02,  6.4302e-01,  ...,  7.3368e-01,\n           -3.9698e-01, -1.1487e-02],\n          [ 1.0995e+00, -4.2273e-01,  1.1779e+00,  ...,  1.1766e+00,\n           -5.0730e-01,  5.6162e-01],\n          [-8.9509e-01, -1.3886e+00,  2.0465e-01,  ...,  4.8772e-01,\n           -2.0272e-01,  8.8293e-01],\n          ...,\n          [ 9.2051e-01,  5.6487e-01,  4.3531e-02,  ..., -4.8766e-01,\n           -9.9055e-01,  2.1665e+00],\n          [-4.0239e+00, -9.5035e-01, -1.2828e+00,  ..., -4.8125e-01,\n           -9.9897e-01,  2.1842e+00],\n          [-5.3069e+00, -2.0895e+00, -2.3861e+00,  ..., -4.7914e-01,\n           -1.0044e+00,  2.2016e+00]],\n\n         [[ 9.7902e-01, -1.4855e-01,  6.7005e-01,  ..., -1.1338e+00,\n            6.0301e-01,  1.1268e+00],\n          [ 4.6155e-01,  6.0588e-01,  8.9555e-01,  ..., -1.6271e+00,\n           -1.4742e+00, -2.5532e-01],\n          [-1.1456e+00,  4.4231e-01, -1.0827e-01,  ..., -1.3330e+00,\n           -1.0158e+00,  5.1241e-01],\n          ...,\n          [ 4.0753e-01, -5.7227e-01,  5.8320e-01,  ..., -2.2683e-01,\n            8.3972e-04,  2.0782e+00],\n          [-9.9293e-01,  1.7725e-01, -1.5182e-01,  ..., -2.2365e-01,\n            4.3399e-03,  2.0838e+00],\n          [-1.4853e+00,  8.6545e-01, -8.6304e-01,  ..., -2.1980e-01,\n            6.3003e-03,  2.0936e+00]]]]), tensor([[[[ 1.4447e-02, -2.2872e-02,  2.8439e-02,  ..., -3.1140e-01,\n           -1.7218e-04, -2.4728e-01],\n          [-2.0630e-01,  3.0129e-01,  3.2102e-03,  ..., -1.9054e-01,\n           -1.3788e-01,  5.4891e-02],\n          [-6.0842e-02,  1.5347e-01, -1.7104e-01,  ...,  1.8165e-02,\n            8.7010e-02, -1.4109e-01],\n          ...,\n          [-2.9643e-02,  8.6933e-02, -3.1833e-02,  ...,  6.8354e-03,\n            5.1197e-02, -1.3701e-01],\n          [-2.8661e-02,  8.6458e-02, -3.1253e-02,  ...,  7.5163e-03,\n            5.2282e-02, -1.3547e-01],\n          [-2.7619e-02,  8.6755e-02, -3.0713e-02,  ...,  8.2816e-03,\n            5.3294e-02, -1.3466e-01]],\n\n         [[-2.3030e-02,  1.5988e-02,  7.0521e-02,  ..., -1.2866e-01,\n            3.9105e-01, -7.0222e-03],\n          [ 3.8278e-02, -3.1062e-01, -8.6973e-02,  ..., -1.1362e-01,\n            1.8016e-01,  6.4756e-02],\n          [-7.4497e-02, -9.7745e-02, -1.1267e-01,  ...,  1.2734e-01,\n            2.1054e-01,  2.2969e-01],\n          ...,\n          [-1.9093e-03, -1.1555e-02,  2.1205e-01,  ...,  6.8455e-02,\n           -9.0297e-02,  1.1943e-01],\n          [-1.5107e-03, -1.0919e-02,  2.1102e-01,  ...,  6.7774e-02,\n           -8.8998e-02,  1.2030e-01],\n          [-1.0313e-03, -1.0637e-02,  2.1039e-01,  ...,  6.6975e-02,\n           -8.7505e-02,  1.2075e-01]],\n\n         [[-6.2254e-03,  1.9792e-01, -9.5613e-02,  ...,  3.9067e-02,\n            2.3266e-01,  1.8607e-01],\n          [-1.0181e-01,  4.1337e-02,  1.0371e-01,  ...,  6.3614e-02,\n            1.5556e-01,  2.8668e-01],\n          [-4.0249e-02,  2.5374e-02,  3.0241e-02,  ...,  7.3522e-02,\n            4.8911e-01,  3.1839e-01],\n          ...,\n          [-2.0643e-01,  2.3762e-02, -3.1972e-02,  ...,  4.8211e-02,\n           -1.1731e-01, -1.3293e-01],\n          [-2.0476e-01,  2.3272e-02, -3.0928e-02,  ...,  5.2351e-02,\n           -1.1548e-01, -1.3128e-01],\n          [-2.0382e-01,  2.2379e-02, -2.9514e-02,  ...,  5.6173e-02,\n           -1.1457e-01, -1.3070e-01]],\n\n         ...,\n\n         [[-2.6763e-02, -2.8806e-02,  2.1098e-01,  ...,  8.4761e-02,\n           -7.6772e-02, -6.6534e-02],\n          [ 7.2415e-02,  1.3075e-01,  1.8551e-01,  ...,  3.7348e-02,\n           -7.3101e-03, -2.5683e-02],\n          [ 1.2826e-02,  1.2223e-01,  2.4445e-02,  ..., -6.6142e-02,\n            5.7760e-02,  8.5633e-04],\n          ...,\n          [-7.9427e-02,  1.2070e-01,  2.2591e-01,  ...,  2.0528e-01,\n            5.3132e-02,  1.6220e-02],\n          [-8.0220e-02,  1.1995e-01,  2.2586e-01,  ...,  2.0842e-01,\n            5.3312e-02,  1.5689e-02],\n          [-8.1157e-02,  1.1939e-01,  2.2622e-01,  ...,  2.1082e-01,\n            5.3141e-02,  1.4984e-02]],\n\n         [[-4.2911e-02,  2.0199e-01,  1.7347e-03,  ..., -1.3629e-02,\n           -7.2544e-02,  9.2884e-02],\n          [ 2.4334e-01, -7.7646e-02, -2.1965e-01,  ..., -1.3429e-01,\n           -5.4185e-02,  1.1948e-01],\n          [ 1.1385e-01,  6.6668e-02, -9.8902e-02,  ...,  1.2601e-01,\n           -7.7474e-02,  1.8954e-01],\n          ...,\n          [-9.0619e-02, -8.5378e-02, -4.1559e-02,  ..., -4.2912e-02,\n           -1.9970e-01,  1.6524e-01],\n          [-9.1260e-02, -8.7104e-02, -4.1765e-02,  ..., -4.2328e-02,\n           -1.9914e-01,  1.6488e-01],\n          [-9.2196e-02, -8.7916e-02, -4.1183e-02,  ..., -4.1663e-02,\n           -1.9837e-01,  1.6511e-01]],\n\n         [[ 1.3014e-01,  2.8417e-01, -4.3286e-02,  ..., -3.2770e-01,\n           -7.4689e-02, -3.4329e-01],\n          [ 1.1480e-01,  1.1510e-01, -2.1332e-02,  ...,  1.3248e-01,\n           -9.0429e-04, -1.0802e-01],\n          [ 1.6904e-01,  2.9334e-01, -3.2248e-02,  ...,  1.5283e-01,\n            1.0652e-01, -1.5438e-01],\n          ...,\n          [-4.2342e-01, -1.8010e-01, -3.1912e-01,  ...,  2.8646e-01,\n            2.0606e-01, -6.3716e-03],\n          [-4.2277e-01, -1.7727e-01, -3.1815e-01,  ...,  2.8449e-01,\n            2.0543e-01, -5.0347e-03],\n          [-4.2121e-01, -1.7572e-01, -3.1745e-01,  ...,  2.8158e-01,\n            2.0546e-01, -4.5197e-03]]]])), (tensor([[[[-2.1103,  1.3580, -0.3502,  ..., -1.6977,  1.9604,  0.4106],\n          [ 0.1948,  2.1964, -0.9377,  ..., -2.6411,  2.0593,  1.7528],\n          [ 2.0638,  0.9883, -1.1711,  ..., -1.5164,  2.4216,  0.8171],\n          ...,\n          [ 1.6736,  1.6427, -3.4647,  ..., -1.1315,  4.4123, -0.6670],\n          [ 3.6688,  2.1512, -3.5510,  ..., -1.2062,  4.4240, -0.6953],\n          [ 2.2760,  1.6954, -2.9242,  ..., -1.2847,  4.4291, -0.7128]],\n\n         [[ 0.0208, -0.7278, -0.0162,  ..., -0.4466,  0.8502, -1.4817],\n          [ 0.4979, -0.2920, -0.8033,  ...,  0.0298,  0.3057, -1.3843],\n          [ 0.6553,  0.6106, -0.9397,  ...,  0.1567,  0.6764, -1.5252],\n          ...,\n          [ 1.3285, -0.0793, -1.8606,  ...,  0.2968, -0.5100, -0.8909],\n          [ 1.5039,  1.0865, -1.9379,  ...,  0.3311, -0.5363, -0.8888],\n          [ 0.3146,  1.7940, -1.6428,  ...,  0.3546, -0.5414, -0.8760]],\n\n         [[ 0.7890,  0.1576,  0.6966,  ...,  1.1659,  1.6686,  1.2455],\n          [-1.4832,  0.0469, -1.3107,  ..., -0.6902,  1.2282,  2.1462],\n          [-1.8563, -0.4424, -0.9199,  ...,  0.0861,  1.7672,  1.9594],\n          ...,\n          [-2.2300, -1.6304, -2.3346,  ..., -0.6799,  4.0070,  4.3156],\n          [-1.8316, -2.3057, -3.3616,  ..., -0.6601,  4.0222,  4.3490],\n          [ 0.2849, -1.9745, -3.7229,  ..., -0.6384,  4.0333,  4.3711]],\n\n         ...,\n\n         [[-2.0899, -1.7185,  0.8343,  ..., -0.0814,  1.4360, -0.4899],\n          [-0.6557, -2.1130,  1.1144,  ..., -0.1727,  0.1294, -0.4084],\n          [ 0.0156, -1.2876,  0.3755,  ..., -1.2307,  0.8275, -0.8419],\n          ...,\n          [-1.3124, -1.9927,  0.8261,  ..., -3.5034,  1.4729, -5.8023],\n          [ 2.1616, -2.0380, -0.4876,  ..., -3.5828,  1.4875, -5.9110],\n          [ 3.7306, -1.1757, -1.7044,  ..., -3.6653,  1.4610, -6.0542]],\n\n         [[-0.3724, -1.4789, -1.6075,  ..., -0.2480, -0.7371,  0.9926],\n          [ 1.5405, -0.8669, -1.7202,  ...,  0.1117,  0.8959, -0.2466],\n          [ 0.9541, -0.0370, -1.0675,  ...,  1.1710,  0.1651, -0.3030],\n          ...,\n          [ 1.8903, -2.1410, -0.2481,  ...,  2.2190,  2.8812, -0.1029],\n          [ 1.8037, -0.9228,  0.7521,  ...,  2.2734,  2.9846, -0.0837],\n          [ 0.0869,  0.7201,  1.5888,  ...,  2.3666,  3.0172, -0.0693]],\n\n         [[ 0.2200, -0.1384,  0.5363,  ...,  0.9767, -0.9843,  0.0407],\n          [ 0.7662,  0.8708,  1.0161,  ...,  1.7062, -1.5516, -0.4787],\n          [ 0.8659,  2.0259,  1.2207,  ...,  2.6870, -1.0285, -0.7436],\n          ...,\n          [ 3.3301,  0.7531,  3.0741,  ...,  0.5580, -0.9941, -1.0199],\n          [ 1.3007,  2.4612,  2.3890,  ...,  0.5623, -0.9974, -1.0286],\n          [-1.9527,  3.1564,  1.2486,  ...,  0.5632, -1.0280, -1.0221]]]]), tensor([[[[ 5.2430e-02,  1.4267e-01, -1.5959e-01,  ..., -9.9973e-02,\n            1.8957e-01, -1.7415e-01],\n          [-6.8220e-02,  5.3956e-01, -1.5559e-01,  ..., -3.8909e-03,\n            7.4356e-02,  1.5116e-02],\n          [-2.3036e-01,  2.0985e-01, -5.9375e-02,  ..., -5.4095e-02,\n            2.0074e-01, -1.2029e-01],\n          ...,\n          [ 1.5653e-02,  4.5994e-02, -3.4713e-01,  ..., -2.0738e-01,\n            3.2161e-02,  2.1476e-01],\n          [-5.7714e-04,  3.7471e-02, -3.4062e-01,  ..., -2.1834e-01,\n            4.9197e-02,  2.0556e-01],\n          [-9.0344e-03,  3.6902e-02, -3.3268e-01,  ..., -2.3167e-01,\n            5.6008e-02,  2.0016e-01]],\n\n         [[ 1.5115e-02, -2.9519e-01,  9.7770e-02,  ..., -1.1622e-01,\n            1.5943e-01,  4.5943e-01],\n          [ 3.4461e-01,  8.0256e-03,  7.8356e-02,  ..., -3.9008e-01,\n            2.6824e-01,  1.8759e-01],\n          [ 3.1040e-01,  1.1357e-01,  2.5057e-02,  ...,  2.7368e-02,\n            3.2645e-01,  5.5456e-02],\n          ...,\n          [-1.4706e-01, -1.1169e-01,  3.2140e-01,  ..., -3.3253e-01,\n           -1.3910e-01,  1.8632e-01],\n          [-1.3305e-01, -1.1851e-01,  3.1538e-01,  ..., -3.2922e-01,\n           -1.3508e-01,  1.7760e-01],\n          [-1.1046e-01, -1.1607e-01,  3.0723e-01,  ..., -3.3146e-01,\n           -1.2691e-01,  1.7083e-01]],\n\n         [[-1.0712e-01,  2.1517e-01,  8.8864e-02,  ...,  9.4277e-02,\n            7.0515e-02, -4.9940e-02],\n          [-8.9189e-02,  2.3333e-02, -3.8891e-01,  ..., -6.8717e-02,\n            6.2950e-02,  1.3675e-01],\n          [-2.6498e-03, -8.2233e-02, -6.3948e-02,  ...,  5.8601e-02,\n            1.6656e-01,  3.9300e-01],\n          ...,\n          [-7.9481e-02, -1.3518e-02, -1.7340e-01,  ...,  2.3973e-01,\n           -7.3848e-02, -2.3995e-01],\n          [-8.4633e-02, -2.7047e-02, -1.8131e-01,  ...,  2.5096e-01,\n           -6.8097e-02, -2.4704e-01],\n          [-9.0237e-02, -3.4526e-02, -1.9387e-01,  ...,  2.5512e-01,\n           -6.6252e-02, -2.5991e-01]],\n\n         ...,\n\n         [[-2.6091e-01,  4.0561e-01, -1.8834e-01,  ...,  7.9479e-01,\n           -8.1135e-02,  1.9622e-01],\n          [ 2.8452e-01,  1.6710e-01,  2.7032e-02,  ...,  5.6895e-01,\n           -2.6891e-01, -1.4482e-01],\n          [ 1.4577e-01, -4.5712e-02, -4.2620e-01,  ...,  5.1167e-01,\n           -4.1087e-01, -6.8441e-02],\n          ...,\n          [ 2.4867e-01,  5.9477e-03, -4.7036e-01,  ...,  8.6052e-01,\n           -1.4492e-01,  7.7081e-02],\n          [ 2.4966e-01,  2.1003e-03, -4.6579e-01,  ...,  8.6257e-01,\n           -1.3487e-01,  8.2406e-02],\n          [ 2.4249e-01, -5.1527e-03, -4.6484e-01,  ...,  8.6524e-01,\n           -1.2575e-01,  8.8817e-02]],\n\n         [[ 1.1630e-01, -1.2315e-01, -6.6477e-02,  ...,  1.7794e-01,\n           -1.6339e-01, -1.0995e-01],\n          [ 1.9682e-01, -8.6605e-02, -5.9633e-03,  ...,  1.3318e-01,\n           -1.7070e-01,  1.6201e-01],\n          [ 3.3601e-01, -1.9795e-02,  1.2337e-01,  ...,  1.0939e-01,\n           -7.0176e-02,  2.0904e-02],\n          ...,\n          [ 1.5629e-02, -1.6555e-01,  1.9932e-01,  ..., -9.5326e-02,\n           -1.8304e-01, -2.9479e-01],\n          [ 9.9838e-03, -1.6100e-01,  2.0513e-01,  ..., -9.9188e-02,\n           -1.8503e-01, -3.0243e-01],\n          [ 1.3629e-02, -1.5908e-01,  2.0454e-01,  ..., -9.9513e-02,\n           -1.9644e-01, -3.1040e-01]],\n\n         [[-2.8580e-01,  8.6636e-02, -3.7038e-02,  ...,  4.7614e-03,\n            1.1924e-01, -3.5063e-01],\n          [-1.4147e-01, -2.8882e-01, -7.9281e-02,  ..., -1.0285e-01,\n            1.9923e-01, -1.5104e-01],\n          [-2.7782e-01, -2.6824e-01,  9.7184e-02,  ...,  1.5063e-01,\n            2.6046e-01, -2.3629e-01],\n          ...,\n          [ 2.1025e-02,  2.2650e-01,  1.9578e-01,  ...,  3.3378e-02,\n            1.0231e-01,  9.8072e-02],\n          [ 3.5671e-02,  2.2802e-01,  1.9927e-01,  ...,  3.9809e-02,\n            9.6955e-02,  1.0008e-01],\n          [ 4.7130e-02,  2.2458e-01,  1.9621e-01,  ...,  4.2904e-02,\n            9.7812e-02,  9.1605e-02]]]])), (tensor([[[[ 1.0919, -2.0722, -1.8899,  ..., -0.5306,  1.1913, -1.4054],\n          [ 2.3795, -1.3506, -3.2459,  ..., -0.5955,  0.7410, -0.7390],\n          [ 0.6978, -0.9812, -2.7268,  ..., -0.5613,  0.6565, -1.1570],\n          ...,\n          [ 1.9380, -1.4261, -2.1345,  ...,  0.8221,  0.9853, -2.8145],\n          [ 1.1684, -1.6996, -1.1643,  ...,  0.8290,  0.9963, -2.7901],\n          [-0.6609, -1.2100,  0.0187,  ...,  0.8311,  1.0116, -2.7786]],\n\n         [[ 1.7819,  1.4391,  2.8497,  ...,  0.0880, -0.8097, -0.1706],\n          [ 0.7469,  1.0967,  2.8831,  ...,  0.2018, -0.0492, -0.4424],\n          [-0.1257,  0.7679,  3.3517,  ...,  1.2832, -0.7836, -0.1122],\n          ...,\n          [ 3.1080,  2.8415,  2.9313,  ...,  0.8895,  1.4564,  0.1674],\n          [-0.6606,  2.2453,  1.6541,  ...,  0.9063,  1.4634,  0.2043],\n          [-3.8596,  0.6793,  0.0526,  ...,  0.9091,  1.4792,  0.2417]],\n\n         [[ 0.2753,  0.1318, -0.7428,  ...,  0.2053, -2.9796,  3.2761],\n          [ 2.8426,  1.1772, -1.9018,  ..., -0.1946, -2.9043,  3.2702],\n          [ 2.8103,  2.1102, -2.6928,  ...,  0.2061, -3.0888,  3.6471],\n          ...,\n          [ 3.7475,  0.6139, -4.1572,  ...,  1.0000, -4.1023,  5.1899],\n          [ 2.1771,  2.2347, -3.4098,  ...,  1.0149, -4.1158,  5.1805],\n          [-1.3733,  2.9245, -1.9906,  ...,  1.0302, -4.1245,  5.1863]],\n\n         ...,\n\n         [[ 1.6952, -1.2771,  0.9915,  ...,  0.0840, -0.2187,  0.4260],\n          [-0.0355, -0.9261,  0.7176,  ...,  0.1456, -0.1227,  0.1090],\n          [-2.1051,  0.1362,  0.9479,  ..., -0.3466, -0.2293,  0.7861],\n          ...,\n          [ 0.1315, -2.0647,  1.3486,  ..., -0.6947,  0.1424,  1.2000],\n          [-1.7784, -1.2305,  0.6647,  ..., -0.7101,  0.1351,  1.1705],\n          [-2.0526,  0.0915, -0.1450,  ..., -0.7169,  0.1300,  1.1411]],\n\n         [[-3.3073, -1.4547, -1.5264,  ..., -0.4971, -0.6869, -0.6170],\n          [-0.0631, -0.5206, -0.0226,  ..., -1.0534, -1.7474, -0.0846],\n          [ 2.0517,  0.5478, -0.0108,  ..., -0.1775, -0.9859, -0.4123],\n          ...,\n          [-0.6398, -2.5193,  1.8265,  ...,  0.2847, -1.8033, -1.2424],\n          [ 2.6653, -1.5768,  3.2891,  ...,  0.2980, -1.8084, -1.2233],\n          [ 3.5393,  0.0261,  4.1210,  ...,  0.3059, -1.8180, -1.2159]],\n\n         [[ 1.3006,  1.5698,  0.8363,  ..., -1.4613,  3.5000, -1.6193],\n          [-0.0441,  2.2986,  0.7640,  ..., -0.5792,  2.7443, -1.4165],\n          [-1.4619,  2.1788,  1.4394,  ..., -0.7127,  3.7336, -1.6894],\n          ...,\n          [ 0.1252,  1.8189,  2.4262,  ..., -0.1056,  5.8996, -1.4938],\n          [-1.7213,  2.7894,  1.5491,  ..., -0.0863,  5.9014, -1.4872],\n          [-2.0352,  2.6054,  0.3638,  ..., -0.0621,  5.9148, -1.4682]]]]), tensor([[[[-0.0047, -0.1982,  0.3230,  ..., -0.0666, -0.0688,  0.6998],\n          [ 0.0372, -0.0428,  0.0756,  ..., -0.0176, -0.2777,  0.2994],\n          [ 0.0420,  0.0375, -0.0912,  ..., -0.2863, -0.2177,  0.0372],\n          ...,\n          [ 0.1954, -0.0166, -0.0300,  ...,  0.0674,  0.0409, -0.0456],\n          [ 0.1889, -0.0274, -0.0338,  ...,  0.0672,  0.0481, -0.0462],\n          [ 0.1940, -0.0304, -0.0359,  ...,  0.0682,  0.0487, -0.0428]],\n\n         [[ 0.0474, -0.0309, -0.0230,  ...,  0.1272,  0.3877,  0.0876],\n          [ 0.1457,  0.0080, -0.2665,  ...,  0.1747,  0.2887, -0.0720],\n          [ 0.0229, -0.0932,  0.0243,  ...,  0.1535,  0.1912,  0.0438],\n          ...,\n          [ 0.0564, -0.2051,  0.0923,  ..., -0.1099, -0.0588, -0.2376],\n          [ 0.0554, -0.2025,  0.0984,  ..., -0.1195, -0.0679, -0.2394],\n          [ 0.0483, -0.2048,  0.1028,  ..., -0.1257, -0.0744, -0.2376]],\n\n         [[-0.0394,  0.1225, -0.0586,  ..., -0.2961,  0.0348,  0.3568],\n          [-0.0940,  0.0031, -0.0695,  ..., -0.2152,  0.1008,  0.7539],\n          [-0.0486,  0.0154,  0.0534,  ..., -0.1308,  0.0678,  0.7294],\n          ...,\n          [-0.1599,  0.2836,  0.0244,  ..., -0.1863, -0.0334, -0.1139],\n          [-0.1600,  0.2832,  0.0311,  ..., -0.1879, -0.0312, -0.1162],\n          [-0.1619,  0.2830,  0.0353,  ..., -0.1860, -0.0319, -0.1117]],\n\n         ...,\n\n         [[ 0.0319,  0.0883, -0.4746,  ..., -0.3371,  0.0439, -0.1778],\n          [-0.2771,  0.1529,  0.0261,  ..., -0.0393,  0.0893,  0.2142],\n          [-0.0844, -0.0817, -0.0692,  ...,  0.0908, -0.0752,  0.4049],\n          ...,\n          [-0.1448,  0.4789, -0.2066,  ...,  0.1798, -0.0943, -0.3244],\n          [-0.1456,  0.4730, -0.2147,  ...,  0.1756, -0.0927, -0.3299],\n          [-0.1512,  0.4662, -0.2126,  ...,  0.1761, -0.0932, -0.3327]],\n\n         [[-0.1162, -0.1438,  0.1506,  ..., -0.1650, -0.1158,  0.3369],\n          [-0.1351, -0.0130, -0.0424,  ...,  0.0582, -0.1669,  0.4030],\n          [-0.3134,  0.0801,  0.0095,  ...,  0.0435, -0.2269,  0.2254],\n          ...,\n          [-0.0031,  0.1761, -0.1343,  ..., -0.0298, -0.1541,  0.0049],\n          [-0.0034,  0.1758, -0.1431,  ..., -0.0235, -0.1531,  0.0012],\n          [-0.0038,  0.1762, -0.1510,  ..., -0.0219, -0.1498, -0.0057]],\n\n         [[-0.0093, -0.0780, -0.0310,  ...,  0.1174, -0.0029, -0.2087],\n          [ 0.0765, -0.2952,  0.2904,  ...,  0.1097, -0.0277, -0.1875],\n          [ 0.2370, -0.1707,  0.1426,  ...,  0.1727,  0.0819, -0.1395],\n          ...,\n          [-0.2914, -0.2225,  0.1860,  ..., -0.2414, -0.2189, -0.0619],\n          [-0.2826, -0.2215,  0.1877,  ..., -0.2415, -0.2251, -0.0635],\n          [-0.2707, -0.2137,  0.1871,  ..., -0.2411, -0.2285, -0.0599]]]])), (tensor([[[[-0.2117,  0.5450,  0.6838,  ..., -0.4599,  0.2916, -0.3115],\n          [-0.1648, -0.3949,  0.2431,  ...,  0.3538,  0.4483, -0.7498],\n          [-0.2033, -1.5000, -0.1907,  ...,  0.6030,  0.1326, -0.7072],\n          ...,\n          [-0.7740,  0.4851, -0.7941,  ..., -0.2744,  0.4704, -0.9238],\n          [ 0.0802, -0.4291, -1.6198,  ..., -0.3156,  0.4695, -0.9094],\n          [ 0.8874, -1.1490, -2.1476,  ..., -0.3266,  0.4602, -0.8866]],\n\n         [[-1.6210,  1.0693, -1.8367,  ...,  0.4109,  0.0725, -1.0996],\n          [-0.3023,  0.2887, -1.0523,  ..., -0.1237, -0.7029, -1.1068],\n          [ 1.5402, -0.5539,  0.0589,  ..., -0.3102, -0.5550, -0.8124],\n          ...,\n          [ 0.1632,  2.3912,  0.3288,  ..., -1.8712,  0.3242, -0.0609],\n          [ 2.4576,  1.3568,  1.5096,  ..., -1.8689,  0.3202, -0.0303],\n          [ 2.5447, -0.2626,  2.4178,  ..., -1.8892,  0.3265, -0.0217]],\n\n         [[ 1.9214, -0.3171, -0.7328,  ..., -0.9959,  1.6619, -0.5890],\n          [ 0.1342, -0.0410, -0.9090,  ..., -0.7874,  1.1279, -0.8291],\n          [-1.8375,  0.7970, -1.1487,  ..., -0.4542,  1.7311, -0.4370],\n          ...,\n          [-0.8963, -1.5960, -1.4868,  ...,  0.6685,  1.9554, -0.4400],\n          [-2.6219, -0.2638, -0.3065,  ...,  0.6555,  1.9519, -0.4582],\n          [-1.9518,  1.1708,  0.9437,  ...,  0.6593,  1.9665, -0.4570]],\n\n         ...,\n\n         [[ 1.4680, -1.0496,  0.8311,  ...,  0.1099, -0.8792, -1.9047],\n          [ 1.5547, -0.7811,  0.9744,  ...,  0.3442, -0.9534, -1.7567],\n          [ 0.8745, -0.0169,  0.8766,  ...,  1.0728, -1.2488, -1.8820],\n          ...,\n          [ 1.6323, -0.6750,  0.9308,  ...,  0.2428,  0.3870, -2.6391],\n          [ 0.0276,  0.1140,  0.5418,  ...,  0.2410,  0.4315, -2.6455],\n          [-1.6220,  0.8529,  0.0475,  ...,  0.2409,  0.4381, -2.6451]],\n\n         [[ 2.4325,  1.1167,  0.9976,  ..., -0.3761, -0.3318,  0.4594],\n          [ 0.6916,  0.7228,  1.7487,  ..., -1.1947,  0.6959,  0.2323],\n          [-1.5393,  0.0716,  2.5674,  ..., -1.1718,  0.9444,  0.2228],\n          ...,\n          [ 0.3590,  1.4855,  2.7649,  ..., -0.5939,  0.8005,  0.4524],\n          [-2.0551,  0.8429,  2.5918,  ..., -0.5637,  0.7751,  0.4546],\n          [-2.5843, -0.1649,  1.9219,  ..., -0.5364,  0.7561,  0.4419]],\n\n         [[-1.7981,  1.5775, -0.9693,  ..., -1.1338, -0.5641, -0.3716],\n          [ 0.0236,  1.7894, -1.5076,  ..., -1.7074,  0.0039,  1.1531],\n          [ 1.7979,  1.3194, -1.9033,  ..., -1.5049,  0.1771,  0.3503],\n          ...,\n          [-0.0832,  1.7659, -1.8767,  ...,  0.9408, -0.6852,  1.0507],\n          [ 1.2463,  2.6468, -1.4908,  ...,  0.9552, -0.7171,  1.0712],\n          [ 1.4281,  2.4040, -0.8073,  ...,  0.9641, -0.7322,  1.0787]]]]), tensor([[[[-1.6211e-01,  8.3498e-01, -2.9222e-01,  ..., -1.9320e-01,\n           -7.6129e-02,  3.7315e-03],\n          [-1.1927e-01,  6.5158e-01, -3.4652e-01,  ..., -7.6090e-02,\n            1.6729e-01, -3.3027e-02],\n          [-4.7356e-02,  7.1072e-01, -3.6089e-01,  ..., -1.1676e-01,\n            3.2942e-01,  1.6957e-01],\n          ...,\n          [ 8.0825e-02,  2.6489e-01, -1.3471e-01,  ...,  6.6664e-02,\n           -1.3887e-01, -5.1246e-02],\n          [ 7.8716e-02,  2.6477e-01, -1.3433e-01,  ...,  6.2268e-02,\n           -1.4885e-01, -5.4797e-02],\n          [ 8.3926e-02,  2.6383e-01, -1.3059e-01,  ...,  6.2986e-02,\n           -1.6046e-01, -5.7253e-02]],\n\n         [[ 1.1636e-01, -3.1575e-01,  1.7552e-01,  ...,  1.1510e-01,\n           -2.4317e-02,  7.4577e-02],\n          [ 1.8230e-01, -4.3091e-01,  6.7849e-02,  ...,  1.5773e-01,\n           -1.2714e-01,  5.0970e-02],\n          [-1.2846e-01, -3.0047e-01,  7.1348e-02,  ..., -3.0659e-02,\n           -2.8378e-01, -9.4431e-02],\n          ...,\n          [ 1.7895e-01, -2.7811e-02, -1.1685e-02,  ...,  7.6549e-02,\n            1.3281e-01,  3.1468e-01],\n          [ 1.7563e-01, -2.5939e-02, -4.0550e-03,  ...,  7.5687e-02,\n            1.2906e-01,  3.1476e-01],\n          [ 1.7224e-01, -2.7997e-02,  2.6255e-03,  ...,  7.4102e-02,\n            1.1784e-01,  3.1178e-01]],\n\n         [[ 2.4831e-01,  2.4199e-01,  7.9065e-02,  ..., -3.5904e-01,\n            1.8613e-01,  4.5022e-02],\n          [ 1.0711e-01,  1.6177e-01, -1.0443e-01,  ..., -4.1550e-01,\n            4.0404e-01,  1.9775e-01],\n          [ 7.3612e-02,  1.0172e-02, -1.5318e-01,  ..., -2.4274e-01,\n            2.8017e-01,  7.9832e-02],\n          ...,\n          [-9.0411e-03,  1.1694e-01, -1.9464e-01,  ...,  2.8589e-02,\n            8.9302e-02, -3.8719e-02],\n          [-5.3653e-03,  1.2024e-01, -2.0372e-01,  ...,  2.9857e-02,\n            9.0347e-02, -3.5070e-02],\n          [-2.9994e-03,  1.2145e-01, -2.0535e-01,  ...,  3.2017e-02,\n            9.0863e-02, -3.1761e-02]],\n\n         ...,\n\n         [[-4.6560e-01,  2.1524e-02, -1.7127e-01,  ...,  3.5002e-01,\n           -1.4733e-01, -5.3751e-01],\n          [-6.5679e-02,  1.4743e-01, -6.6512e-02,  ...,  4.1395e-01,\n           -4.0281e-01, -4.9127e-01],\n          [-1.5408e-01,  2.7449e-01, -8.5480e-02,  ...,  3.4045e-01,\n           -2.6780e-01, -4.4309e-01],\n          ...,\n          [-9.1559e-01, -3.3553e-03,  3.8994e-01,  ...,  6.8244e-02,\n            1.1029e-01,  3.6151e-02],\n          [-9.2038e-01, -4.2976e-03,  3.8357e-01,  ...,  7.3412e-02,\n            1.0884e-01,  3.4033e-02],\n          [-9.2341e-01,  5.5893e-03,  3.7680e-01,  ...,  7.6174e-02,\n            1.0843e-01,  3.3785e-02]],\n\n         [[ 1.4067e-01,  2.2224e-01, -1.8394e-01,  ...,  2.7646e-03,\n           -2.1804e-01,  1.8530e-01],\n          [ 2.9923e-01, -2.1166e-03, -1.2689e-01,  ...,  5.8001e-02,\n            1.8134e-01,  3.4058e-01],\n          [ 2.3391e-01, -5.2078e-02, -1.0016e-01,  ..., -2.3674e-02,\n            1.7153e-01,  3.3578e-01],\n          ...,\n          [ 3.1662e-01,  1.5665e-01,  6.8820e-04,  ..., -4.2962e-01,\n            1.1615e-01, -1.3984e-01],\n          [ 3.0379e-01,  1.5389e-01, -4.1066e-03,  ..., -4.2436e-01,\n            1.0666e-01, -1.4857e-01],\n          [ 2.9441e-01,  1.5456e-01, -1.2141e-02,  ..., -4.1989e-01,\n            9.8769e-02, -1.5529e-01]],\n\n         [[ 5.9689e-02,  5.3050e-02,  1.6806e-01,  ..., -2.2736e-01,\n            9.1998e-02,  5.2923e-02],\n          [-1.5250e-01,  2.8924e-01,  1.6018e-01,  ..., -3.3719e-01,\n            2.0141e-01,  3.8967e-01],\n          [ 2.2106e-01,  2.7241e-01, -1.1437e-02,  ..., -2.1653e-01,\n            2.3480e-01,  2.5687e-01],\n          ...,\n          [ 1.3041e-01,  3.9082e-02, -7.0362e-02,  ..., -2.8593e-01,\n            2.8367e-01,  1.6131e-02],\n          [ 1.2599e-01,  3.7824e-02, -7.0399e-02,  ..., -2.9232e-01,\n            2.9783e-01,  2.4440e-02],\n          [ 1.2461e-01,  3.3153e-02, -7.1148e-02,  ..., -2.9721e-01,\n            3.1312e-01,  2.1484e-02]]]])), (tensor([[[[-1.3101,  1.2705,  1.2804,  ..., -0.2600, -0.9622, -3.9957],\n          [ 0.9472,  2.0581,  1.7375,  ..., -0.4113, -1.3094, -3.8085],\n          [ 2.0265,  1.7593,  2.0577,  ..., -0.7187, -1.4540, -3.7419],\n          ...,\n          [ 2.3162,  1.8236,  2.5702,  ..., -0.4085, -1.6485, -3.7055],\n          [ 2.3406,  2.5122,  1.6559,  ..., -0.4129, -1.6479, -3.7209],\n          [ 0.2205,  2.1441,  0.4110,  ..., -0.4276, -1.6435, -3.7390]],\n\n         [[-0.0329,  0.7924, -0.9401,  ...,  1.5270,  2.2281,  1.4499],\n          [-1.0613,  1.1506, -1.4941,  ...,  2.3353,  2.1059,  1.4710],\n          [-0.6493,  0.6986, -1.4912,  ...,  2.5946,  2.0066,  0.9108],\n          ...,\n          [-0.7348,  1.4812, -1.0399,  ...,  4.2282,  4.4991, -0.5246],\n          [ 0.3220,  1.3878, -0.1251,  ...,  4.2142,  4.4907, -0.5129],\n          [ 1.0723,  0.7013,  0.8237,  ...,  4.2054,  4.4587, -0.4977]],\n\n         [[-0.2355, -0.4580, -0.1212,  ...,  1.4651, -2.2542,  2.4285],\n          [ 0.1501, -0.9047, -0.5484,  ...,  2.2495, -2.8277,  1.3994],\n          [ 0.6007, -0.4400, -0.8879,  ...,  3.6846, -3.0800,  1.3988],\n          ...,\n          [ 0.5098, -0.7822, -0.8489,  ...,  1.9586, -5.5138, -3.1869],\n          [ 0.5114, -0.6903, -0.5377,  ...,  1.9988, -5.3740, -3.1871],\n          [ 0.0426, -0.3122, -0.1278,  ...,  1.9675, -5.2233, -3.1485]],\n\n         ...,\n\n         [[ 1.4583, -1.3942,  2.1450,  ..., -2.1736,  0.8207,  0.9470],\n          [ 1.8388, -0.0887,  1.8441,  ..., -2.2948,  0.2585,  0.5275],\n          [ 0.5215,  1.4966,  1.0916,  ..., -2.5141,  0.1458,  0.6750],\n          ...,\n          [ 1.7892, -0.8016, -0.6579,  ..., -2.4208, -0.2291, -0.3234],\n          [-0.6979,  0.7890, -1.6556,  ..., -2.4288, -0.2243, -0.3129],\n          [-2.5425,  2.0382, -2.3447,  ..., -2.4431, -0.2073, -0.2869]],\n\n         [[-0.9437, -1.1374, -0.1258,  ...,  0.5135, -3.3297, -0.9514],\n          [-0.6435, -0.9426, -0.9779,  ...,  0.4569, -2.8721,  0.0928],\n          [-0.1986, -0.8554, -1.1336,  ...,  0.1723, -3.2699,  0.0128],\n          ...,\n          [-1.2267, -1.2114, -0.9519,  ...,  0.0502, -3.4709,  0.9663],\n          [ 0.2878, -1.1911, -0.6219,  ...,  0.0562, -3.4646,  0.9648],\n          [ 1.5255, -0.6657, -0.1752,  ...,  0.0544, -3.4536,  0.9789]],\n\n         [[-0.2661,  1.3301,  3.0238,  ..., -0.1148,  0.4615, -0.1900],\n          [-2.1632,  1.8244,  2.1488,  ..., -0.4469, -0.2693, -0.3951],\n          [-1.8853,  1.8869,  0.8405,  ..., -0.3495, -0.1050, -0.4901],\n          ...,\n          [-2.6481,  1.6341, -0.7227,  ..., -0.7135,  0.2425, -0.4447],\n          [-0.6991,  2.5948, -2.1593,  ..., -0.7355,  0.2797, -0.4524],\n          [ 1.8652,  2.4379, -3.2018,  ..., -0.7741,  0.2939, -0.4553]]]]), tensor([[[[ 0.0845, -0.0184,  0.1812,  ...,  0.2202, -0.2657, -0.0713],\n          [-0.0775,  0.0643,  0.1919,  ...,  0.0363,  0.0225, -0.1327],\n          [-0.1003,  0.1491,  0.1097,  ...,  0.0628,  0.0127, -0.0946],\n          ...,\n          [-0.3175,  0.1574,  0.3484,  ...,  0.0042, -0.0072, -0.1039],\n          [-0.3167,  0.1606,  0.3535,  ...,  0.0041, -0.0043, -0.1033],\n          [-0.3220,  0.1640,  0.3563,  ...,  0.0037, -0.0011, -0.0979]],\n\n         [[-0.0701,  0.1409, -0.1842,  ..., -0.0018, -0.2392,  0.0511],\n          [ 0.0023,  0.1443, -0.1577,  ...,  0.0534, -0.0708,  0.0300],\n          [ 0.0106,  0.1277, -0.0950,  ..., -0.0416, -0.0573, -0.1014],\n          ...,\n          [ 0.2183,  0.0251, -0.3180,  ..., -0.1039,  0.2151,  0.1949],\n          [ 0.2165,  0.0272, -0.3154,  ..., -0.1118,  0.2221,  0.1919],\n          [ 0.2160,  0.0265, -0.3116,  ..., -0.1131,  0.2270,  0.1876]],\n\n         [[-0.0070, -0.2458, -0.2118,  ...,  0.0962,  0.5380,  0.1551],\n          [-0.0877, -0.0990, -0.2682,  ..., -0.1010,  0.5151,  0.1202],\n          [-0.1102, -0.1250, -0.3386,  ..., -0.1653,  0.3852,  0.1226],\n          ...,\n          [ 0.1642, -0.1066, -0.1008,  ..., -0.1544,  0.5056,  0.1481],\n          [ 0.1597, -0.1033, -0.1028,  ..., -0.1560,  0.5071,  0.1438],\n          [ 0.1539, -0.0939, -0.0987,  ..., -0.1581,  0.5056,  0.1403]],\n\n         ...,\n\n         [[ 0.0132,  0.0562,  0.0736,  ..., -0.0458, -0.0979, -0.0750],\n          [-0.0822, -0.1517,  0.1115,  ...,  0.1136, -0.2413, -0.0778],\n          [-0.0726, -0.2724,  0.1149,  ...,  0.0813, -0.3447, -0.1213],\n          ...,\n          [ 0.2513, -0.1122, -0.0613,  ..., -0.1083,  0.0205,  0.3023],\n          [ 0.2465, -0.1129, -0.0682,  ..., -0.1120,  0.0161,  0.3002],\n          [ 0.2446, -0.1123, -0.0746,  ..., -0.1138,  0.0133,  0.2969]],\n\n         [[ 0.4783, -0.6412, -0.2133,  ..., -0.2224,  0.0576,  0.0031],\n          [ 0.2315, -0.4100, -0.2077,  ..., -0.4046, -0.0937,  0.0519],\n          [ 0.3141, -0.3527, -0.1300,  ..., -0.3935, -0.0784,  0.0370],\n          ...,\n          [ 0.1484, -0.3880, -0.0007,  ..., -0.3562, -0.0819,  0.1652],\n          [ 0.1485, -0.3963,  0.0026,  ..., -0.3559, -0.0774,  0.1717],\n          [ 0.1476, -0.4002,  0.0031,  ..., -0.3565, -0.0712,  0.1822]],\n\n         [[-0.0945,  0.0579, -0.3334,  ..., -0.1044, -0.1373,  0.1098],\n          [-0.0483,  0.0855, -0.2361,  ..., -0.1175, -0.2187, -0.0119],\n          [-0.0334,  0.1478, -0.1905,  ..., -0.1801, -0.2732,  0.0282],\n          ...,\n          [-0.0308,  0.0520,  0.0574,  ...,  0.0266, -0.2278,  0.0142],\n          [-0.0295,  0.0525,  0.0572,  ...,  0.0348, -0.2291,  0.0195],\n          [-0.0309,  0.0555,  0.0543,  ...,  0.0368, -0.2281,  0.0224]]]])), (tensor([[[[-1.0137e+00,  4.4157e+00, -1.7180e+00,  ...,  3.9754e-01,\n            9.6427e-01,  6.1539e-01],\n          [ 3.3267e+00,  2.9945e+00, -3.3756e+00,  ..., -3.6148e-01,\n            8.1191e-01,  5.6247e-01],\n          [ 5.2620e+00,  2.3281e-01, -4.6630e+00,  ..., -3.7082e-01,\n            8.9604e-01,  7.3175e-01],\n          ...,\n          [ 6.5194e+00,  5.5351e+00, -5.5907e+00,  ..., -9.7863e-01,\n            9.7804e-02,  6.3304e-01],\n          [ 4.9465e+00,  3.2551e+00, -4.2842e+00,  ..., -9.7938e-01,\n            9.8657e-02,  6.3254e-01],\n          [-1.1672e+00, -4.1000e-01, -2.1551e+00,  ..., -9.8138e-01,\n            8.6148e-02,  6.2426e-01]],\n\n         [[ 1.7203e+00, -7.3606e-01,  5.5993e-01,  ..., -6.9716e-02,\n           -5.3799e-01, -2.8584e-01],\n          [ 1.8084e+00, -1.5946e+00, -8.7750e-02,  ..., -5.1039e-01,\n           -4.7388e-01, -5.1285e-01],\n          [ 3.3976e-01, -1.7001e+00, -1.0941e+00,  ..., -7.2102e-01,\n           -8.1473e-01, -2.5266e-01],\n          ...,\n          [ 9.6541e-01, -1.7201e+00, -1.8377e+00,  ..., -3.6978e-01,\n           -1.8586e-01,  6.7032e-01],\n          [-1.3045e+00, -1.8811e+00, -2.2536e+00,  ..., -3.6960e-01,\n           -1.7716e-01,  6.8554e-01],\n          [-2.3810e+00, -1.2446e+00, -2.2428e+00,  ..., -3.7086e-01,\n           -1.5503e-01,  6.8722e-01]],\n\n         [[ 3.5604e-01, -4.3804e-01,  6.8557e-03,  ...,  4.0096e+00,\n            3.5348e+00, -4.3216e+00],\n          [-2.4104e-01,  2.7023e-01, -4.2813e-01,  ...,  4.6408e+00,\n            3.6618e+00, -5.4070e+00],\n          [-6.2526e-01,  5.7377e-01, -1.0077e+00,  ...,  5.4988e+00,\n            3.8160e+00, -5.3449e+00],\n          ...,\n          [ 3.5950e-01, -1.1045e+00, -1.2571e+00,  ...,  5.8272e+00,\n            1.7506e+00, -1.1820e+01],\n          [-1.0871e+00, -6.0300e-01, -7.0806e-01,  ...,  5.8325e+00,\n            1.7823e+00, -1.1856e+01],\n          [-1.5495e+00,  1.6504e-01, -2.8521e-02,  ...,  5.8815e+00,\n            1.7455e+00, -1.1906e+01]],\n\n         ...,\n\n         [[ 1.8256e-01,  1.1218e+00,  1.3900e+00,  ..., -2.3299e-01,\n            2.9083e-01,  4.4763e+00],\n          [-9.7548e-01,  1.1426e+00,  1.0077e+00,  ..., -1.2144e-01,\n           -3.5427e-02,  3.9065e+00],\n          [-8.5654e-01,  6.1833e-01,  7.7965e-01,  ...,  1.4030e-01,\n           -7.9775e-01,  4.3739e+00],\n          ...,\n          [-1.5335e+00,  1.5920e+00,  5.4487e-01,  ...,  1.6113e-01,\n           -9.7385e-01,  4.1044e+00],\n          [-9.5373e-01,  1.3578e+00, -4.2474e-01,  ...,  1.5627e-01,\n           -9.6790e-01,  4.0913e+00],\n          [ 5.0556e-01,  5.4051e-01, -1.3226e+00,  ...,  1.6339e-01,\n           -9.5867e-01,  4.0834e+00]],\n\n         [[ 5.2213e-02, -1.0436e-01, -1.9928e+00,  ..., -1.0786e+00,\n            2.2591e+00, -8.9926e-01],\n          [-1.7184e+00,  8.1323e-01, -2.6243e+00,  ..., -5.9174e-01,\n            1.5464e+00, -9.1631e-01],\n          [-2.0370e+00,  1.1446e+00, -2.7397e+00,  ..., -1.1920e+00,\n            1.5132e+00, -1.1383e+00],\n          ...,\n          [-2.3371e+00,  1.1287e+00, -2.9136e+00,  ...,  1.5285e-01,\n            9.4231e-01, -1.1311e+00],\n          [-1.8858e+00,  2.4617e+00, -1.8649e+00,  ...,  1.5230e-01,\n            9.4621e-01, -1.1256e+00],\n          [ 2.6243e-01,  2.7522e+00, -4.7454e-01,  ...,  1.2527e-01,\n            9.4362e-01, -1.1230e+00]],\n\n         [[-1.4419e+00, -1.5903e+00,  3.9576e-01,  ...,  1.6484e+00,\n           -2.8943e+00,  4.5572e+00],\n          [-9.3582e-01, -1.2134e+00,  1.3226e+00,  ...,  1.3557e+00,\n           -1.5887e+00,  3.7634e+00],\n          [ 5.4439e-01, -1.7405e-01,  1.8581e+00,  ...,  1.8786e+00,\n           -1.6944e+00,  3.1762e+00],\n          ...,\n          [-3.0050e-02, -2.5769e+00,  3.4178e+00,  ...,  3.9478e+00,\n            4.6439e-01,  4.0948e+00],\n          [ 2.4397e+00, -1.3719e+00,  2.6901e+00,  ...,  3.9643e+00,\n            4.8865e-01,  4.0906e+00],\n          [ 2.6654e+00,  4.0397e-01,  1.4544e+00,  ...,  3.9773e+00,\n            5.2044e-01,  4.0877e+00]]]]), tensor([[[[ 0.1092,  0.1211,  0.1527,  ...,  0.0348,  0.1694, -0.0528],\n          [-0.0103,  0.1275,  0.1625,  ...,  0.1415,  0.0311, -0.1589],\n          [-0.0151,  0.0561,  0.0826,  ...,  0.1524,  0.0542, -0.2238],\n          ...,\n          [ 0.1259,  0.3489, -0.0695,  ..., -0.1641,  0.1778, -0.0982],\n          [ 0.1257,  0.3415, -0.0640,  ..., -0.1698,  0.1779, -0.1019],\n          [ 0.1224,  0.3356, -0.0565,  ..., -0.1748,  0.1797, -0.1035]],\n\n         [[-0.0159,  0.0895,  0.0723,  ..., -0.1371, -0.2569, -0.0892],\n          [ 0.1487, -0.0661,  0.1232,  ..., -0.2906, -0.4839, -0.0494],\n          [ 0.2231,  0.0695,  0.1369,  ..., -0.3198, -0.4166,  0.0534],\n          ...,\n          [ 0.0048, -0.1508, -0.0949,  ..., -0.5315, -0.2205,  0.0495],\n          [ 0.0074, -0.1504, -0.0926,  ..., -0.5314, -0.2195,  0.0470],\n          [ 0.0093, -0.1508, -0.0937,  ..., -0.5258, -0.2122,  0.0449]],\n\n         [[-0.1633, -0.2033, -0.1349,  ...,  0.2763,  0.0766, -0.0271],\n          [-0.0932, -0.1893, -0.1568,  ...,  0.2706,  0.0776, -0.1726],\n          [-0.1142, -0.3063, -0.1257,  ...,  0.2077,  0.0892, -0.1877],\n          ...,\n          [-0.3555, -0.3732, -0.3151,  ...,  0.7360,  0.3396, -0.0726],\n          [-0.3546, -0.3741, -0.3196,  ...,  0.7438,  0.3407, -0.0735],\n          [-0.3583, -0.3715, -0.3219,  ...,  0.7497,  0.3369, -0.0767]],\n\n         ...,\n\n         [[ 0.2904,  0.1769,  0.0708,  ..., -0.1430,  0.4042,  0.2712],\n          [ 0.2639,  0.0185,  0.0225,  ..., -0.0564,  0.3977,  0.0475],\n          [ 0.2753, -0.0255,  0.0609,  ...,  0.0019,  0.3150,  0.0240],\n          ...,\n          [ 0.0789, -0.1206, -0.0215,  ...,  0.3194,  0.2326,  0.2955],\n          [ 0.0723, -0.1164, -0.0294,  ...,  0.3129,  0.2341,  0.2931],\n          [ 0.0654, -0.1121, -0.0350,  ...,  0.3060,  0.2331,  0.2919]],\n\n         [[-0.1661, -0.0197,  0.2780,  ...,  0.0788, -0.0379, -0.0995],\n          [-0.0501, -0.0843,  0.0548,  ...,  0.1912, -0.0630, -0.1141],\n          [ 0.0746,  0.0028,  0.0849,  ...,  0.2452, -0.0221, -0.1439],\n          ...,\n          [-0.0657, -0.0263, -0.0592,  ..., -0.2250, -0.0574,  0.0431],\n          [-0.0558, -0.0204, -0.0634,  ..., -0.2288, -0.0588,  0.0387],\n          [-0.0527, -0.0194, -0.0658,  ..., -0.2338, -0.0614,  0.0380]],\n\n         [[ 0.0069, -0.0639,  0.2990,  ..., -0.1783, -0.0684,  0.0420],\n          [ 0.2549, -0.0857,  0.2754,  ..., -0.1173, -0.1130,  0.1068],\n          [ 0.1839, -0.0580,  0.2137,  ..., -0.1470, -0.0553,  0.0738],\n          ...,\n          [ 0.0525,  0.0074, -0.0903,  ..., -0.4667, -0.1028, -0.0593],\n          [ 0.0512,  0.0071, -0.0893,  ..., -0.4611, -0.0974, -0.0583],\n          [ 0.0499,  0.0079, -0.0954,  ..., -0.4529, -0.0918, -0.0579]]]])), (tensor([[[[-0.1280, -0.9510,  1.7532,  ...,  0.1850,  0.0829, -0.7993],\n          [-1.0859, -0.1770,  1.3974,  ..., -0.1125,  0.1183, -0.1471],\n          [-1.2895,  0.8042,  1.1774,  ...,  0.2404, -0.0127,  0.1848],\n          ...,\n          [-1.8513, -0.6841,  0.5660,  ..., -0.1319,  0.7769,  0.2381],\n          [-0.9257,  0.2563, -0.4170,  ..., -0.1359,  0.7839,  0.2431],\n          [ 0.8507,  1.0862, -1.3261,  ..., -0.1285,  0.7783,  0.2365]],\n\n         [[-0.4449,  2.1951,  0.3756,  ..., -3.3468, -0.4547, -1.6348],\n          [-2.2037,  1.7949,  1.7037,  ..., -3.2215, -0.3300, -1.3919],\n          [-2.1371,  0.7360,  2.2006,  ..., -2.7201,  0.1939, -1.5678],\n          ...,\n          [-2.6305,  2.7628,  1.7166,  ..., -0.8528, -3.5904, -0.2954],\n          [-0.3306,  1.4363,  1.3115,  ..., -0.8455, -3.5622, -0.3146],\n          [ 2.2570, -0.4861,  0.6573,  ..., -0.8404, -3.5353, -0.3264]],\n\n         [[ 0.2847, -1.4417, -0.2527,  ...,  1.2183, -0.3871, -1.1923],\n          [-0.0311, -0.5938, -0.9607,  ...,  1.4914, -0.4053, -1.3584],\n          [-0.2515,  0.1736, -1.3432,  ...,  1.6892, -0.0157, -1.7431],\n          ...,\n          [-0.7123, -0.7094, -0.9953,  ...,  0.0250,  0.7783, -2.0524],\n          [-0.3904, -0.1135, -0.5509,  ...,  0.0211,  0.7706, -2.0641],\n          [ 0.2707,  0.5258, -0.0100,  ...,  0.0221,  0.7729, -2.0771]],\n\n         ...,\n\n         [[ 1.3092, -1.8159,  1.0837,  ...,  0.8299, -1.8178,  0.1960],\n          [ 1.8832, -1.1895,  0.9487,  ...,  1.2251, -2.2898,  0.1439],\n          [ 0.8334,  0.0060,  0.5499,  ...,  1.0880, -2.3430,  0.3195],\n          ...,\n          [ 1.2827, -2.2495, -1.0055,  ...,  0.1982, -2.4890,  1.9391],\n          [-0.6484, -1.3072, -1.5841,  ...,  0.1809, -2.4721,  1.9218],\n          [-1.9847,  0.1846, -1.8688,  ...,  0.1319, -2.4751,  1.9105]],\n\n         [[-0.5771,  1.0311,  0.6064,  ...,  0.5785,  0.2570,  0.8342],\n          [-0.3018,  0.9639,  0.8885,  ...,  0.8952, -0.4312, -0.6310],\n          [ 0.3479,  0.6028,  1.1642,  ...,  0.8725, -0.7022, -0.8262],\n          ...,\n          [ 0.0314,  1.6526,  1.2087,  ...,  0.2290,  0.2360,  1.6066],\n          [ 1.0466,  1.3989,  0.7641,  ...,  0.2251,  0.2153,  1.6140],\n          [ 1.0915,  0.5536,  0.1735,  ...,  0.2256,  0.1646,  1.6350]],\n\n         [[ 1.6141, -2.1797, -1.5128,  ..., -1.0819, -2.5847, -2.0042],\n          [-0.1466, -1.5366, -2.1017,  ...,  0.3666, -1.7395, -1.6988],\n          [-1.6592, -0.4421, -2.5675,  ...,  0.5360, -1.2847, -1.6244],\n          ...,\n          [-0.6037, -2.4909, -1.6245,  ..., -0.5353, -3.8352, -1.4436],\n          [-2.3814, -2.0704, -0.9773,  ..., -0.5295, -3.8426, -1.4618],\n          [-1.9752, -0.7593, -0.1274,  ..., -0.5172, -3.8680, -1.4871]]]]), tensor([[[[-0.1548, -0.2264,  0.0875,  ...,  0.0242,  0.3179,  0.2924],\n          [-0.0012, -0.2923, -0.0139,  ..., -0.0674,  0.0734,  0.3293],\n          [ 0.0627, -0.1444, -0.1092,  ..., -0.1426,  0.0798,  0.3310],\n          ...,\n          [-0.0230, -0.0897,  0.1289,  ..., -0.0177,  0.1733,  0.0713],\n          [-0.0224, -0.0847,  0.1251,  ..., -0.0229,  0.1696,  0.0704],\n          [-0.0215, -0.0794,  0.1233,  ..., -0.0268,  0.1649,  0.0657]],\n\n         [[-0.0297,  0.1569, -0.2357,  ...,  0.0924, -0.1138, -0.0813],\n          [-0.0651,  0.1785, -0.1526,  ...,  0.0819, -0.1560, -0.1191],\n          [-0.0609,  0.1188,  0.0121,  ..., -0.0564, -0.1233, -0.1028],\n          ...,\n          [-0.2256,  0.3053,  0.1546,  ..., -0.0850, -0.0724,  0.1288],\n          [-0.2239,  0.2990,  0.1461,  ..., -0.0881, -0.0724,  0.1257],\n          [-0.2210,  0.2937,  0.1397,  ..., -0.0884, -0.0735,  0.1219]],\n\n         [[-0.1306, -0.1450,  0.0499,  ...,  0.0528, -0.1729,  0.0238],\n          [-0.0236, -0.1427, -0.0908,  ...,  0.0340, -0.2535,  0.0283],\n          [ 0.0389, -0.3474, -0.1393,  ..., -0.0133, -0.1028,  0.1311],\n          ...,\n          [ 0.1262, -0.1202, -0.2288,  ..., -0.3636,  0.3570,  0.1085],\n          [ 0.1220, -0.1157, -0.2350,  ..., -0.3605,  0.3517,  0.1098],\n          [ 0.1167, -0.1154, -0.2397,  ..., -0.3576,  0.3449,  0.1149]],\n\n         ...,\n\n         [[ 0.0237,  0.1799, -0.0781,  ..., -0.1631, -0.0132,  0.1074],\n          [ 0.0065,  0.2463,  0.0441,  ..., -0.0716, -0.1387,  0.1320],\n          [ 0.0432,  0.1713, -0.0370,  ..., -0.0947, -0.1782,  0.1280],\n          ...,\n          [-0.0545, -0.0854, -0.1524,  ..., -0.1397, -0.3575, -0.2727],\n          [-0.0504, -0.0855, -0.1467,  ..., -0.1343, -0.3595, -0.2735],\n          [-0.0445, -0.0852, -0.1374,  ..., -0.1301, -0.3606, -0.2755]],\n\n         [[ 0.0464, -0.1460, -0.4164,  ...,  0.0352, -0.0028,  0.1789],\n          [ 0.0352, -0.1378, -0.3630,  ...,  0.1578, -0.0170,  0.2323],\n          [ 0.1232,  0.0009, -0.3120,  ...,  0.2162,  0.1383,  0.2497],\n          ...,\n          [-0.0748,  0.0038, -0.2731,  ...,  0.0789, -0.2454,  0.2263],\n          [-0.0776,  0.0055, -0.2701,  ...,  0.0789, -0.2427,  0.2244],\n          [-0.0807,  0.0036, -0.2659,  ...,  0.0778, -0.2370,  0.2228]],\n\n         [[ 0.1709, -0.0807, -0.0843,  ...,  0.0088,  0.2820,  0.1143],\n          [ 0.3072, -0.0863, -0.1720,  ...,  0.0377,  0.3377,  0.0216],\n          [ 0.2621, -0.0574, -0.0594,  ..., -0.1218,  0.3006,  0.0495],\n          ...,\n          [ 0.1726, -0.1584, -0.0216,  ...,  0.2465,  0.2220, -0.2514],\n          [ 0.1716, -0.1524, -0.0211,  ...,  0.2369,  0.2196, -0.2576],\n          [ 0.1705, -0.1453, -0.0146,  ...,  0.2285,  0.2178, -0.2599]]]])), (tensor([[[[ 0.8957,  0.2331, -0.9429,  ...,  0.5679,  0.0846, -1.4459],\n          [-0.2450,  0.9826, -0.8836,  ..., -0.1166,  0.1326, -2.2957],\n          [-1.1205,  1.5119, -0.9515,  ..., -0.3631, -0.3003, -2.5254],\n          ...,\n          [-1.1882,  1.4963, -0.4116,  ...,  1.7296, -0.8304,  2.1780],\n          [-1.5867,  2.0856,  0.2600,  ...,  1.7204, -0.8267,  2.1613],\n          [-0.5429,  1.7927,  0.8787,  ...,  1.6968, -0.8307,  2.1479]],\n\n         [[-0.7172,  1.4013, -1.0483,  ..., -1.0669,  0.2036, -2.0154],\n          [-1.8143,  0.3823, -1.7357,  ..., -1.0808,  0.2746, -2.1683],\n          [-1.2710, -0.4286, -2.2631,  ..., -1.0916, -0.5733, -1.9142],\n          ...,\n          [-1.9694,  1.5509, -2.0649,  ..., -1.2221,  0.1765, -1.8918],\n          [-0.5639,  0.4133, -1.4274,  ..., -1.2230,  0.1702, -1.9025],\n          [ 1.3516, -0.9082, -0.5154,  ..., -1.2308,  0.1799, -1.9086]],\n\n         [[-2.2334, -2.6356,  2.4535,  ..., -0.4260,  1.5944,  1.1476],\n          [-2.1230, -1.2962,  2.1035,  ..., -0.1813,  1.5570,  0.3389],\n          [ 0.0802,  0.3584,  1.2363,  ...,  0.1069,  2.2045,  0.3384],\n          ...,\n          [-1.1679, -2.2151,  0.2812,  ...,  0.2113,  1.8153, -0.2891],\n          [ 1.4149, -0.6060, -0.9251,  ...,  0.2012,  1.8037, -0.3092],\n          [ 2.7078,  1.2734, -1.9648,  ...,  0.1899,  1.7911, -0.3182]],\n\n         ...,\n\n         [[ 0.9498, -0.7173, -1.1277,  ..., -4.2796, -2.5957, -1.6498],\n          [ 0.4480, -0.0261, -0.7638,  ..., -3.4088, -2.1354, -2.6065],\n          [-0.5914,  0.8730, -0.2922,  ..., -3.5228, -2.2582, -2.1345],\n          ...,\n          [ 0.0175, -0.9944,  0.7535,  ..., -7.7803, -1.3302, -1.7957],\n          [-1.7892,  0.3966,  1.2764,  ..., -7.7538, -1.3270, -1.7460],\n          [-1.9307,  1.6080,  1.5546,  ..., -7.7116, -1.3116, -1.6918]],\n\n         [[ 4.4547,  2.0166,  2.0195,  ..., -0.1864,  0.3628,  1.1634],\n          [ 0.6003,  0.3128,  1.9669,  ..., -0.5850,  0.7535,  1.3289],\n          [-4.2471, -2.1132,  1.6593,  ..., -0.5638,  1.3780,  1.6219],\n          ...,\n          [-1.4872,  2.4813, -0.5501,  ..., -1.4270,  2.1202,  0.9284],\n          [-5.9881,  0.0395, -1.7005,  ..., -1.4063,  2.1097,  0.9200],\n          [-4.9986, -2.4200, -2.5292,  ..., -1.3693,  2.0711,  0.9219]],\n\n         [[ 1.4197,  1.1791, -0.6261,  ..., -2.7858, -2.2587,  0.2128],\n          [ 0.5481,  1.0282, -1.4252,  ..., -1.8090, -2.4000,  0.6957],\n          [-1.0913,  0.3079, -1.7866,  ..., -1.4190, -1.8695,  0.5398],\n          ...,\n          [-0.2496,  1.8494, -0.6196,  ..., -1.2746, -0.3561,  2.2179],\n          [-1.5140,  1.3500,  0.1857,  ..., -1.3259, -0.3629,  2.2511],\n          [-1.3709,  0.2926,  0.9643,  ..., -1.3403, -0.3610,  2.2787]]]]), tensor([[[[-2.4863e-01,  4.0201e-02,  9.0707e-02,  ...,  4.5013e-01,\n            1.0700e-04,  1.5977e-02],\n          [-1.1868e-01,  2.3174e-01,  1.0525e-02,  ...,  8.5431e-02,\n           -1.3782e-01,  2.6077e-02],\n          [-1.1404e-01,  2.9344e-01,  9.3679e-02,  ...,  7.8499e-03,\n           -2.0987e-01,  2.2180e-03],\n          ...,\n          [-3.0101e-02,  3.8506e-01,  3.9456e-02,  ..., -8.9584e-02,\n            2.4132e-01, -3.2023e-01],\n          [-2.7133e-02,  3.7945e-01,  4.0319e-02,  ..., -8.7585e-02,\n            2.4471e-01, -3.1586e-01],\n          [-2.6396e-02,  3.7233e-01,  4.1478e-02,  ..., -9.0276e-02,\n            2.4680e-01, -3.1269e-01]],\n\n         [[-1.2105e-01, -1.8744e-01,  2.9309e-01,  ...,  8.1710e-03,\n           -4.8349e-01, -2.4157e-02],\n          [-5.0285e-02, -3.9093e-01,  3.3773e-01,  ..., -2.8237e-02,\n           -4.5804e-01, -9.6748e-02],\n          [-6.7781e-02, -1.0231e-01,  3.6558e-01,  ...,  1.5810e-01,\n           -4.3324e-01, -4.6568e-02],\n          ...,\n          [ 6.7103e-02,  1.5667e-01,  2.3717e-01,  ...,  2.2176e-02,\n           -4.7129e-01,  8.1270e-02],\n          [ 6.7774e-02,  1.5682e-01,  2.3740e-01,  ...,  2.2974e-02,\n           -4.7293e-01,  8.3305e-02],\n          [ 6.8012e-02,  1.5527e-01,  2.3872e-01,  ...,  2.2728e-02,\n           -4.7255e-01,  8.3164e-02]],\n\n         [[-9.1847e-02, -4.3843e-02, -4.1665e-01,  ...,  4.3533e-02,\n            1.7054e-01, -4.1861e-01],\n          [-5.8217e-02, -1.0356e-01, -5.6664e-01,  ..., -6.2769e-02,\n           -6.0349e-02, -2.6491e-01],\n          [-1.0375e-01, -2.2637e-02, -5.6511e-01,  ...,  2.1993e-02,\n           -9.8388e-02, -3.6867e-01],\n          ...,\n          [-3.0536e-01, -1.1477e-01, -7.1796e-01,  ...,  1.3132e-01,\n           -1.1247e-02, -3.4477e-01],\n          [-3.0784e-01, -1.1526e-01, -7.1410e-01,  ...,  1.3625e-01,\n           -1.0228e-02, -3.4204e-01],\n          [-3.1295e-01, -1.1239e-01, -7.1467e-01,  ...,  1.3855e-01,\n           -1.0302e-02, -3.4002e-01]],\n\n         ...,\n\n         [[-2.0323e-01, -1.7657e-01,  2.4017e-02,  ...,  7.7215e-02,\n           -1.1170e-01, -7.3015e-03],\n          [-1.7649e-01, -2.0320e-01,  6.2946e-02,  ...,  6.1706e-02,\n           -2.8494e-02, -2.6944e-02],\n          [-1.5012e-01, -1.4068e-01,  9.7909e-02,  ...,  3.7997e-02,\n           -9.2198e-02, -6.7018e-02],\n          ...,\n          [-1.3027e-01, -4.8744e-02, -1.9091e-01,  ...,  1.4651e-01,\n            7.3837e-02, -1.6111e-01],\n          [-1.3138e-01, -4.6028e-02, -1.9801e-01,  ...,  1.4675e-01,\n            6.8963e-02, -1.6484e-01],\n          [-1.2701e-01, -4.4196e-02, -2.0167e-01,  ...,  1.4700e-01,\n            6.2259e-02, -1.6682e-01]],\n\n         [[-8.8952e-02,  5.8414e-02,  5.0009e-02,  ..., -2.0886e-01,\n           -1.3522e-01, -1.1300e-02],\n          [-8.8689e-02,  4.1818e-02,  4.0824e-03,  ..., -2.1616e-01,\n            1.9818e-01, -7.1565e-02],\n          [-1.1829e-02,  3.4363e-02,  7.2085e-02,  ..., -2.6775e-01,\n            5.7541e-02, -8.0138e-02],\n          ...,\n          [ 5.7997e-02, -1.3187e-01,  4.1623e-02,  ..., -1.6027e-01,\n            2.3390e-01, -3.8136e-02],\n          [ 5.8310e-02, -1.3063e-01,  4.3108e-02,  ..., -1.5460e-01,\n            2.3734e-01, -3.9778e-02],\n          [ 5.8937e-02, -1.2994e-01,  4.6305e-02,  ..., -1.4948e-01,\n            2.4206e-01, -3.8427e-02]],\n\n         [[ 6.9616e-02,  3.5096e-01, -2.8020e-01,  ...,  2.1938e-01,\n           -1.2947e-01,  1.0258e-01],\n          [-3.5357e-02,  2.4474e-01, -2.1503e-02,  ...,  4.8562e-01,\n            6.1675e-03,  3.0179e-02],\n          [-1.9834e-01,  1.6990e-01, -2.1386e-01,  ...,  4.2387e-01,\n           -7.5875e-02,  1.1272e-02],\n          ...,\n          [ 2.4119e-01,  1.5126e-01,  2.3082e-01,  ...,  8.8281e-01,\n           -9.8562e-02,  9.7885e-01],\n          [ 2.4121e-01,  1.5669e-01,  2.3156e-01,  ...,  8.8261e-01,\n           -9.9172e-02,  9.8093e-01],\n          [ 2.3833e-01,  1.6440e-01,  2.3104e-01,  ...,  8.8477e-01,\n           -1.0071e-01,  9.8179e-01]]]])), (tensor([[[[-2.4736, -2.5335,  1.8967,  ..., -0.0520, -2.2309, -0.1355],\n          [-1.0860, -1.3082,  2.1056,  ...,  0.5724, -1.5817, -0.1718],\n          [ 1.2211,  0.5841,  2.0011,  ...,  0.9570, -1.2428, -0.5349],\n          ...,\n          [-0.3315, -2.6500,  0.7570,  ...,  2.4722, -0.5201, -0.0221],\n          [ 2.3279, -1.4239, -0.0657,  ...,  2.4494, -0.5175, -0.0381],\n          [ 2.8564,  0.4116, -0.8698,  ...,  2.4165, -0.5114, -0.0409]],\n\n         [[-0.3524,  1.7712, -0.1707,  ..., -1.9832, -1.2792,  1.9541],\n          [-2.6257,  1.3351, -1.6100,  ..., -1.5909, -1.0814,  2.1642],\n          [-2.7603,  0.3277, -2.5796,  ..., -1.8305, -1.1489,  2.1267],\n          ...,\n          [-2.8603,  2.4849, -2.8417,  ..., -1.6427, -1.9791,  1.7369],\n          [-1.5379,  1.6587, -2.5092,  ..., -1.6489, -1.9558,  1.7527],\n          [ 1.1900,  0.1262, -1.7110,  ..., -1.6555, -1.9345,  1.7705]],\n\n         [[-0.0960,  1.6004,  1.1308,  ..., -1.3089,  0.3479,  4.1666],\n          [ 1.6647,  1.8336,  1.7841,  ..., -2.0047, -1.4758,  4.1126],\n          [ 2.1602,  1.4373,  2.2886,  ..., -2.2239, -1.5075,  4.2478],\n          ...,\n          [ 2.5460,  1.8181,  1.9534,  ..., -3.0148, -0.3985,  2.5506],\n          [ 0.6111,  1.4063,  1.4721,  ..., -3.0020, -0.3846,  2.5660],\n          [-1.9065,  0.3901,  0.7089,  ..., -3.0040, -0.3723,  2.5730]],\n\n         ...,\n\n         [[ 0.6043,  1.6215,  1.7838,  ..., -0.4998, -5.8151, -1.1530],\n          [ 2.1476,  1.4897,  1.0304,  ..., -1.2569, -6.0682, -1.3997],\n          [ 2.0249,  0.7490,  0.5614,  ..., -1.5325, -5.3901, -1.3917],\n          ...,\n          [ 2.0695,  1.4819, -0.4058,  ..., -3.2884, -3.6244, -1.2046],\n          [ 0.7960,  1.6718, -1.3721,  ..., -3.2851, -3.6248, -1.2004],\n          [-1.2114,  1.1604, -2.0883,  ..., -3.2831, -3.6230, -1.1948]],\n\n         [[-1.8023, -0.9646,  1.5714,  ..., -1.3732,  4.1091,  0.1643],\n          [-0.1812, -2.0131,  1.2804,  ..., -0.8257,  3.5748,  0.8689],\n          [ 1.6413, -2.2431,  1.3293,  ..., -0.8152,  3.0486,  0.3540],\n          ...,\n          [ 0.5651, -1.1895,  0.0319,  ...,  0.3144,  2.6424,  0.4258],\n          [ 1.8975, -1.7919, -0.7663,  ...,  0.2935,  2.6360,  0.4204],\n          [ 1.5092, -1.6508, -1.4240,  ...,  0.2594,  2.6359,  0.4085]],\n\n         [[ 0.3238,  0.1515,  4.7405,  ...,  0.9231,  1.0603, -0.3604],\n          [ 3.8983,  2.0108,  3.3199,  ...,  0.9444,  1.3674,  0.0797],\n          [ 4.4800,  3.2353,  1.6360,  ...,  1.2545,  1.4230, -0.0613],\n          ...,\n          [ 5.7053,  1.5392, -1.6860,  ...,  2.7584,  1.9524,  0.5590],\n          [ 2.6918,  3.3010, -3.5729,  ...,  2.7530,  1.9567,  0.5565],\n          [-2.7893,  3.6966, -4.7733,  ...,  2.7545,  1.9586,  0.5554]]]]), tensor([[[[-0.8157, -0.1992, -0.1327,  ...,  0.0103,  0.3709,  0.0762],\n          [-0.5525, -0.4289, -0.1429,  ...,  0.1102,  0.5585,  0.2379],\n          [-0.3561, -0.1561, -0.0420,  ...,  0.1444,  0.3913,  0.0323],\n          ...,\n          [-0.0945,  0.1373,  0.0799,  ..., -0.1776, -0.1227, -0.0328],\n          [-0.0930,  0.1392,  0.0809,  ..., -0.1789, -0.1280, -0.0332],\n          [-0.0930,  0.1378,  0.0835,  ..., -0.1838, -0.1313, -0.0332]],\n\n         [[-0.1893, -0.1565,  0.0853,  ..., -0.0340,  0.1901, -0.1290],\n          [-0.3471, -0.1231,  0.1092,  ...,  0.0922,  0.0516, -0.1379],\n          [-0.2758, -0.3155,  0.0647,  ...,  0.1441, -0.0343, -0.0821],\n          ...,\n          [-0.1305,  0.0189, -0.1709,  ..., -0.0938,  0.0247,  0.0237],\n          [-0.1274,  0.0175, -0.1726,  ..., -0.0930,  0.0256,  0.0212],\n          [-0.1276,  0.0157, -0.1757,  ..., -0.0949,  0.0276,  0.0178]],\n\n         [[ 0.0657,  0.1529, -0.1009,  ...,  0.3439, -0.2911, -0.2069],\n          [ 0.2036,  0.0090, -0.0712,  ...,  0.1402, -0.3194, -0.1336],\n          [ 0.4183,  0.0101,  0.0834,  ...,  0.0816, -0.1244, -0.1434],\n          ...,\n          [ 0.3608,  0.1828, -0.1941,  ...,  0.0948, -0.0664, -0.0420],\n          [ 0.3606,  0.1817, -0.1939,  ...,  0.0926, -0.0681, -0.0369],\n          [ 0.3556,  0.1783, -0.1967,  ...,  0.0950, -0.0682, -0.0324]],\n\n         ...,\n\n         [[ 0.1226, -0.1771,  0.4907,  ..., -0.0474, -0.1205, -0.1344],\n          [ 0.1100, -0.1151,  0.3665,  ..., -0.1541,  0.0785,  0.0634],\n          [ 0.1577, -0.1024,  0.5210,  ..., -0.3108,  0.2913,  0.0908],\n          ...,\n          [-0.0729,  0.0339,  0.0231,  ..., -0.0752,  0.2346,  0.0147],\n          [-0.0718,  0.0369,  0.0211,  ..., -0.0710,  0.2373,  0.0171],\n          [-0.0700,  0.0395,  0.0216,  ..., -0.0700,  0.2372,  0.0198]],\n\n         [[-0.4490, -0.0198, -0.0548,  ...,  0.1507,  0.4929,  0.1063],\n          [-0.2227, -0.1578, -0.0490,  ...,  0.1623,  0.4421,  0.1379],\n          [ 0.0319, -0.1794,  0.0189,  ...,  0.1777,  0.4082,  0.3232],\n          ...,\n          [-0.0685, -0.2350,  0.0840,  ..., -0.2558,  0.2856,  0.3544],\n          [-0.0853, -0.2355,  0.0768,  ..., -0.2606,  0.2881,  0.3547],\n          [-0.1000, -0.2431,  0.0740,  ..., -0.2632,  0.2869,  0.3504]],\n\n         [[-0.0576,  0.4143,  0.1708,  ..., -0.1432,  0.1554, -0.1770],\n          [ 0.0258,  0.2765,  0.0814,  ..., -0.0234,  0.2103, -0.0771],\n          [-0.0613,  0.0280,  0.1447,  ...,  0.1717,  0.2699,  0.0300],\n          ...,\n          [-0.1625,  0.1140, -0.0643,  ...,  0.1928,  0.1762,  0.0025],\n          [-0.1623,  0.1121, -0.0644,  ...,  0.1914,  0.1766,  0.0069],\n          [-0.1608,  0.1091, -0.0668,  ...,  0.1902,  0.1739,  0.0105]]]])), (tensor([[[[-3.4106,  1.0499,  2.1180,  ..., -0.0299, -3.5771,  0.0979],\n          [-2.4761, -0.5007,  2.9037,  ...,  1.4443, -3.0205, -1.2534],\n          [ 0.7560, -2.0753,  3.2895,  ...,  1.9819, -2.4702, -1.8797],\n          ...,\n          [ 1.0660,  1.7131,  1.2053,  ..., -2.4139, -1.5094, -0.5307],\n          [ 3.7309, -0.3421,  0.1625,  ..., -2.4081, -1.5175, -0.5284],\n          [ 2.9782, -2.2610, -0.9289,  ..., -2.4206, -1.5096, -0.5829]],\n\n         [[ 0.4332, -1.4495, -2.3355,  ..., -0.5812,  1.4042, -0.3319],\n          [-1.9800, -1.3695, -2.0322,  ..., -0.4363,  1.7990, -0.1369],\n          [-2.7641, -0.1379, -1.1524,  ..., -0.1281,  1.6947, -0.2166],\n          ...,\n          [-2.3390, -1.4640,  0.8702,  ...,  0.0062,  1.3089, -0.1237],\n          [-1.7926, -1.0847,  1.6607,  ...,  0.0081,  1.3137, -0.1214],\n          [ 0.4001, -0.2373,  2.1439,  ...,  0.0206,  1.3223, -0.1180]],\n\n         [[ 2.3532,  1.9742, -1.1683,  ...,  1.0767, -4.9489, -2.3164],\n          [ 1.4935,  1.8074,  0.0731,  ...,  1.4260, -3.2244, -1.3030],\n          [-0.7067,  0.9581,  0.7374,  ...,  0.3979, -2.1111,  0.0736],\n          ...,\n          [ 0.3775,  1.5796,  2.4960,  ...,  0.2420, -0.9845, -1.1891],\n          [-1.4427,  1.5786,  2.6396,  ...,  0.2143, -1.0190, -1.2146],\n          [-1.9613,  0.9165,  2.2842,  ...,  0.1682, -1.0641, -1.2309]],\n\n         ...,\n\n         [[ 0.2111,  0.2004, -0.2790,  ...,  1.3884,  4.2023, -2.1922],\n          [-0.8641,  1.0801, -0.1423,  ...,  2.7304,  4.5415, -1.6090],\n          [-1.3696,  1.5243,  0.3886,  ...,  4.0016,  4.1237, -1.0394],\n          ...,\n          [-0.9894,  0.4325,  1.3535,  ...,  5.3493,  4.6682, -1.6788],\n          [-0.4451,  1.1297,  1.3213,  ...,  5.3243,  4.6468, -1.6959],\n          [ 0.5155,  1.3469,  1.0312,  ...,  5.3027,  4.6290, -1.6720]],\n\n         [[-0.6615,  0.1254,  0.6223,  ...,  2.8721, -2.9146,  2.6436],\n          [-1.4010,  0.2597,  0.3427,  ...,  3.9805, -1.4897,  2.3435],\n          [-0.9686,  0.8518, -0.0639,  ...,  3.3741, -0.6833,  2.7064],\n          ...,\n          [-1.0405,  0.8379, -0.9713,  ...,  4.6129, -1.8055, -0.5844],\n          [ 0.3077,  1.2532, -1.2642,  ...,  4.6138, -1.8513, -0.5941],\n          [ 1.3723,  1.1344, -1.3180,  ...,  4.6199, -1.9057, -0.6230]],\n\n         [[-1.2706, -0.0587,  0.3426,  ...,  2.0528, -3.9760,  0.7072],\n          [ 0.3355,  1.2722, -1.1102,  ...,  2.2935, -4.1062,  1.2674],\n          [ 1.8727,  2.4962, -1.8402,  ...,  2.4661, -3.7711,  1.5973],\n          ...,\n          [ 0.9533, -0.2785, -1.8717,  ...,  2.3798, -5.7513,  0.8330],\n          [ 1.7471,  1.2743, -2.1889,  ...,  2.3797, -5.7471,  0.8212],\n          [ 0.9488,  2.3038, -2.0991,  ...,  2.3804, -5.7419,  0.8128]]]]), tensor([[[[ 2.3856e-01, -3.0472e-01,  2.8810e-01,  ...,  3.0858e-02,\n            2.3993e-02,  6.8173e-02],\n          [ 4.2594e-01, -5.7040e-01,  2.9383e-01,  ..., -3.1920e-02,\n            2.6308e-01,  2.7511e-01],\n          [ 2.6045e-01, -6.7604e-01,  5.7550e-01,  ..., -7.6957e-02,\n            2.0616e-01,  8.8157e-03],\n          ...,\n          [ 6.0206e-01, -3.9913e-01,  3.6707e-02,  ..., -8.8560e-02,\n           -6.4923e-04,  1.3322e-01],\n          [ 6.0615e-01, -3.8985e-01,  2.9323e-02,  ..., -8.9672e-02,\n           -4.7515e-03,  1.4339e-01],\n          [ 6.0973e-01, -3.8540e-01,  2.6796e-02,  ..., -9.0174e-02,\n           -6.9600e-03,  1.5104e-01]],\n\n         [[-2.6131e-01,  1.6237e-01, -1.2922e-01,  ...,  4.0854e-01,\n           -1.3952e-01,  2.9603e-01],\n          [-1.2719e-01,  2.9617e-01, -2.6759e-01,  ...,  3.1560e-01,\n           -1.4871e-01,  4.6742e-01],\n          [-1.3028e-01,  3.4659e-01, -4.3184e-01,  ...,  4.0803e-01,\n           -1.0764e-01,  3.3824e-01],\n          ...,\n          [ 1.1471e-01,  2.4450e-01,  3.4014e-02,  ..., -1.3279e-01,\n            1.7701e-01,  2.2641e-01],\n          [ 1.1577e-01,  2.5001e-01,  3.6068e-02,  ..., -1.2715e-01,\n            1.7704e-01,  2.3552e-01],\n          [ 1.1238e-01,  2.5527e-01,  4.0279e-02,  ..., -1.2229e-01,\n            1.7867e-01,  2.4137e-01]],\n\n         [[-2.6701e-01, -8.2835e-02, -2.5189e-01,  ...,  2.4560e-01,\n           -1.9601e-01,  3.5502e-01],\n          [-1.4222e-01, -6.6809e-02, -2.2068e-01,  ...,  9.6034e-02,\n           -1.7178e-01, -6.0056e-03],\n          [-1.8108e-01, -1.9057e-02, -1.4666e-01,  ...,  2.9247e-01,\n           -1.0094e-01, -1.3298e-01],\n          ...,\n          [-3.8544e-02,  5.2394e-02,  1.4115e-01,  ...,  1.3232e-01,\n            4.7022e-02, -1.7273e-01],\n          [-3.6659e-02,  5.1704e-02,  1.4057e-01,  ...,  1.3520e-01,\n            4.3901e-02, -1.7424e-01],\n          [-3.3228e-02,  5.3919e-02,  1.3720e-01,  ...,  1.3762e-01,\n            4.0304e-02, -1.7762e-01]],\n\n         ...,\n\n         [[ 7.6982e-02,  8.3019e-02, -3.1472e-01,  ...,  2.0498e-01,\n           -1.0665e-01,  6.3213e-02],\n          [-8.4412e-02, -8.9224e-02, -3.7230e-01,  ...,  1.4514e-01,\n           -2.7479e-01, -2.6880e-02],\n          [-1.1743e-01,  5.9228e-03, -2.8103e-01,  ...,  6.3351e-02,\n           -1.1331e-02,  2.6685e-02],\n          ...,\n          [-1.0105e-01,  2.2267e-03, -4.2285e-01,  ...,  4.0679e-03,\n            1.6468e-01, -2.2940e-02],\n          [-9.8415e-02,  5.7241e-03, -4.2406e-01,  ...,  3.4909e-04,\n            1.6558e-01, -2.2864e-02],\n          [-9.8605e-02,  1.0659e-02, -4.2631e-01,  ..., -1.4350e-03,\n            1.6737e-01, -2.7172e-02]],\n\n         [[ 8.8116e-02, -1.6322e-01,  2.5243e-01,  ...,  6.7136e-02,\n           -3.4085e-01, -5.0152e-01],\n          [ 3.7669e-01, -2.2342e-01,  1.9574e-01,  ...,  1.9971e-01,\n           -1.8441e-01, -1.8993e-01],\n          [ 2.2921e-01, -1.8263e-01,  1.2437e-01,  ...,  2.8361e-01,\n           -1.2955e-01,  1.8918e-02],\n          ...,\n          [-1.8520e-01,  4.1816e-02,  4.6134e-01,  ...,  1.3229e-01,\n           -2.2606e-01,  2.4643e-01],\n          [-1.9258e-01,  4.0244e-02,  4.6405e-01,  ...,  1.3355e-01,\n           -2.3001e-01,  2.4300e-01],\n          [-1.9569e-01,  3.7590e-02,  4.5784e-01,  ...,  1.3712e-01,\n           -2.3533e-01,  2.4959e-01]],\n\n         [[ 1.6388e-01,  7.3227e-02, -9.4476e-02,  ..., -3.2920e-01,\n            6.1955e-02,  1.8873e-01],\n          [ 3.2282e-02,  6.2711e-02, -8.2350e-02,  ..., -3.6368e-01,\n            4.1781e-02,  8.2280e-02],\n          [ 1.2985e-01, -1.5664e-02, -1.7662e-01,  ..., -2.9198e-01,\n           -3.2448e-03, -1.3859e-02],\n          ...,\n          [ 1.3707e-01, -5.3870e-02, -2.7453e-01,  ..., -9.0120e-02,\n           -4.9835e-02,  3.5812e-02],\n          [ 1.3931e-01, -5.5386e-02, -2.7577e-01,  ..., -9.2937e-02,\n           -5.4621e-02,  3.3765e-02],\n          [ 1.4021e-01, -5.5854e-02, -2.7716e-01,  ..., -9.5038e-02,\n           -5.7089e-02,  2.9965e-02]]]])), (tensor([[[[ 1.3407e+00, -7.0045e-01,  1.0253e+00,  ..., -8.3018e-03,\n           -2.3384e+00,  1.3265e+00],\n          [ 1.5544e+00, -1.5396e+00,  1.6718e+00,  ..., -6.7238e-02,\n           -1.9228e+00,  1.5528e+00],\n          [ 4.3868e-01, -2.2026e+00,  1.7333e+00,  ..., -2.9135e-02,\n           -2.2118e+00,  1.4436e+00],\n          ...,\n          [ 1.2461e+00, -1.1109e+00,  1.7290e+00,  ..., -2.9020e-01,\n           -1.4535e+00,  1.1044e+00],\n          [-2.5643e-01, -1.0228e+00,  1.5938e+00,  ..., -2.8167e-01,\n           -1.4586e+00,  1.1221e+00],\n          [-1.5160e+00, -5.0717e-01,  1.1682e+00,  ..., -2.8745e-01,\n           -1.4626e+00,  1.1440e+00]],\n\n         [[-1.7816e+00, -7.0839e-02,  1.9956e+00,  ...,  2.7365e+00,\n           -2.4669e+00,  3.7156e+00],\n          [-7.6615e-01,  1.6787e-01,  1.9736e+00,  ...,  4.2237e+00,\n           -3.0170e+00,  3.9396e+00],\n          [ 6.9597e-01,  3.6268e-01,  1.7757e+00,  ...,  4.1445e+00,\n           -2.8531e+00,  3.4990e+00],\n          ...,\n          [ 4.2515e-01,  3.2255e-01,  3.7724e-01,  ...,  2.6729e+00,\n           -2.3092e+00,  2.4777e+00],\n          [ 1.4727e+00,  2.2530e-01, -4.2734e-01,  ...,  2.6757e+00,\n           -2.2984e+00,  2.4494e+00],\n          [ 1.1599e+00,  2.5155e-02, -1.1458e+00,  ...,  2.6741e+00,\n           -2.2380e+00,  2.4246e+00]],\n\n         [[ 2.0480e+00, -1.2544e+00,  3.4410e+00,  ...,  3.0713e+00,\n            6.9559e-01,  9.6036e-01],\n          [ 6.8820e+00, -4.2120e+00,  3.7127e+00,  ...,  2.6279e+00,\n            6.6500e-01,  5.3777e-03],\n          [ 5.7609e+00, -5.0168e+00,  3.2188e+00,  ...,  1.9486e+00,\n            6.2841e-01,  6.8795e-02],\n          ...,\n          [ 7.1724e+00, -1.8147e+00,  1.1738e+00,  ...,  2.0526e+00,\n            6.4824e-01, -1.3099e+00],\n          [ 2.8520e+00, -4.0787e+00, -3.8133e-01,  ...,  2.0732e+00,\n            6.7338e-01, -1.3337e+00],\n          [-4.0858e+00, -4.6005e+00, -1.8667e+00,  ...,  2.0673e+00,\n            6.8926e-01, -1.3448e+00]],\n\n         ...,\n\n         [[ 1.0761e+00,  1.7999e+00, -1.7825e+00,  ...,  1.4231e+00,\n           -9.0796e-01, -6.1552e+00],\n          [-1.2737e+00,  2.6877e+00, -1.8716e+00,  ..., -2.3365e-02,\n           -7.8575e-01, -5.6314e+00],\n          [-2.1554e+00,  2.2663e+00, -1.8445e+00,  ..., -2.0539e-01,\n           -1.3193e+00, -5.0470e+00],\n          ...,\n          [-1.4990e+00,  1.4810e+00, -3.3815e-01,  ...,  8.2516e-01,\n           -1.6497e+00, -5.4177e+00],\n          [-1.1234e+00,  1.7532e+00,  2.5342e-01,  ...,  8.0808e-01,\n           -1.6307e+00, -5.4368e+00],\n          [ 2.7959e-01,  1.2855e+00,  7.8366e-01,  ...,  7.8354e-01,\n           -1.5936e+00, -5.4636e+00]],\n\n         [[-2.0378e+00,  2.2665e+00, -2.2707e+00,  ..., -2.0060e+00,\n           -3.4920e-01, -3.8672e-01],\n          [ 2.7155e-01,  1.9996e+00, -2.7253e+00,  ..., -3.8283e-03,\n           -1.5380e+00,  1.2156e-01],\n          [ 1.8565e+00,  3.9796e-01, -2.8149e+00,  ...,  1.5155e-01,\n           -1.2225e+00,  7.8005e-01],\n          ...,\n          [ 8.2032e-01,  2.4309e+00, -1.0762e+00,  ...,  8.3078e-01,\n           -1.3483e-02,  2.0662e+00],\n          [ 2.3574e+00,  2.0574e+00, -1.4411e-01,  ...,  8.1532e-01,\n            7.8891e-04,  2.0726e+00],\n          [ 1.7337e+00,  8.1486e-01,  8.1333e-01,  ...,  8.0618e-01,\n            2.2414e-02,  2.0750e+00]],\n\n         [[ 4.1885e+00, -2.2261e+00, -4.3329e-01,  ..., -1.9247e+00,\n           -5.6252e+00,  1.3198e+00],\n          [ 2.5977e+00, -4.6917e+00,  1.1455e+00,  ..., -1.4310e+00,\n           -5.3578e+00,  1.5267e+00],\n          [-1.2545e+00, -4.8836e+00,  2.6547e+00,  ..., -1.6115e+00,\n           -5.5659e+00,  1.3610e+00],\n          ...,\n          [ 1.0861e+00, -1.8393e+00,  3.5132e+00,  ..., -1.0181e+00,\n           -3.5613e+00,  8.3257e-01],\n          [-1.9946e+00, -4.1611e+00,  3.4041e+00,  ..., -1.0311e+00,\n           -3.5627e+00,  8.5132e-01],\n          [-3.2613e+00, -4.7215e+00,  2.6622e+00,  ..., -1.0319e+00,\n           -3.5575e+00,  8.6417e-01]]]]), tensor([[[[ 0.2294, -0.0831,  0.0276,  ...,  0.3465,  0.1646, -0.1411],\n          [ 0.1604, -0.0516,  0.1817,  ...,  0.2001,  0.0799, -0.4173],\n          [ 0.0456,  0.0581,  0.1558,  ...,  0.0642,  0.0088, -0.4330],\n          ...,\n          [-0.2049,  0.1950,  0.1705,  ..., -0.1609, -0.3178,  0.1198],\n          [-0.2051,  0.1961,  0.1667,  ..., -0.1616, -0.3198,  0.1193],\n          [-0.2089,  0.1948,  0.1655,  ..., -0.1624, -0.3216,  0.1197]],\n\n         [[ 0.1065,  0.3231,  0.1359,  ...,  0.0108, -0.4035, -0.1469],\n          [ 0.1822,  0.1553,  0.0824,  ...,  0.0270, -0.2217, -0.2543],\n          [ 0.0945,  0.0924,  0.1869,  ..., -0.2008, -0.2477, -0.2973],\n          ...,\n          [ 0.0866,  0.1800,  0.0855,  ..., -0.0281,  0.1285, -0.3022],\n          [ 0.0882,  0.1739,  0.0878,  ..., -0.0229,  0.1295, -0.3045],\n          [ 0.0876,  0.1702,  0.0910,  ..., -0.0170,  0.1330, -0.3158]],\n\n         [[ 0.1070,  0.2681,  0.4305,  ...,  0.4334,  0.0266,  0.0941],\n          [ 0.0882,  0.1322,  0.2794,  ...,  0.3842, -0.1456,  0.1005],\n          [ 0.0801,  0.1279,  0.1657,  ...,  0.3336, -0.0095,  0.1549],\n          ...,\n          [ 0.3349, -0.0816, -0.0376,  ...,  0.1740, -0.2301, -0.2229],\n          [ 0.3356, -0.0905, -0.0390,  ...,  0.1716, -0.2331, -0.2233],\n          [ 0.3391, -0.0985, -0.0439,  ...,  0.1692, -0.2380, -0.2173]],\n\n         ...,\n\n         [[-0.2016, -0.0498,  0.2440,  ...,  0.1034,  0.1417,  0.0950],\n          [-0.0153, -0.3449,  0.2675,  ...,  0.1800,  0.1193,  0.3090],\n          [-0.1139, -0.2704,  0.2840,  ...,  0.0453,  0.0225,  0.4359],\n          ...,\n          [-0.0541, -0.1550,  0.2016,  ...,  0.0816,  0.2021,  0.3271],\n          [-0.0567, -0.1550,  0.2102,  ...,  0.0864,  0.2083,  0.3233],\n          [-0.0613, -0.1585,  0.2166,  ...,  0.0853,  0.2107,  0.3220]],\n\n         [[-0.6330, -0.1192,  0.2123,  ...,  0.4768,  0.1281, -0.5442],\n          [-0.4826, -0.0125,  0.1169,  ...,  0.4997,  0.1915, -0.2532],\n          [-0.3567, -0.0667, -0.0207,  ...,  0.2114,  0.1276, -0.1518],\n          ...,\n          [ 0.1179,  0.1618,  0.2948,  ..., -0.3066, -0.0219, -0.3305],\n          [ 0.1208,  0.1617,  0.2940,  ..., -0.3093, -0.0273, -0.3275],\n          [ 0.1238,  0.1614,  0.2896,  ..., -0.3140, -0.0312, -0.3267]],\n\n         [[ 0.1313,  0.5091,  0.1842,  ...,  0.4140,  0.0447,  0.2788],\n          [ 0.1527,  0.3938,  0.2497,  ...,  0.2325, -0.0681,  0.2034],\n          [ 0.1356,  0.2834,  0.2672,  ...,  0.2084, -0.1392, -0.0088],\n          ...,\n          [-0.0650,  0.0324,  0.1372,  ...,  0.0700,  0.0948, -0.0830],\n          [-0.0650,  0.0353,  0.1407,  ...,  0.0682,  0.0942, -0.0825],\n          [-0.0665,  0.0391,  0.1435,  ...,  0.0627,  0.0935, -0.0826]]]])), (tensor([[[[ 4.0887e+00,  3.2748e-01, -1.6569e-01,  ..., -3.5996e-02,\n            2.4451e+00,  2.0616e+00],\n          [ 2.6718e+00, -1.7972e+00, -1.2895e+00,  ...,  4.2622e-01,\n            1.9568e+00,  1.4025e+00],\n          [-1.2520e+00, -3.4990e+00, -1.9896e+00,  ...,  4.3041e-01,\n            2.4326e+00,  1.0010e+00],\n          ...,\n          [ 1.3820e+00, -1.7536e-01, -2.7984e+00,  ...,  1.8225e+00,\n            3.5683e+00,  2.7227e+00],\n          [-2.2975e+00, -2.3820e+00, -2.3724e+00,  ...,  1.8779e+00,\n            3.5625e+00,  2.7222e+00],\n          [-3.8705e+00, -3.5671e+00, -1.5042e+00,  ...,  1.9091e+00,\n            3.5592e+00,  2.7179e+00]],\n\n         [[-1.5678e+00,  3.4897e-01,  2.6595e+00,  ...,  1.8147e+00,\n           -8.7594e-01,  4.6185e-01],\n          [ 8.3624e-02, -1.0713e+00,  1.7197e+00,  ...,  1.2337e+00,\n           -7.1164e-01, -1.2332e-01],\n          [ 1.8150e+00, -2.2112e+00,  5.1205e-01,  ...,  8.8230e-01,\n           -6.8298e-01, -1.4594e-01],\n          ...,\n          [ 1.6910e-01, -5.5517e-01, -1.5189e+00,  ...,  3.9265e-01,\n           -8.2020e-01,  2.6420e-03],\n          [ 1.0735e+00, -1.2330e+00, -2.2154e+00,  ...,  3.6187e-01,\n           -8.3966e-01, -1.7104e-04],\n          [ 1.0051e+00, -1.3819e+00, -2.4941e+00,  ...,  3.4529e-01,\n           -8.4002e-01,  2.0134e-02]],\n\n         [[-9.9645e-01,  1.0689e+00, -5.7070e-01,  ..., -1.4129e+00,\n           -4.4428e-01, -1.8539e-01],\n          [ 4.3689e-01, -6.0819e-01, -1.8977e+00,  ..., -8.8235e-01,\n           -8.0455e-01, -3.3663e-01],\n          [ 1.6103e+00, -1.8432e+00, -2.5897e+00,  ..., -8.8264e-01,\n           -1.0514e+00, -5.7483e-01],\n          ...,\n          [ 6.1162e-03,  1.4171e+00, -1.9277e+00,  ..., -1.2316e+00,\n           -2.3978e+00,  2.8195e-01],\n          [ 6.2172e-01,  2.2962e-01, -1.6888e+00,  ..., -1.2205e+00,\n           -2.3920e+00,  3.1323e-01],\n          [ 6.6084e-01, -1.0626e+00, -1.1258e+00,  ..., -1.2101e+00,\n           -2.3936e+00,  3.3985e-01]],\n\n         ...,\n\n         [[-4.1650e-01, -2.7751e+00, -4.3270e+00,  ...,  3.5658e-01,\n           -7.2760e+00,  2.2112e+00],\n          [-3.2713e+00, -2.4946e+00, -3.6688e+00,  ..., -7.8700e-02,\n           -6.9392e+00,  2.2791e+00],\n          [-3.5204e+00, -7.7762e-01, -2.4112e+00,  ...,  1.2515e-01,\n           -6.5452e+00,  2.1381e+00],\n          ...,\n          [-3.3836e+00, -3.7924e+00, -7.8670e-01,  ...,  2.7249e-01,\n           -6.8578e+00,  2.2089e+00],\n          [-1.8384e+00, -3.8370e+00,  8.3520e-01,  ...,  2.8066e-01,\n           -6.8355e+00,  2.2165e+00],\n          [ 1.3827e+00, -2.2600e+00,  2.2842e+00,  ...,  2.8656e-01,\n           -6.8109e+00,  2.2232e+00]],\n\n         [[ 4.6911e-01,  1.7554e+00,  8.0224e-02,  ..., -4.5994e+00,\n           -8.5412e-01, -2.3702e+00],\n          [ 3.2844e+00,  3.5364e+00,  1.2109e+00,  ..., -3.8067e+00,\n            4.6548e-01, -2.7581e+00],\n          [ 3.1695e+00,  3.5605e+00,  2.0737e+00,  ..., -3.7330e+00,\n            8.8554e-01, -2.7454e+00],\n          ...,\n          [ 3.7008e+00,  1.7021e+00,  3.2354e+00,  ..., -3.9365e+00,\n           -1.7477e+00, -4.3253e+00],\n          [ 2.0185e+00,  3.7262e+00,  3.2805e+00,  ..., -3.9519e+00,\n           -1.7531e+00, -4.3098e+00],\n          [-1.5223e+00,  4.1770e+00,  2.6983e+00,  ..., -3.9353e+00,\n           -1.7424e+00, -4.2966e+00]],\n\n         [[-3.2773e-01, -1.6890e+00, -2.7147e+00,  ..., -6.8325e-01,\n           -3.0029e+00,  1.8612e+00],\n          [ 1.5494e+00, -1.8444e+00, -2.1060e+00,  ..., -9.1621e-01,\n           -2.0787e+00,  1.6598e+00],\n          [ 1.9053e+00, -1.4265e+00, -1.1078e+00,  ..., -1.7643e+00,\n           -1.6656e+00,  1.2002e+00],\n          ...,\n          [ 2.7313e+00, -1.6264e+00,  2.0971e-01,  ..., -2.0345e+00,\n           -2.5413e+00,  6.8921e-02],\n          [ 1.1403e+00, -1.6238e+00,  1.1283e+00,  ..., -2.0610e+00,\n           -2.5983e+00,  8.9604e-02],\n          [-1.4938e+00, -9.3510e-01,  1.8317e+00,  ..., -2.0975e+00,\n           -2.6373e+00,  1.3380e-01]]]]), tensor([[[[ 2.2712e-03,  2.8754e-01,  7.7029e-02,  ...,  4.6423e-01,\n            4.4540e-01,  4.8117e-02],\n          [ 1.2401e-01,  8.5438e-02,  3.5542e-01,  ...,  3.9838e-01,\n            4.1792e-01,  1.4247e-01],\n          [ 8.2357e-02,  1.1741e-01,  6.3340e-01,  ...,  4.2053e-01,\n            4.2102e-01,  6.6037e-02],\n          ...,\n          [-1.9523e-02,  1.0184e-01,  2.8699e-01,  ...,  2.0295e-02,\n            1.3692e-01,  6.4691e-02],\n          [-8.0269e-03,  9.8434e-02,  2.8651e-01,  ...,  1.9346e-02,\n            1.2763e-01,  6.5685e-02],\n          [-7.9945e-04,  9.4754e-02,  2.8904e-01,  ...,  2.3227e-02,\n            1.1680e-01,  7.0100e-02]],\n\n         [[ 2.4590e-01,  2.3189e-03, -2.2217e-01,  ...,  2.1136e-01,\n            5.4936e-02,  4.8682e-01],\n          [ 3.0765e-01,  3.2737e-02, -1.8711e-01,  ...,  2.5997e-01,\n           -4.6772e-02,  4.6851e-01],\n          [ 1.3314e-01,  1.4444e-01, -1.7131e-01,  ...,  3.0789e-02,\n            1.7485e-02,  4.7372e-01],\n          ...,\n          [ 1.7579e-01, -1.8247e-01, -1.1046e-01,  ..., -1.0607e-01,\n            2.1359e-01,  1.9011e-01],\n          [ 1.8137e-01, -1.7988e-01, -9.8746e-02,  ..., -1.1144e-01,\n            2.1240e-01,  1.8765e-01],\n          [ 1.8355e-01, -1.7979e-01, -9.3671e-02,  ..., -1.2038e-01,\n            2.0345e-01,  1.7758e-01]],\n\n         [[ 9.8382e-02, -5.9050e-02, -5.6579e-02,  ..., -1.8640e-01,\n           -9.9123e-01, -1.2905e-01],\n          [-4.3950e-02,  4.6925e-02, -1.3234e-01,  ..., -2.6173e-01,\n           -1.0038e+00, -1.9919e-01],\n          [ 1.8770e-01, -2.2023e-02, -1.4845e-01,  ..., -3.8231e-01,\n           -9.2594e-01, -1.7470e-01],\n          ...,\n          [-1.8172e-01, -1.3118e-01, -1.5120e-01,  ..., -6.8856e-01,\n           -4.0495e-01, -3.6107e-01],\n          [-1.7915e-01, -1.3463e-01, -1.4974e-01,  ..., -6.8713e-01,\n           -4.0565e-01, -3.6272e-01],\n          [-1.7126e-01, -1.3331e-01, -1.4847e-01,  ..., -6.8425e-01,\n           -4.0648e-01, -3.6382e-01]],\n\n         ...,\n\n         [[ 2.0514e-01,  3.6217e-01,  4.1744e-01,  ..., -3.5085e-03,\n           -4.6478e-01, -4.9269e-02],\n          [ 5.1988e-02,  6.1139e-02,  4.4411e-01,  ..., -2.9344e-02,\n           -4.2829e-01, -1.1008e-01],\n          [ 2.8566e-02,  1.4231e-01,  4.1731e-01,  ..., -9.9202e-02,\n           -3.6000e-01, -2.2448e-01],\n          ...,\n          [ 1.8158e-01,  4.5517e-02,  1.7603e-01,  ..., -2.2312e-01,\n           -2.3683e-01,  2.1508e-01],\n          [ 1.8181e-01,  4.5683e-02,  1.7360e-01,  ..., -2.2391e-01,\n           -2.3788e-01,  2.1971e-01],\n          [ 1.8333e-01,  4.9273e-02,  1.7450e-01,  ..., -2.2456e-01,\n           -2.3369e-01,  2.2289e-01]],\n\n         [[ 1.7893e-01,  1.3055e-01,  2.2713e-01,  ...,  2.8205e-01,\n            2.5390e-01, -2.4429e-01],\n          [-2.4505e-02,  7.7169e-02,  3.3571e-01,  ...,  2.4665e-01,\n            1.9566e-01, -3.1748e-01],\n          [ 2.1838e-02,  3.7681e-02,  2.6448e-01,  ...,  1.9987e-01,\n            1.7355e-01, -2.6918e-01],\n          ...,\n          [-2.2186e-01,  8.3192e-02, -1.8535e-01,  ...,  5.9368e-02,\n           -4.4933e-02,  5.1333e-02],\n          [-2.2006e-01,  8.5329e-02, -1.8692e-01,  ...,  5.8227e-02,\n           -4.6799e-02,  4.9828e-02],\n          [-2.2003e-01,  8.5164e-02, -1.8937e-01,  ...,  5.6540e-02,\n           -5.1877e-02,  4.8141e-02]],\n\n         [[ 8.3477e-01, -1.5153e-01,  1.0238e-01,  ...,  3.8697e-02,\n            2.4501e-01,  2.0085e-01],\n          [ 7.1969e-01,  1.2636e-01, -4.0180e-03,  ...,  2.3469e-01,\n            5.5340e-02,  3.3855e-01],\n          [ 6.6282e-01,  1.9503e-01, -2.7488e-02,  ...,  4.5909e-01,\n            1.3674e-01,  2.9550e-01],\n          ...,\n          [ 5.1304e-01,  1.3907e-01,  2.9728e-01,  ...,  3.4854e-01,\n           -1.1156e-01,  2.2583e-01],\n          [ 5.1961e-01,  1.4699e-01,  2.9660e-01,  ...,  3.4140e-01,\n           -1.3846e-01,  2.2665e-01],\n          [ 5.2051e-01,  1.5167e-01,  3.0557e-01,  ...,  3.4052e-01,\n           -1.5852e-01,  2.1205e-01]]]])), (tensor([[[[-6.0293e-01,  1.1831e+00, -1.2318e+00,  ..., -1.7761e+00,\n            3.9344e+00, -2.1048e+00],\n          [-1.0398e-01,  2.0117e+00, -1.5038e+00,  ..., -1.9379e+00,\n            3.1535e+00, -1.8110e+00],\n          [ 3.8843e-01,  1.5733e+00, -1.6012e+00,  ..., -2.0624e+00,\n            2.3855e+00, -1.7605e+00],\n          ...,\n          [ 4.2361e-01,  1.0795e+00, -4.7310e-01,  ..., -2.5710e+00,\n            4.2902e+00, -2.1571e+00],\n          [ 1.0931e+00,  1.5883e+00,  6.5457e-02,  ..., -2.5753e+00,\n            4.2635e+00, -2.0978e+00],\n          [ 7.7395e-01,  1.4302e+00,  6.0258e-01,  ..., -2.5142e+00,\n            4.2530e+00, -2.0567e+00]],\n\n         [[-1.6462e+00, -1.7790e+00, -2.9742e+00,  ...,  8.0080e-01,\n           -1.6497e+00,  1.7079e-01],\n          [ 1.5234e+00, -1.4945e-01, -2.4889e+00,  ...,  2.5682e-01,\n           -1.3303e+00, -7.9308e-02],\n          [ 3.3974e+00,  1.3017e+00, -1.4793e+00,  ...,  4.3410e-01,\n           -8.3985e-01,  6.4977e-02],\n          ...,\n          [ 2.6787e+00, -1.1699e+00, -1.1161e-02,  ...,  2.2048e-02,\n           -2.3767e+00, -4.5140e-01],\n          [ 3.0958e+00,  4.8797e-01,  1.2366e+00,  ...,  7.3543e-02,\n           -2.3679e+00, -4.9963e-01],\n          [ 6.6332e-01,  1.9566e+00,  2.2367e+00,  ...,  1.2531e-01,\n           -2.3589e+00, -5.7790e-01]],\n\n         [[ 1.4813e+00,  3.1242e+00,  1.4306e+00,  ...,  2.9608e+00,\n            3.0403e+00, -2.5609e+00],\n          [-2.5149e+00,  2.2922e+00,  2.8360e+00,  ...,  2.5646e+00,\n            2.5551e+00, -1.9162e+00],\n          [-3.9015e+00,  7.3043e-01,  3.8559e+00,  ...,  2.4225e+00,\n            2.1127e+00, -1.6515e+00],\n          ...,\n          [-3.3369e+00,  3.8172e+00,  5.0278e+00,  ...,  3.7959e+00,\n            2.1111e+00, -1.0351e+00],\n          [-2.2837e+00,  3.3154e+00,  4.0937e+00,  ...,  3.7911e+00,\n            2.1110e+00, -1.0435e+00],\n          [ 8.5514e-01,  1.4114e+00,  2.3841e+00,  ...,  3.7648e+00,\n            2.1160e+00, -1.0539e+00]],\n\n         ...,\n\n         [[ 5.8325e-01, -1.4514e+00, -8.5414e-01,  ..., -2.1646e+00,\n            7.2874e-01, -3.0978e+00],\n          [ 1.4440e+00, -9.1828e-01, -5.6528e-01,  ..., -2.3568e+00,\n           -9.1152e-03, -2.5556e+00],\n          [ 9.8631e-01,  1.0780e-01,  8.1670e-02,  ..., -2.2156e+00,\n           -3.4371e-01, -2.7533e+00],\n          ...,\n          [ 9.0456e-01, -1.9370e+00,  1.5319e+00,  ..., -1.7721e+00,\n           -1.2656e+00, -4.6477e+00],\n          [-1.2235e-02, -8.0089e-01,  1.4517e+00,  ..., -1.7726e+00,\n           -1.2598e+00, -4.6329e+00],\n          [-9.0982e-01,  6.6284e-01,  1.1005e+00,  ..., -1.7563e+00,\n           -1.2830e+00, -4.6298e+00]],\n\n         [[ 8.7202e+00,  3.0001e+00, -4.4354e+00,  ..., -2.7713e-01,\n           -4.5762e-01,  3.1991e-01],\n          [ 9.1121e+00,  4.6852e+00, -3.0733e+00,  ..., -4.3296e-01,\n           -4.0086e-01,  4.4893e-01],\n          [ 1.6679e+00,  4.6976e+00, -1.2182e+00,  ..., -5.7769e-01,\n           -6.8876e-01,  3.6349e-01],\n          ...,\n          [ 6.0866e+00,  2.8431e+00,  2.0150e+00,  ..., -1.9086e-01,\n           -5.3370e-01,  9.5897e-01],\n          [-3.6469e+00,  4.7528e+00,  3.7342e+00,  ..., -2.0273e-01,\n           -5.3154e-01,  9.5349e-01],\n          [-1.0043e+01,  4.6184e+00,  4.7226e+00,  ..., -2.1489e-01,\n           -5.2719e-01,  9.6719e-01]],\n\n         [[-3.7024e+00, -7.7714e-02,  3.8660e+00,  ...,  4.6342e+00,\n            2.9417e-01,  2.8286e+00],\n          [-1.0096e+00, -1.4617e+00,  3.8734e+00,  ...,  3.7355e+00,\n            1.3509e-01,  2.5583e+00],\n          [ 2.1597e+00, -2.6706e+00,  3.1962e+00,  ...,  3.3273e+00,\n           -3.0936e-01,  2.5479e+00],\n          ...,\n          [ 7.6544e-01,  7.2807e-01,  1.0427e+00,  ...,  4.3549e+00,\n           -1.0786e-01,  2.8679e+00],\n          [ 3.6690e+00, -1.4271e+00,  3.5730e-03,  ...,  4.3157e+00,\n           -1.1488e-01,  2.8506e+00],\n          [ 3.2193e+00, -2.9703e+00, -1.0329e+00,  ...,  4.2578e+00,\n           -1.0527e-01,  2.8740e+00]]]]), tensor([[[[-1.3015e-01,  2.1157e-01,  2.6243e-01,  ...,  2.4245e-01,\n            2.8933e-02,  2.3170e-01],\n          [-1.6240e-01,  4.2512e-02,  4.7032e-01,  ...,  5.5728e-02,\n           -1.3870e-01, -4.4962e-02],\n          [-2.5204e-01, -2.5217e-01,  2.6253e-01,  ..., -7.5027e-02,\n            1.7664e-02,  5.8048e-02],\n          ...,\n          [-2.3142e-01, -3.8668e-01,  2.1924e-01,  ...,  5.7136e-04,\n            3.1094e-01,  4.0090e-01],\n          [-2.3788e-01, -3.7532e-01,  2.2588e-01,  ...,  1.6513e-02,\n            3.1266e-01,  3.8574e-01],\n          [-2.3906e-01, -3.6350e-01,  2.3183e-01,  ...,  2.3053e-02,\n            3.0550e-01,  3.7050e-01]],\n\n         [[ 4.1102e-02,  3.3914e-01, -1.7242e-01,  ...,  1.3781e-01,\n           -3.2538e-01,  1.4625e-01],\n          [-1.6713e-01,  6.2138e-01, -3.2635e-01,  ...,  1.6213e-01,\n           -1.7525e-01,  2.3400e-01],\n          [-1.8426e-01,  5.7439e-01, -2.5780e-01,  ...,  1.2455e-01,\n           -8.2944e-02,  2.6811e-01],\n          ...,\n          [-6.2044e-02,  1.2338e-01, -5.0754e-01,  ..., -3.7997e-01,\n           -2.8558e-01,  1.2093e-01],\n          [-6.7034e-02,  1.1755e-01, -5.0635e-01,  ..., -3.7596e-01,\n           -2.8120e-01,  1.1848e-01],\n          [-7.2967e-02,  1.1003e-01, -5.0728e-01,  ..., -3.7366e-01,\n           -2.7848e-01,  1.1911e-01]],\n\n         [[ 5.3281e-01,  3.1209e-02,  8.7650e-02,  ...,  2.3887e-01,\n            3.5120e-01, -4.4756e-01],\n          [ 4.1478e-01, -2.4214e-02,  8.8928e-02,  ...,  2.9524e-01,\n            4.1421e-01, -3.0823e-01],\n          [ 5.4977e-01, -2.4902e-02, -3.1677e-03,  ...,  3.1409e-01,\n            4.2422e-01, -1.5962e-01],\n          ...,\n          [ 1.7563e-02, -7.5696e-02, -1.4253e-01,  ...,  4.6824e-02,\n           -1.1127e-01,  1.1897e-01],\n          [ 1.1722e-02, -7.6217e-02, -1.4610e-01,  ...,  4.5188e-02,\n           -1.1849e-01,  1.1550e-01],\n          [ 1.1155e-02, -7.3725e-02, -1.4608e-01,  ...,  4.3603e-02,\n           -1.2637e-01,  1.1186e-01]],\n\n         ...,\n\n         [[-1.0383e-01,  3.9601e-02,  2.1770e-02,  ..., -3.5041e-02,\n           -1.3841e-01, -8.7006e-02],\n          [-3.3885e-01, -1.4427e-01, -9.6334e-02,  ...,  8.0129e-04,\n           -1.6542e-01,  2.1106e-02],\n          [-3.2633e-01, -1.1984e-01, -2.5906e-01,  ..., -4.6580e-02,\n           -2.1496e-01, -1.8052e-02],\n          ...,\n          [-3.9711e-01, -1.7372e-02, -3.0839e-01,  ..., -2.3785e-01,\n           -2.1088e-01,  2.7364e-02],\n          [-3.8481e-01, -1.6535e-02, -3.0765e-01,  ..., -2.4617e-01,\n           -2.1815e-01,  2.6198e-02],\n          [-3.7553e-01, -2.7750e-02, -3.0087e-01,  ..., -2.6577e-01,\n           -2.2404e-01,  2.5209e-02]],\n\n         [[ 3.3556e-02, -4.7259e-01, -1.1798e-01,  ...,  1.1659e-01,\n            8.4243e-02,  1.7660e-01],\n          [-1.9625e-01, -3.9479e-01, -3.5000e-02,  ...,  1.0848e-01,\n           -8.2989e-02,  1.8507e-01],\n          [-3.9594e-01, -4.1089e-01, -8.6020e-02,  ...,  1.9806e-01,\n           -1.9703e-01,  2.5733e-01],\n          ...,\n          [-4.3970e-01, -1.1109e-02, -4.3065e-01,  ...,  1.7673e-01,\n           -8.6794e-02,  1.6500e-01],\n          [-4.4445e-01, -1.1779e-02, -4.2981e-01,  ...,  1.7251e-01,\n           -9.0803e-02,  1.6414e-01],\n          [-4.4236e-01, -1.0962e-02, -4.2924e-01,  ...,  1.6995e-01,\n           -9.3379e-02,  1.5879e-01]],\n\n         [[ 1.3257e-01,  4.2065e-01,  1.0428e-01,  ..., -1.5073e-02,\n            4.4802e-02,  2.3265e-01],\n          [ 2.6634e-01,  3.0741e-01, -6.2137e-02,  ..., -4.5931e-02,\n            9.8886e-02,  2.5010e-01],\n          [ 2.2958e-01,  1.3916e-01, -8.1847e-02,  ..., -3.3944e-03,\n            1.7159e-02,  2.4748e-01],\n          ...,\n          [ 3.8233e-01, -1.3444e-01, -1.4059e-01,  ..., -1.9485e-01,\n            1.6774e-01,  2.0020e-01],\n          [ 3.7917e-01, -1.3832e-01, -1.3784e-01,  ..., -1.9192e-01,\n            1.6374e-01,  2.0351e-01],\n          [ 3.7372e-01, -1.3927e-01, -1.3776e-01,  ..., -1.8841e-01,\n            1.5953e-01,  2.0513e-01]]]])), (tensor([[[[-1.3999e+00, -9.6929e-01, -6.4588e-01,  ..., -1.5553e+00,\n           -2.2508e+00,  2.6811e+00],\n          [ 1.0132e-01,  1.9011e-01, -5.9528e-02,  ..., -1.1111e+00,\n           -1.8351e+00,  1.9873e+00],\n          [ 1.6069e+00,  1.4909e+00,  8.4156e-01,  ..., -8.7063e-01,\n           -1.2684e+00,  2.0444e+00],\n          ...,\n          [ 9.4792e-01, -1.2045e+00,  1.9828e+00,  ..., -1.0135e+00,\n           -2.4383e+00,  3.3609e+00],\n          [ 1.9815e+00,  1.4174e-01,  2.0915e+00,  ..., -9.5100e-01,\n           -2.4167e+00,  3.3446e+00],\n          [ 1.1943e+00,  1.4245e+00,  1.8033e+00,  ..., -8.7993e-01,\n           -2.3980e+00,  3.3408e+00]],\n\n         [[-3.8714e+00,  1.8289e+00, -6.7289e-01,  ...,  1.9908e-01,\n            7.5389e-01, -6.0441e-01],\n          [-1.7237e+00, -7.4398e-01, -2.4484e+00,  ...,  4.4667e-01,\n            7.7305e-01, -3.2114e-01],\n          [ 2.0475e+00, -2.7525e+00, -3.4085e+00,  ...,  4.2686e-01,\n            6.0797e-01, -4.7994e-03],\n          ...,\n          [-6.7446e-01,  1.9028e+00, -4.2227e+00,  ..., -3.3477e-01,\n            9.0564e-01, -9.9438e-01],\n          [ 3.2141e+00, -5.1123e-01, -3.9203e+00,  ..., -3.4273e-01,\n            8.9575e-01, -9.8894e-01],\n          [ 4.1600e+00, -2.7138e+00, -2.8737e+00,  ..., -3.3689e-01,\n            8.9501e-01, -9.8963e-01]],\n\n         [[-9.3499e-01, -2.7539e+00,  1.8266e+00,  ..., -9.2270e-01,\n            4.9071e-01,  1.5282e+00],\n          [ 6.6939e-01, -2.3217e+00,  1.8887e+00,  ..., -4.9462e-01,\n            4.8469e-01,  9.6656e-01],\n          [ 1.9097e+00, -1.1008e+00,  1.6411e+00,  ..., -2.2286e-01,\n            6.1203e-01,  7.2310e-01],\n          ...,\n          [ 1.1406e+00, -2.2303e+00, -9.6498e-03,  ...,  1.1379e+00,\n            3.5147e-02,  2.4575e+00],\n          [ 1.6794e+00, -2.0591e+00, -6.5581e-01,  ...,  1.1644e+00,\n            1.5734e-02,  2.4914e+00],\n          [ 6.7024e-01, -9.9737e-01, -1.1673e+00,  ...,  1.1977e+00,\n           -1.3601e-02,  2.5157e+00]],\n\n         ...,\n\n         [[-4.7721e-01, -1.3319e+00, -8.6835e-01,  ...,  1.0494e+00,\n           -1.1835e+00, -3.3403e+00],\n          [-1.2874e+00, -1.4367e+00, -1.4251e+00,  ...,  1.0908e+00,\n           -7.2929e-01, -2.5296e+00],\n          [-9.1607e-01, -6.9999e-01, -1.9001e+00,  ...,  8.8602e-01,\n           -3.3305e-01, -2.6759e+00],\n          ...,\n          [-1.2903e+00, -1.6088e+00, -1.9081e+00,  ...,  8.8842e-01,\n            9.3267e-03, -3.4470e+00],\n          [-4.3387e-01, -1.2949e+00, -1.4505e+00,  ...,  9.2355e-01,\n            6.6008e-02, -3.4070e+00],\n          [ 8.2691e-01, -4.2070e-01, -7.1449e-01,  ...,  9.5112e-01,\n            1.2284e-01, -3.3867e+00]],\n\n         [[ 4.2455e+00, -4.7817e-01, -1.9100e+00,  ...,  3.7405e-01,\n            1.1719e+00,  5.1337e+00],\n          [ 2.6080e+00, -3.4986e+00, -2.8348e+00,  ...,  9.8199e-01,\n            1.7496e+00,  4.8108e+00],\n          [-1.6793e+00, -5.0533e+00, -3.7036e+00,  ...,  1.4007e+00,\n            1.5056e+00,  4.7369e+00],\n          ...,\n          [ 5.9418e-01, -1.2022e+00, -4.6105e+00,  ...,  1.2991e+00,\n           -6.6588e-01,  4.9691e+00],\n          [-3.4536e+00, -3.6267e+00, -4.2280e+00,  ...,  1.2536e+00,\n           -6.5302e-01,  5.0118e+00],\n          [-4.3164e+00, -4.4983e+00, -3.0396e+00,  ...,  1.2481e+00,\n           -6.3071e-01,  5.0621e+00]],\n\n         [[-1.8169e-01,  7.0227e-01, -1.9207e-01,  ..., -7.2078e-01,\n           -1.3655e+00, -3.2798e+00],\n          [-3.9938e-01,  3.5618e-01,  2.1236e-01,  ..., -1.4625e+00,\n           -1.5786e+00, -2.3948e+00],\n          [-7.3758e-01, -3.2525e-01,  7.1930e-01,  ..., -1.9439e+00,\n           -1.4315e+00, -2.1864e+00],\n          ...,\n          [-6.0494e-01,  3.7810e-01,  1.4737e+00,  ..., -1.5668e+00,\n           -2.1057e+00, -2.9479e+00],\n          [-4.2530e-01, -2.0721e-01,  1.4527e+00,  ..., -1.5851e+00,\n           -2.1080e+00, -2.9258e+00],\n          [ 1.3648e-01, -7.0592e-01,  1.1511e+00,  ..., -1.6213e+00,\n           -2.1217e+00, -2.9081e+00]]]]), tensor([[[[-1.3242e-01, -3.8542e-01,  1.6313e-01,  ..., -2.9885e-02,\n           -2.6682e-01,  1.8670e-02],\n          [-2.8960e-01, -4.6490e-02, -6.2951e-02,  ...,  9.0366e-03,\n            2.3377e-01, -2.0264e-01],\n          [-2.2438e-01, -1.9649e-01, -2.7885e-01,  ...,  2.6118e-01,\n            1.5541e-01, -2.6607e-01],\n          ...,\n          [-2.2986e-01, -1.4073e-01,  6.2501e-02,  ..., -6.9301e-02,\n            6.6724e-03, -2.8560e-01],\n          [-2.4124e-01, -1.2882e-01,  7.2270e-02,  ..., -7.2180e-02,\n            1.0740e-02, -2.7340e-01],\n          [-2.5276e-01, -1.0463e-01,  8.0214e-02,  ..., -7.6244e-02,\n            2.8866e-02, -2.5415e-01]],\n\n         [[-1.0550e+00, -1.8059e-01, -9.9265e-02,  ...,  4.6494e-01,\n            4.3358e-02, -7.4154e-02],\n          [-8.5643e-01, -3.0284e-01, -1.5428e-01,  ...,  4.8382e-01,\n           -1.1953e-01, -1.5117e-01],\n          [-6.5440e-01, -1.9452e-01, -5.1295e-02,  ...,  4.4090e-01,\n           -3.2343e-02, -1.6883e-01],\n          ...,\n          [-3.2931e-01,  1.1206e-01, -1.8566e-01,  ...,  2.1598e-01,\n           -2.5882e-02,  3.2788e-02],\n          [-3.2808e-01,  1.1433e-01, -1.9010e-01,  ...,  2.1184e-01,\n           -2.5209e-02,  2.4542e-02],\n          [-3.2771e-01,  1.1422e-01, -1.9041e-01,  ...,  2.0791e-01,\n           -2.1785e-02,  1.7310e-02]],\n\n         [[-4.7093e-01,  1.1560e-01,  2.3704e-01,  ...,  7.8798e-03,\n            8.1924e-02, -8.0466e-02],\n          [-2.7147e-01,  1.7360e-01,  1.9010e-01,  ...,  4.1736e-04,\n            1.8379e-01,  7.8482e-02],\n          [-3.2141e-01,  3.5205e-01,  1.8064e-01,  ...,  9.2525e-02,\n            5.3761e-02,  2.0459e-01],\n          ...,\n          [-2.4975e-01,  2.6059e-01,  2.7225e-01,  ...,  1.1658e-01,\n           -3.6540e-01, -2.5269e-03],\n          [-2.5039e-01,  2.5328e-01,  2.6381e-01,  ...,  1.2347e-01,\n           -3.4610e-01, -1.7040e-03],\n          [-2.4909e-01,  2.3854e-01,  2.6097e-01,  ...,  1.2506e-01,\n           -3.3721e-01, -6.8680e-03]],\n\n         ...,\n\n         [[ 3.5258e-01, -3.3503e-02, -1.1754e-01,  ...,  7.7510e-02,\n            3.6069e-01, -1.6847e-01],\n          [ 2.3895e-01,  7.4814e-02,  1.5985e-01,  ...,  1.3339e-01,\n            2.5440e-01,  1.4592e-02],\n          [ 3.2358e-01, -1.7736e-01,  3.8566e-01,  ...,  5.9982e-02,\n            5.1172e-01,  2.1964e-01],\n          ...,\n          [-3.1857e-01,  4.4059e-01,  4.3033e-02,  ..., -3.0045e-01,\n            2.2957e-01,  6.1627e-02],\n          [-3.2034e-01,  4.3354e-01,  4.4108e-02,  ..., -2.9692e-01,\n            2.4823e-01,  5.1466e-02],\n          [-3.3067e-01,  4.3837e-01,  2.9461e-02,  ..., -2.9262e-01,\n            2.5602e-01,  4.2652e-02]],\n\n         [[ 1.2198e-01,  4.4603e-01,  6.5107e-01,  ...,  2.2881e-01,\n            9.4040e-02, -1.3880e+00],\n          [-6.9366e-04,  5.4011e-01,  5.2259e-01,  ...,  8.2417e-03,\n            1.0337e-01, -1.3242e+00],\n          [ 1.6663e-01,  4.0893e-01,  4.2703e-01,  ...,  1.5085e-01,\n            2.2262e-01, -1.3460e+00],\n          ...,\n          [ 2.2165e-01,  2.2882e-01,  3.6907e-01,  ...,  4.4041e-01,\n           -1.3063e-01, -8.8369e-01],\n          [ 2.1806e-01,  2.2517e-01,  3.6780e-01,  ...,  4.4631e-01,\n           -1.3297e-01, -8.8706e-01],\n          [ 2.1668e-01,  2.1962e-01,  3.6716e-01,  ...,  4.5215e-01,\n           -1.3712e-01, -8.8877e-01]],\n\n         [[-1.2226e-01,  4.6296e-01, -2.0915e-01,  ..., -2.1896e-01,\n           -1.1695e-01, -5.2306e-01],\n          [-5.2585e-03,  4.4172e-02,  1.1436e-01,  ..., -2.8900e-01,\n           -2.0831e-02, -6.7089e-01],\n          [ 4.4075e-02,  1.7094e-02, -2.4628e-02,  ...,  6.3227e-02,\n            1.5715e-01, -5.5661e-01],\n          ...,\n          [-6.1807e-02, -1.5103e-01,  1.2429e-01,  ...,  2.6530e-01,\n            2.2623e-01, -4.0725e-01],\n          [-7.9629e-02, -1.5723e-01,  1.2972e-01,  ...,  2.5559e-01,\n            2.0467e-01, -3.9570e-01],\n          [-1.0554e-01, -1.7012e-01,  1.3624e-01,  ...,  2.4608e-01,\n            1.6229e-01, -3.8618e-01]]]])), (tensor([[[[-3.0009e-01,  1.5457e+00,  6.3242e+00,  ...,  2.3500e-01,\n            1.0150e+00,  4.3972e-01],\n          [ 3.8828e+00,  1.5823e+00,  5.1486e+00,  ...,  8.1836e-02,\n            6.6742e-01,  5.9396e-01],\n          [ 4.1427e+00,  8.3302e-01,  2.5885e+00,  ...,  8.1187e-02,\n            3.8556e-01,  5.2558e-01],\n          ...,\n          [ 5.1319e+00,  2.7812e-01, -1.4357e+00,  ...,  1.7175e+00,\n           -1.2084e+00,  3.2827e-01],\n          [ 3.0746e+00,  1.3353e+00, -4.0543e+00,  ...,  1.6931e+00,\n           -1.2282e+00,  3.4311e-01],\n          [-1.8071e+00,  1.8298e+00, -5.8965e+00,  ...,  1.6611e+00,\n           -1.2123e+00,  3.5845e-01]],\n\n         [[ 2.4819e-01,  1.1080e+00, -2.2495e+00,  ...,  2.3270e-01,\n            3.3263e+00, -6.6941e+00],\n          [-6.3397e-01, -3.6629e-01, -1.4099e+00,  ..., -3.6568e-01,\n            2.4369e+00, -7.2986e+00],\n          [-1.2957e+00, -1.4684e+00, -5.9223e-01,  ..., -6.8756e-01,\n            2.6388e+00, -7.5103e+00],\n          ...,\n          [-1.3341e+00,  6.9682e-01,  1.4093e-01,  ...,  1.0429e+00,\n            3.4703e+00, -5.0556e+00],\n          [-1.1723e+00, -6.3861e-01,  1.2540e+00,  ...,  1.0486e+00,\n            3.4447e+00, -5.0650e+00],\n          [ 7.1192e-02, -1.7143e+00,  2.1323e+00,  ...,  1.0387e+00,\n            3.4378e+00, -5.0480e+00]],\n\n         [[-5.2587e+00, -4.5586e+00, -4.0780e+00,  ...,  1.7221e+00,\n           -2.1429e-01, -2.1749e-01],\n          [-3.5436e+00, -3.6746e+00, -2.4404e+00,  ...,  1.2887e+00,\n           -5.9491e-01, -5.6644e-01],\n          [ 2.1068e+00, -6.3610e-01, -9.1553e-01,  ...,  1.1540e+00,\n           -2.2726e-01, -3.3249e-01],\n          ...,\n          [-5.1106e-01, -3.9782e+00, -7.3545e-02,  ...,  1.7251e+00,\n            4.5728e-01, -3.0258e-01],\n          [ 5.8783e+00, -3.0948e+00,  1.8334e+00,  ...,  1.7140e+00,\n            4.3755e-01, -3.0615e-01],\n          [ 6.8631e+00, -8.9828e-01,  3.3980e+00,  ...,  1.7088e+00,\n            4.1998e-01, -3.0053e-01]],\n\n         ...,\n\n         [[-4.2373e-01, -3.5812e+00, -9.5634e-01,  ...,  7.4151e-01,\n            3.3088e+00, -4.2123e-01],\n          [ 1.5241e+00, -1.6235e+00, -1.3981e+00,  ...,  8.1906e-01,\n            2.9264e+00, -2.3208e-01],\n          [ 2.3996e+00,  8.2099e-01, -1.8404e+00,  ...,  2.2859e-01,\n            2.7021e+00, -6.1032e-01],\n          ...,\n          [ 2.7529e+00, -3.7233e+00, -1.6364e+00,  ...,  5.3130e-01,\n            3.2014e+00,  2.4454e-01],\n          [ 2.2355e+00, -1.0215e+00, -1.2775e+00,  ...,  5.4589e-01,\n            3.2005e+00,  3.0349e-01],\n          [-3.2823e-01,  2.1163e+00, -6.7281e-01,  ...,  5.4127e-01,\n            3.2126e+00,  3.5382e-01]],\n\n         [[ 2.4461e-01,  3.2316e+00,  5.0450e+00,  ...,  5.1448e+00,\n            1.0026e+00,  4.2095e+00],\n          [-4.3834e+00,  3.2104e+00,  4.7588e+00,  ...,  4.5947e+00,\n            1.0060e+00,  4.0111e+00],\n          [-4.3752e+00,  9.4483e-01,  3.3121e+00,  ...,  4.4841e+00,\n            9.4386e-01,  3.9019e+00],\n          ...,\n          [-5.0066e+00,  5.0553e+00,  3.9825e-01,  ...,  4.8691e+00,\n            1.8698e+00,  4.0354e+00],\n          [-3.6135e+00,  4.0651e+00, -1.5513e+00,  ...,  4.8554e+00,\n            1.8854e+00,  4.0353e+00],\n          [ 1.0989e+00,  1.3554e+00, -3.2198e+00,  ...,  4.8337e+00,\n            1.8768e+00,  4.0372e+00]],\n\n         [[ 4.4367e-01, -1.2512e+00,  9.4185e-01,  ...,  2.1592e+00,\n            2.5961e-01, -3.4330e+00],\n          [ 2.0278e-01, -1.2857e+00,  7.8210e-01,  ...,  2.2742e+00,\n            7.9530e-01, -2.9507e+00],\n          [-3.2732e-01, -7.7627e-01,  2.3182e-01,  ...,  1.7343e+00,\n            8.2533e-01, -2.5105e+00],\n          ...,\n          [-5.1880e-03, -1.1456e+00, -1.0844e-01,  ...,  2.5909e+00,\n            2.3422e+00,  5.7955e-01],\n          [-6.2402e-01, -1.1783e+00, -5.2174e-01,  ...,  2.6206e+00,\n            2.3352e+00,  6.3743e-01],\n          [-6.7066e-01, -7.0507e-01, -8.3688e-01,  ...,  2.6836e+00,\n            2.3416e+00,  6.6427e-01]]]]), tensor([[[[-3.3032e-01,  1.7788e-01,  1.0534e-01,  ..., -5.7092e-01,\n           -1.4299e-02, -2.3608e-01],\n          [-2.4296e-01,  4.9262e-02, -1.5184e-01,  ..., -2.7688e-01,\n            1.6324e-01, -1.3299e-01],\n          [-1.4663e-01,  2.3695e-01, -2.7490e-01,  ..., -5.2894e-01,\n            1.2548e-01, -1.6004e-02],\n          ...,\n          [-1.6972e-02,  1.1886e-01, -4.2733e-01,  ..., -1.4334e-01,\n            7.1683e-02,  5.0776e-02],\n          [-1.6030e-02,  1.1570e-01, -4.3674e-01,  ..., -1.4801e-01,\n            7.2469e-02,  4.6435e-02],\n          [-2.0425e-02,  1.1685e-01, -4.4285e-01,  ..., -1.4678e-01,\n            6.7919e-02,  3.6883e-02]],\n\n         [[ 1.5752e-01, -2.9383e-01,  2.2604e-01,  ..., -1.2862e-01,\n           -4.0243e-01,  1.8006e-01],\n          [ 3.4718e-02, -2.0228e-01,  2.9355e-02,  ..., -7.2312e-02,\n           -1.6646e-01,  4.2412e-01],\n          [ 2.3094e-01,  3.9205e-02,  1.0411e-01,  ...,  9.7120e-02,\n           -3.0050e-01,  2.0592e-01],\n          ...,\n          [ 5.7007e-01,  1.4476e-01, -4.5368e-01,  ...,  8.5978e-02,\n           -7.2401e-01,  1.6934e-01],\n          [ 5.5975e-01,  1.3717e-01, -4.6632e-01,  ...,  9.4968e-02,\n           -7.1852e-01,  1.6962e-01],\n          [ 5.5178e-01,  1.2437e-01, -4.8502e-01,  ...,  1.0133e-01,\n           -7.1205e-01,  1.7274e-01]],\n\n         [[-2.6543e-01, -1.0540e-01,  5.4510e-01,  ..., -5.2829e-02,\n           -3.1494e+00,  1.2586e-02],\n          [-8.2794e-02, -9.1372e-02,  6.4965e-01,  ...,  1.8256e-01,\n           -3.1102e+00, -2.1008e-01],\n          [-3.9311e-01,  2.9181e-01,  6.7386e-01,  ...,  1.3653e-01,\n           -3.1782e+00, -2.0241e-01],\n          ...,\n          [-5.0462e-01,  3.9688e-01,  2.1331e-01,  ..., -1.1955e+00,\n           -2.6360e+00, -1.4769e-01],\n          [-5.0747e-01,  3.9507e-01,  2.1381e-01,  ..., -1.2006e+00,\n           -2.6304e+00, -1.4469e-01],\n          [-5.1953e-01,  3.9337e-01,  2.1491e-01,  ..., -1.2025e+00,\n           -2.6285e+00, -1.3511e-01]],\n\n         ...,\n\n         [[ 2.1321e-02, -4.6755e-01,  1.4905e-01,  ...,  3.9917e-02,\n            1.8416e-01, -5.7967e-01],\n          [-2.1502e-01, -2.1186e-01,  1.2004e-01,  ...,  2.2914e-01,\n           -1.1296e-01, -5.3115e-01],\n          [-4.4512e-01,  8.5515e-02, -2.0195e-01,  ...,  3.2991e-02,\n           -3.1041e-01, -7.5373e-01],\n          ...,\n          [ 5.6580e-02,  1.4501e-01, -4.6580e-02,  ...,  1.2783e-01,\n           -2.4266e-01, -6.4983e-01],\n          [ 8.5139e-02,  1.3696e-01, -6.1886e-02,  ...,  1.2422e-01,\n           -2.5988e-01, -6.7162e-01],\n          [ 1.0946e-01,  1.6142e-01, -8.5197e-02,  ...,  1.3250e-01,\n           -2.5346e-01, -6.8160e-01]],\n\n         [[ 6.9433e-01, -4.9916e-01, -4.4279e-01,  ..., -5.7247e-01,\n           -3.3100e-01, -3.8216e-01],\n          [ 4.1680e-01, -7.0314e-01, -1.6640e-01,  ..., -2.2614e-01,\n           -2.6686e-01, -4.8208e-01],\n          [ 6.1199e-01, -6.5476e-01, -8.7278e-02,  ..., -2.3553e-01,\n           -2.7158e-01, -5.1611e-01],\n          ...,\n          [ 4.5634e-01, -3.9684e-01, -2.4713e-01,  ..., -4.2709e-01,\n            5.6571e-01, -1.1508e-01],\n          [ 4.6057e-01, -3.9742e-01, -2.5194e-01,  ..., -4.2371e-01,\n            5.6429e-01, -1.0829e-01],\n          [ 4.6453e-01, -3.9508e-01, -2.4740e-01,  ..., -4.2096e-01,\n            5.6205e-01, -1.0023e-01]],\n\n         [[-1.7564e-02,  1.6841e-01, -1.5365e-02,  ...,  3.9622e-01,\n           -5.1738e-03, -2.1409e-01],\n          [-1.7510e-01, -1.7950e-01,  2.1199e-01,  ...,  2.9397e-01,\n            7.3593e-02, -6.0202e-03],\n          [-5.1427e-01, -7.3869e-02, -2.9637e-01,  ...,  1.4750e-01,\n           -1.3506e-01, -6.1788e-02],\n          ...,\n          [-7.2435e-02, -4.5078e-04,  1.1932e-02,  ...,  3.5875e-01,\n           -1.2987e-01, -3.1713e-02],\n          [-6.1103e-02, -1.4442e-02, -8.6499e-03,  ...,  3.2552e-01,\n           -1.1049e-01, -3.7206e-02],\n          [-5.2618e-02, -1.2136e-02, -2.4083e-02,  ...,  2.9125e-01,\n           -9.0260e-02, -4.3381e-02]]]]))), hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:48.003172Z","iopub.execute_input":"2024-10-24T16:44:48.003718Z","iopub.status.idle":"2024-10-24T16:44:48.011108Z","shell.execute_reply.started":"2024-10-24T16:44:48.003670Z","shell.execute_reply":"2024-10-24T16:44:48.010208Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 306, 128256])"},"metadata":{}}]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:45:01.905077Z","iopub.execute_input":"2024-10-24T16:45:01.905523Z","iopub.status.idle":"2024-10-24T16:45:01.958682Z","shell.execute_reply.started":"2024-10-24T16:45:01.905481Z","shell.execute_reply":"2024-10-24T16:45:01.957868Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:45:03.626424Z","iopub.execute_input":"2024-10-24T16:45:03.627110Z","iopub.status.idle":"2024-10-24T16:45:03.633990Z","shell.execute_reply.started":"2024-10-24T16:45:03.627048Z","shell.execute_reply":"2024-10-24T16:45:03.633058Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"'....Tags..................................................'"},"metadata":{}}]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:44:48.087127Z","iopub.execute_input":"2024-10-24T16:44:48.087547Z","iopub.status.idle":"2024-10-24T16:44:48.100215Z","shell.execute_reply.started":"2024-10-24T16:44:48.087499Z","shell.execute_reply":"2024-10-24T16:44:48.099332Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 306, 128256])"},"metadata":{}}]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = torch.randn(7484, 1, 1)\n\n# works as we are expanding singleton dimensions\nb = a.expand(-1, 100, 200)\nprint(b.shape)\n# torch.Size([7484, 100, 200])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fails\nb = a.expand(19, 100, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}