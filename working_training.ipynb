{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442},{"sourceId":9732867,"sourceType":"datasetVersion","datasetId":5945412},{"sourceId":9734757,"sourceType":"datasetVersion","datasetId":5957788}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T06:19:36.406137Z","iopub.execute_input":"2024-10-27T06:19:36.406531Z","iopub.status.idle":"2024-10-27T06:19:36.416369Z","shell.execute_reply.started":"2024-10-27T06:19:36.406498Z","shell.execute_reply":"2024-10-27T06:19:36.414983Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl\n/kaggle/input/sample-cap/sample.csv\n/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip -q install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:19:39.682616Z","iopub.execute_input":"2024-10-27T06:19:39.683008Z","iopub.status.idle":"2024-10-27T06:19:50.497553Z","shell.execute_reply.started":"2024-10-27T06:19:39.682975Z","shell.execute_reply":"2024-10-27T06:19:50.495825Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:19:50.499739Z","iopub.execute_input":"2024-10-27T06:19:50.500125Z","iopub.status.idle":"2024-10-27T06:20:01.395203Z","shell.execute_reply.started":"2024-10-27T06:19:50.500086Z","shell.execute_reply":"2024-10-27T06:20:01.393704Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your .pkl file\nfile_path = '/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl'\n\n# Load the embeddings from the .pkl file\nwith open(file_path, 'rb') as file:\n    embeddings = pickle.load(file)\n\n\n# import pickle\n\n# # Specify the path to your .pkl file\n# file_path = '/kaggle/input/llava-processed/final_clip_embeddings_part_1.pkl'\n\n# # Load the embeddings from the .pkl file\n# with open(file_path, 'rb') as file:\n#     embeddings_1 = pickle.load(file)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:20:01.397508Z","iopub.execute_input":"2024-10-27T06:20:01.397928Z","iopub.status.idle":"2024-10-27T06:20:01.426737Z","shell.execute_reply.started":"2024-10-27T06:20:01.397888Z","shell.execute_reply":"2024-10-27T06:20:01.425773Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:21:08.413330Z","iopub.execute_input":"2024-10-27T06:21:08.414682Z","iopub.status.idle":"2024-10-27T06:21:08.422347Z","shell.execute_reply.started":"2024-10-27T06:21:08.414632Z","shell.execute_reply":"2024-10-27T06:21:08.421071Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}]},{"cell_type":"code","source":"from sys import getsizeof\n\n\ngetsizeof(embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:17:00.884857Z","iopub.execute_input":"2024-10-27T06:17:00.885809Z","iopub.status.idle":"2024-10-27T06:17:00.891502Z","shell.execute_reply.started":"2024-10-27T06:17:00.885752Z","shell.execute_reply":"2024-10-27T06:17:00.890085Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:28:08.545218Z","iopub.execute_input":"2024-10-27T06:28:08.545889Z","iopub.status.idle":"2024-10-27T06:28:08.555400Z","shell.execute_reply.started":"2024-10-27T06:28:08.545827Z","shell.execute_reply":"2024-10-27T06:28:08.553963Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:28:10.839203Z","iopub.execute_input":"2024-10-27T06:28:10.839728Z","iopub.status.idle":"2024-10-27T06:28:11.187250Z","shell.execute_reply.started":"2024-10-27T06:28:10.839661Z","shell.execute_reply":"2024-10-27T06:28:11.185825Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e986ea6cd6248d4b7e2572ad0bb97f6"}},"metadata":{}}]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T06:28:28.746229Z","iopub.execute_input":"2024-10-27T06:28:28.746947Z","iopub.status.idle":"2024-10-27T06:29:33.990865Z","shell.execute_reply.started":"2024-10-27T06:28:28.746905Z","shell.execute_reply":"2024-10-27T06:29:33.989705Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62852d25bcde4968ade4049e5424421e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c7a06a8655422a9e35aed3307714a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7882dbd9eb4869b3f0457d1b02da1e"}},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n        self.config = phi2_model.config\n\n    def forward(self, image_embeddings=None,\n                inputs_embeds=None,\n                input_ids=None,\n                attention_mask=None,\n                labels=None,\n                output_attentions=False, \n        output_hidden_states=False, \n        **kwargs):  # Catch any additional arguments):\n        \n        if input_ids is not None:\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        elif inputs_embeds is not None:\n            token_embeddings = inputs_embeds\n        else:\n            raise ValueError(\"You must provide either input_ids or inputs_embeds.\")\n\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n\n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            \n            # Combine image and token embeddings\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            \n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), -100, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([label_padding,labels], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels, output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            **kwargs)\n\n        return outputs\n    \n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        # Generate inputs with projections where necessary\n        if image_embeddings is not None:\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)\n            new_attention_mask = torch.cat([torch.ones_like(projected_image_embeddings[..., :1]), attention_mask], dim=1)\n        else:\n            combined_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            new_attention_mask = attention_mask\n\n        return {\n            \"inputs_embeds\": combined_embeddings,\n            \"attention_mask\": new_attention_mask,\n            **kwargs\n        }\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n\n\nfrom peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Wrap PHI 2 with QLoRA\n# phi_lora_model = LoraModel(model, lora_config,\"default\")\n\n\n\n\nphi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:13.660861Z","iopub.execute_input":"2024-10-27T09:37:13.661436Z","iopub.status.idle":"2024-10-27T09:37:13.785420Z","shell.execute_reply.started":"2024-10-27T09:37:13.661392Z","shell.execute_reply":"2024-10-27T09:37:13.784107Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:14.148917Z","iopub.execute_input":"2024-10-27T09:37:14.149314Z","iopub.status.idle":"2024-10-27T09:37:14.164635Z","shell.execute_reply.started":"2024-10-27T09:37:14.149282Z","shell.execute_reply":"2024-10-27T09:37:14.163441Z"},"trusted":true},"execution_count":182,"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:14.889970Z","iopub.execute_input":"2024-10-27T09:37:14.890372Z","iopub.status.idle":"2024-10-27T09:37:14.901544Z","shell.execute_reply.started":"2024-10-27T09:37:14.890337Z","shell.execute_reply":"2024-10-27T09:37:14.898881Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 1,240,401,920 || trainable%: 0.1374\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:15.316896Z","iopub.execute_input":"2024-10-27T09:37:15.317301Z","iopub.status.idle":"2024-10-27T09:37:15.326592Z","shell.execute_reply.started":"2024-10-27T09:37:15.317266Z","shell.execute_reply":"2024-10-27T09:37:15.325066Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:15.984031Z","iopub.execute_input":"2024-10-27T09:37:15.984517Z","iopub.status.idle":"2024-10-27T09:37:15.994675Z","shell.execute_reply.started":"2024-10-27T09:37:15.984475Z","shell.execute_reply":"2024-10-27T09:37:15.993077Z"},"trusted":true},"execution_count":185,"outputs":[{"name":"stdout","text":"trainable params: 4,587,520 || all params: 1,240,401,920 || trainable%: 0.3698\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import CLIPModel\n\n# Load CLIP and PHI2\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:16.545062Z","iopub.execute_input":"2024-10-27T09:37:16.545575Z","iopub.status.idle":"2024-10-27T09:37:17.390976Z","shell.execute_reply.started":"2024-10-27T09:37:16.545533Z","shell.execute_reply":"2024-10-27T09:37:17.389603Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"\n# # Now the model can be trained, and the optimizer only updates LoRA and projection\n# optimizer = torch.optim.AdamW(\n#     [p for p in combined_model.parameters() if p.requires_grad], lr=1e-4\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:17.394630Z","iopub.execute_input":"2024-10-27T09:37:17.395317Z","iopub.status.idle":"2024-10-27T09:37:17.401790Z","shell.execute_reply.started":"2024-10-27T09:37:17.395251Z","shell.execute_reply":"2024-10-27T09:37:17.400115Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"\n# # Training loop\n# def train_model(combined_model, data_loader, optimizer, num_epochs=1, device=\"cuda\"):\n#     combined_model.train()\n#     combined_model = combined_model.to(device)\n    \n#     for epoch in range(num_epochs):\n#         total_loss = 0\n#         for batch in data_loader:\n#             image_embeddings = batch['image_embeddings'].to(device)\n#             input_ids = batch['input_ids'].to(device)\n#             labels = batch['labels'].to(device)\n            \n#             # Forward pass\n#             optimizer.zero_grad()\n#             outputs = combined_model(image_embeddings, input_ids)\n            \n#             # Assume outputs is a tuple where the first element is logits\n#             logits = outputs.logits\n            \n#             # Flatten the logits and labels for cross-entropy loss\n#             logits = logits.view(-1, logits.size(-1))\n#             labels = labels.view(-1)\n            \n#             # Calculate loss (cross-entropy loss for language modeling)\n#             loss = F.cross_entropy(logits, labels)\n#             total_loss += loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             optimizer.step()\n        \n#         avg_loss = total_loss / len(data_loader)\n#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# # Usage\n# data_loader = get_data_loader(batch_size=16)  # Adjust the batch size as needed\n# train_model(combined_model, data_loader, optimizer, num_epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:17.768114Z","iopub.execute_input":"2024-10-27T09:37:17.768837Z","iopub.status.idle":"2024-10-27T09:37:17.775785Z","shell.execute_reply.started":"2024-10-27T09:37:17.768782Z","shell.execute_reply":"2024-10-27T09:37:17.774356Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"# for batch in data_loader:\n#     image_embeddings = batch['image_embeddings'].to(device)  # Assuming pre-extracted embeddings\n#     input_ids = batch['input_ids'].to(device)  # Tokenized text input\n#     labels = batch['labels'].to(device)  # Labels for training\n    \n#     # Forward pass through the model\n#     optimizer.zero_grad()\n#     outputs = combined_model(image_embeddings, input_ids)\n    \n#     # Get logits and calculate loss\n#     logits = outputs.logits.view(-1, logits.size(-1))\n#     labels = labels.view(-1)\n#     loss = F.cross_entropy(logits, labels)\n    \n#     # Backward pass and optimization\n#     loss.backward()\n#     optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:18.293571Z","iopub.execute_input":"2024-10-27T09:37:18.294949Z","iopub.status.idle":"2024-10-27T09:37:18.301378Z","shell.execute_reply.started":"2024-10-27T09:37:18.294896Z","shell.execute_reply":"2024-10-27T09:37:18.299594Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:18.998254Z","iopub.execute_input":"2024-10-27T09:37:18.998788Z","iopub.status.idle":"2024-10-27T09:37:19.004553Z","shell.execute_reply.started":"2024-10-27T09:37:18.998747Z","shell.execute_reply":"2024-10-27T09:37:19.003427Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"# import pickle\n\n# # Specify the path to your .pkl file\n# file_path = '/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl'\n\n# # Load the embeddings from the .pkl file\n# with open(file_path, 'rb') as file:\n#     embeddings = pickle.load(file)\n\n# # Now you can use your embeddings\n# # print(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:20.406247Z","iopub.execute_input":"2024-10-27T09:37:20.407294Z","iopub.status.idle":"2024-10-27T09:37:20.411841Z","shell.execute_reply.started":"2024-10-27T09:37:20.407248Z","shell.execute_reply":"2024-10-27T09:37:20.410795Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sample-cap/turns_50_sample.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:21.330975Z","iopub.execute_input":"2024-10-27T09:37:21.331403Z","iopub.status.idle":"2024-10-27T09:37:21.348178Z","shell.execute_reply.started":"2024-10-27T09:37:21.331360Z","shell.execute_reply":"2024-10-27T09:37:21.346955Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import CLIPProcessor, CLIPModel\n# from transformers import Trainer, TrainingArguments\n# from datasets import Dataset\n# from torch.utils.data import Dataset as TorchDataset\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from PIL import Image\n\n# # Initialize the tokenizer and image model\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\n# class CustomDataset(TorchDataset):\n#     def __init__(self, image_paths, text_inputs, text_labels):\n#         self.image_paths = image_paths\n#         self.text_inputs = text_inputs\n#         self.text_labels = text_labels\n#         self.max_length = 256\n\n#     def __len__(self):\n#         return len(self.text_labels)\n\n#     def __getitem__(self, idx):\n#         image = Image.open(self.image_paths[idx])\n#         inputs = clip_processor(images=image, return_tensors=\"pt\")\n#         image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n#         image_features = feature_select(image_forward_outs)\n#         image_embedding = image_features.squeeze(0).to(device)\n        \n#         # Tokenize text input\n#         input_encoding = tokenizer(\n#             self.text_inputs[idx].replace('<image>',' '),\n#             return_tensors='pt',\n#             padding='max_length',  # Pad to max length\n#             truncation=True,  # Truncate if needed\n#             max_length=self.max_length\n#         )\n        \n#         # Tokenize text label (similar to inputs)\n#         label_encoding = tokenizer(\n#             self.text_labels[idx],\n#             return_tensors='pt',\n#             padding='max_length',\n#             truncation=True,\n#             max_length=self.max_length\n#         )\n\n#         # Extract input_ids and attention_mask for both inputs and labels\n#         input_ids = input_encoding['input_ids'].squeeze(0)\n#         input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n#         label_ids = label_encoding['input_ids'].squeeze(0)\n#         label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n#         # Return the image embeddings, tokenized inputs/labels, and attention masks\n#         return {\n#             'image_embeddings': image_embedding,  # Precomputed image embedding\n#             'input_ids': input_ids,  # Tokenized input\n#             'attention_mask': input_attention_mask,  # Attention mask for input\n#             'labels': label_ids,  # Tokenized label\n# #             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n#         }\n\n# # Create dataset (you will replace this with actual paths and data)\n# image_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\n# text_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\n# text_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# # Instantiate dataset\n# dataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:37:21.965843Z","iopub.execute_input":"2024-10-27T09:37:21.967072Z","iopub.status.idle":"2024-10-27T09:37:21.974911Z","shell.execute_reply.started":"2024-10-27T09:37:21.967021Z","shell.execute_reply":"2024-10-27T09:37:21.973246Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image_embedding = embeddings[self.image_paths[idx]]\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n#             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:40:18.876814Z","iopub.execute_input":"2024-10-27T09:40:18.877432Z","iopub.status.idle":"2024-10-27T09:40:21.969566Z","shell.execute_reply.started":"2024-10-27T09:40:18.877386Z","shell.execute_reply":"2024-10-27T09:40:21.968295Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:40:21.971542Z","iopub.execute_input":"2024-10-27T09:40:21.971906Z","iopub.status.idle":"2024-10-27T09:40:22.000277Z","shell.execute_reply.started":"2024-10-27T09:40:21.971872Z","shell.execute_reply":"2024-10-27T09:40:21.999291Z"},"trusted":true},"execution_count":199,"outputs":[{"execution_count":199,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7aebe538cd00>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:40:22.001649Z","iopub.execute_input":"2024-10-27T09:40:22.002082Z","iopub.status.idle":"2024-10-27T09:40:22.011466Z","shell.execute_reply.started":"2024-10-27T09:40:22.002040Z","shell.execute_reply":"2024-10-27T09:40:22.010131Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=4,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n# mlp_projection = MLPProjection(input_dim=512, output_dim=768, hidden_dim=1024, num_layers=2)\n# model = PHI2WithMLP(mlp_projection,phi_lora_model = phi_lora_model)\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:40:22.013640Z","iopub.execute_input":"2024-10-27T09:40:22.014061Z","iopub.status.idle":"2024-10-27T10:26:24.113455Z","shell.execute_reply.started":"2024-10-27T09:40:22.014026Z","shell.execute_reply":"2024-10-27T10:26:24.110431Z"},"trusted":true},"execution_count":201,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 45:13, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":201,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=57, training_loss=1.3543715895267956, metrics={'train_runtime': 2761.5686, 'train_samples_per_second': 0.081, 'train_steps_per_second': 0.021, 'total_flos': 0.0, 'train_loss': 1.3543715895267956, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"image_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.118734Z","iopub.execute_input":"2024-10-27T10:26:24.119324Z","iopub.status.idle":"2024-10-27T10:26:24.133019Z","shell.execute_reply.started":"2024-10-27T10:26:24.119269Z","shell.execute_reply":"2024-10-27T10:26:24.131672Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.137023Z","iopub.execute_input":"2024-10-27T10:26:24.137467Z","iopub.status.idle":"2024-10-27T10:26:24.156698Z","shell.execute_reply.started":"2024-10-27T10:26:24.137422Z","shell.execute_reply":"2024-10-27T10:26:24.155529Z"},"trusted":true},"execution_count":203,"outputs":[{"execution_count":203,"output_type":"execute_result","data":{"text/plain":"{'image_embeddings': tensor([[ 0.3817,  0.9930,  0.2789,  ...,  0.0197, -0.0689, -0.4641],\n         [ 0.3424,  0.9193,  1.4041,  ..., -0.1432, -0.2891,  0.5565],\n         [-0.3738,  0.6973,  0.2574,  ...,  0.1940,  0.2922,  0.6109],\n         ...,\n         [ 0.3682,  0.4731, -0.4251,  ...,  0.5928,  0.2523, -0.6850],\n         [-0.1267,  0.3708, -0.3366,  ...,  0.1251,  0.5551, -0.3766],\n         [ 0.5355,  1.0992, -0.4195,  ...,  0.3650, -0.2440,  0.3265]]),\n 'input_ids': tensor([128000,    198,   3923,    527,    279,   8146,    315,    279,   5951,\n            304,    279,   2217,     30,    791,   5951,    304,    279,   2217,\n            374,   4251,    323,   2579,     13, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor([128000,    198,   3923,    527,    279,   8146,    315,    279,   5951,\n            304,    279,   2217,     30,    791,   5951,    304,    279,   2217,\n            374,   4251,    323,   2579,     13, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009])}"},"metadata":{}}]},{"cell_type":"code","source":"image_path,text_input,text_label # Example text label (if needed for comparison)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.158283Z","iopub.execute_input":"2024-10-27T10:26:24.158636Z","iopub.status.idle":"2024-10-27T10:26:24.166649Z","shell.execute_reply.started":"2024-10-27T10:26:24.158602Z","shell.execute_reply":"2024-10-27T10:26:24.165426Z"},"trusted":true},"execution_count":204,"outputs":[{"execution_count":204,"output_type":"execute_result","data":{"text/plain":"('000000033471.jpg',\n '<image>\\nWhat are the colors of the bus in the image?',\n 'The bus in the image is white and red.')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_path = image_paths[0]\ntext_input = text_inputs[0]  # Example text input\ntext_label = text_labels[0]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimage_embedding = embeddings[image_path]\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.168144Z","iopub.execute_input":"2024-10-27T10:26:24.168571Z","iopub.status.idle":"2024-10-27T10:26:24.181022Z","shell.execute_reply.started":"2024-10-27T10:26:24.168520Z","shell.execute_reply":"2024-10-27T10:26:24.179789Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.182775Z","iopub.execute_input":"2024-10-27T10:26:24.183980Z","iopub.status.idle":"2024-10-27T10:26:24.191852Z","shell.execute_reply.started":"2024-10-27T10:26:24.183919Z","shell.execute_reply":"2024-10-27T10:26:24.190882Z"},"trusted":true},"execution_count":206,"outputs":[{"execution_count":206,"output_type":"execute_result","data":{"text/plain":"torch.Size([49, 768])"},"metadata":{}}]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.193687Z","iopub.execute_input":"2024-10-27T10:26:24.194199Z","iopub.status.idle":"2024-10-27T10:26:24.205289Z","shell.execute_reply.started":"2024-10-27T10:26:24.194153Z","shell.execute_reply":"2024-10-27T10:26:24.203829Z"},"trusted":true},"execution_count":207,"outputs":[{"execution_count":207,"output_type":"execute_result","data":{"text/plain":"torch.Size([256])"},"metadata":{}}]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.208990Z","iopub.execute_input":"2024-10-27T10:26:24.209392Z","iopub.status.idle":"2024-10-27T10:26:24.215757Z","shell.execute_reply.started":"2024-10-27T10:26:24.209356Z","shell.execute_reply":"2024-10-27T10:26:24.214361Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.217809Z","iopub.execute_input":"2024-10-27T10:26:24.218506Z","iopub.status.idle":"2024-10-27T10:26:24.232362Z","shell.execute_reply.started":"2024-10-27T10:26:24.218446Z","shell.execute_reply":"2024-10-27T10:26:24.231069Z"},"trusted":true},"execution_count":209,"outputs":[{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_embedding)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.234013Z","iopub.execute_input":"2024-10-27T10:26:24.234629Z","iopub.status.idle":"2024-10-27T10:26:24.245450Z","shell.execute_reply.started":"2024-10-27T10:26:24.234585Z","shell.execute_reply":"2024-10-27T10:26:24.243825Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.247324Z","iopub.execute_input":"2024-10-27T10:26:24.247671Z","iopub.status.idle":"2024-10-27T10:26:24.255673Z","shell.execute_reply.started":"2024-10-27T10:26:24.247638Z","shell.execute_reply":"2024-10-27T10:26:24.254467Z"},"trusted":true},"execution_count":211,"outputs":[{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"(torch.Size([49, 768]), torch.Size([256, 2048]), torch.Size([49, 2048]))"},"metadata":{}}]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:24.257081Z","iopub.execute_input":"2024-10-27T10:26:24.258155Z","iopub.status.idle":"2024-10-27T10:26:24.267476Z","shell.execute_reply.started":"2024-10-27T10:26:24.258092Z","shell.execute_reply":"2024-10-27T10:26:24.266374Z"},"trusted":true},"execution_count":212,"outputs":[{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"torch.Size([305, 2048])"},"metadata":{}}]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:48.861409Z","iopub.execute_input":"2024-10-27T10:26:48.861947Z","iopub.status.idle":"2024-10-27T10:26:48.971963Z","shell.execute_reply.started":"2024-10-27T10:26:48.861905Z","shell.execute_reply":"2024-10-27T10:26:48.970496Z"},"trusted":true},"execution_count":213,"outputs":[{"execution_count":213,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=None)  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\n# print(f\"Predicted labels: {predicted_labels}\")\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_labels[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:26:49.904511Z","iopub.execute_input":"2024-10-27T10:26:49.904948Z","iopub.status.idle":"2024-10-27T10:26:54.867059Z","shell.execute_reply.started":"2024-10-27T10:26:49.904911Z","shell.execute_reply":"2024-10-27T10:26:54.865802Z"},"trusted":true},"execution_count":214,"outputs":[{"name":"stdout","text":"Predicted text: Tagstemplate>\n  is the main of the image?\n the image?\n \n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n?\n?\n?\n?\n?\n\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n>\n>\n>\n?\n?\n?\n?\n?\n?\n\n\n>\n>\n>\n>\n?\n?\n?\n>\n>\n>\n\n\n\n>\n>\n>\n>\n}\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n}\n}\n}\n>\n>\n>\n}\n}\n}\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\nll>\n>\n>\n>\nlll\n\n>\nl}\nl\n\n>\n>\nlll\n\n\n\n}\n}\nll}\n\n\n\n\n\n\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n</</>\n\n>\n\n>\n\n>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n>\n>\n}\nÑŒ\n\nÑŒÑŒÑŒÑŒÑŒ>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n}\n}\n}\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:27:00.670139Z","iopub.execute_input":"2024-10-27T10:27:00.670564Z","iopub.status.idle":"2024-10-27T10:27:06.526224Z","shell.execute_reply.started":"2024-10-27T10:27:00.670530Z","shell.execute_reply":"2024-10-27T10:27:06.524921Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stdout","text":"Predicted labels: tensor([[128009, 128009,     13,    304, 128009, 128009, 128009, 128009, 128009,\n         128009,    374,    264, 128009, 128009, 128009, 128009,    304,    279,\n         128009, 128009, 128009, 128009, 128009,    304,    264, 128009, 128009,\n         128009,    374, 128009,    304,    304, 128009, 128009,     13, 128009,\n            304,    304, 128009,     13, 128009, 128009, 128009,    279, 128009,\n         128009, 128009, 128009, 128009,  14924, 128009, 128009,    791,    374,\n            279,   2035,    527,    279,  12235, 128009,    279,   1176, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:27:11.982577Z","iopub.execute_input":"2024-10-27T10:27:11.983068Z","iopub.status.idle":"2024-10-27T10:27:12.037152Z","shell.execute_reply.started":"2024-10-27T10:27:11.983029Z","shell.execute_reply":"2024-10-27T10:27:12.035993Z"},"trusted":true},"execution_count":216,"outputs":[{"name":"stdout","text":"Predicted text: . in is a in the in a is in in. in in. theQuestionThe is the place are the plate the first\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 256\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_args,\n    data_collator=data_collator\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}