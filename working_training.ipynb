{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442},{"sourceId":9732867,"sourceType":"datasetVersion","datasetId":5945412},{"sourceId":9734757,"sourceType":"datasetVersion","datasetId":5957788}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install -U bitsandbytes peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install --upgrade huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your .pkl file\nfile_path = '/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl'\n\n# Load the embeddings from the .pkl file\nwith open(file_path, 'rb') as file:\n    embeddings = pickle.load(file)\n\n\n# import pickle\n\n# # Specify the path to your .pkl file\n# file_path = '/kaggle/input/llava-processed/final_clip_embeddings_part_1.pkl'\n\n# # Load the embeddings from the .pkl file\n# with open(file_path, 'rb') as file:\n#     embeddings_1 = pickle.load(file)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sys import getsizeof\n\n\ngetsizeof(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n)\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bnb_config,\n    torch_dtype = torch.float32,\n    trust_remote_code=True\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n        self.config = phi2_model.config\n\n    def forward(self, image_embeddings=None,\n                inputs_embeds=None,\n                input_ids=None,\n                attention_mask=None,\n                labels=None,\n                output_attentions=False, \n        output_hidden_states=False, \n        **kwargs):  # Catch any additional arguments):\n        \n        if input_ids is not None:\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        elif inputs_embeds is not None:\n            token_embeddings = inputs_embeds\n        else:\n            raise ValueError(\"You must provide either input_ids or inputs_embeds.\")\n\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings).to(device = token_embeddings.device)\n            \n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            \n            # Combine image and token embeddings\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            \n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), 1, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([label_padding,labels], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels, output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            **kwargs)\n\n        return outputs\n    \n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        # Generate inputs with projections where necessary\n        if image_embeddings is not None:\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)\n            new_attention_mask = torch.cat([torch.ones_like(projected_image_embeddings[..., :1]), attention_mask], dim=1)\n        else:\n            combined_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            new_attention_mask = attention_mask\n\n        return {\n            \"inputs_embeds\": combined_embeddings,\n            \"attention_mask\": new_attention_mask,\n            **kwargs\n        }\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2).to(device)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n\n\nfrom peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\n\nphi_lora_model = get_peft_model(combined_model, lora_config).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True\n\nphi_lora_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import CLIPModel\n\n# # Load CLIP and PHI2\n# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle\n\n# # Specify the path to your .pkl file\n# file_path = '/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl'\n\n# # Load the embeddings from the .pkl file\n# with open(file_path, 'rb') as file:\n#     embeddings = pickle.load(file)\n\n# # Now you can use your embeddings\n# # print(embeddings)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sub-capstone/turns_60k_sample.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import CLIPProcessor, CLIPModel\n# from transformers import Trainer, TrainingArguments\n# from datasets import Dataset\n# from torch.utils.data import Dataset as TorchDataset\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from PIL import Image\n\n# # Initialize the tokenizer and image model\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\n# class CustomDataset(TorchDataset):\n#     def __init__(self, image_paths, text_inputs, text_labels):\n#         self.image_paths = image_paths\n#         self.text_inputs = text_inputs\n#         self.text_labels = text_labels\n#         self.max_length = 256\n\n#     def __len__(self):\n#         return len(self.text_labels)\n\n#     def __getitem__(self, idx):\n#         image = Image.open(self.image_paths[idx])\n#         inputs = clip_processor(images=image, return_tensors=\"pt\")\n#         image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n#         image_features = feature_select(image_forward_outs)\n#         image_embedding = image_features.squeeze(0).to(device)\n        \n#         # Tokenize text input\n#         input_encoding = tokenizer(\n#             self.text_inputs[idx].replace('<image>',' '),\n#             return_tensors='pt',\n#             padding='max_length',  # Pad to max length\n#             truncation=True,  # Truncate if needed\n#             max_length=self.max_length\n#         )\n        \n#         # Tokenize text label (similar to inputs)\n#         label_encoding = tokenizer(\n#             self.text_labels[idx],\n#             return_tensors='pt',\n#             padding='max_length',\n#             truncation=True,\n#             max_length=self.max_length\n#         )\n\n#         # Extract input_ids and attention_mask for both inputs and labels\n#         input_ids = input_encoding['input_ids'].squeeze(0)\n#         input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n#         label_ids = label_encoding['input_ids'].squeeze(0)\n#         label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n#         # Return the image embeddings, tokenized inputs/labels, and attention masks\n#         return {\n#             'image_embeddings': image_embedding,  # Precomputed image embedding\n#             'input_ids': input_ids,  # Tokenized input\n#             'attention_mask': input_attention_mask,  # Attention mask for input\n#             'labels': label_ids,  # Tokenized label\n# #             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n#         }\n\n# # Create dataset (you will replace this with actual paths and data)\n# image_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\n# text_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\n# text_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# # Instantiate dataset\n# dataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image_embedding = embeddings[self.image_paths[idx]]\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n#             'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 1000\nlogging_steps = 10\nlearning_rate = 1e-4\nmax_grad_norm = 0.3\nwarmup_ratio = 0.03\nlr_scheduler_type = \"cosine\"\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=1,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    lr_scheduler_type=lr_scheduler_type,\n    fp16=False,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path,text_input,text_label # Example text label (if needed for comparison)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_path = image_paths[0]\ntext_input = text_inputs[0]  # Example text input\ntext_label = text_labels[0]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimage_embedding = embeddings[image_path]\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=None)  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\n# print(f\"Predicted labels: {predicted_labels}\")\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_labels[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 256\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_args,\n    data_collator=data_collator\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = phi_lora_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}