{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T11:28:26.499722Z","iopub.execute_input":"2024-10-24T11:28:26.500157Z","iopub.status.idle":"2024-10-24T11:28:26.884870Z","shell.execute_reply.started":"2024-10-24T11:28:26.500116Z","shell.execute_reply":"2024-10-24T11:28:26.883846Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:28:32.010138Z","iopub.execute_input":"2024-10-24T11:28:32.010701Z","iopub.status.idle":"2024-10-24T11:28:50.275578Z","shell.execute_reply.started":"2024-10-24T11:28:32.010663Z","shell.execute_reply":"2024-10-24T11:28:50.274233Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:28:50.277651Z","iopub.execute_input":"2024-10-24T11:28:50.278027Z","iopub.status.idle":"2024-10-24T11:29:02.614386Z","shell.execute_reply.started":"2024-10-24T11:28:50.277989Z","shell.execute_reply":"2024-10-24T11:29:02.613193Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\nDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.25.1\n    Uninstalling huggingface-hub-0.25.1:\n      Successfully uninstalled huggingface-hub-0.25.1\nSuccessfully installed huggingface_hub-0.26.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:34:33.077047Z","iopub.execute_input":"2024-10-24T11:34:33.077861Z","iopub.status.idle":"2024-10-24T11:34:33.101844Z","shell.execute_reply.started":"2024-10-24T11:34:33.077798Z","shell.execute_reply":"2024-10-24T11:34:33.100915Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d863e886fc684b0e9234b2b1645c251d"}},"metadata":{}}]},{"cell_type":"code","source":"def feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:46:57.046530Z","iopub.execute_input":"2024-10-24T11:46:57.047784Z","iopub.status.idle":"2024-10-24T11:46:57.053464Z","shell.execute_reply.started":"2024-10-24T11:46:57.047735Z","shell.execute_reply":"2024-10-24T11:46:57.052626Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:34:49.647612Z","iopub.execute_input":"2024-10-24T11:34:49.648056Z","iopub.status.idle":"2024-10-24T11:38:51.286337Z","shell.execute_reply.started":"2024-10-24T11:34:49.648016Z","shell.execute_reply":"2024-10-24T11:38:51.285561Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fe5448b7234639bb76c1184071efc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6921686eaba24a62b17a5491cbe3da88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e58e13a0824dc5ba887b6355fc1041"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:38:51.288105Z","iopub.execute_input":"2024-10-24T11:38:51.288589Z","iopub.status.idle":"2024-10-24T11:38:51.296537Z","shell.execute_reply.started":"2024-10-24T11:38:51.288553Z","shell.execute_reply":"2024-10-24T11:38:51.295605Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n\n    def forward(self, image_embeddings=None, input_ids=None, attention_mask=None, labels=None):\n       \n        # Get token embeddings from PHI2 model\n        token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n\n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            print(attention_mask.shape)\n            print(batch_size, text_sequence_length)\n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            print(new_attention_mask.shape)\n            print(projected_image_embeddings.shape,token_embeddings.shape)\n            # Combine image and token embeddings\n            if random.random() < 0.5:\n                combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            else:\n                combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=1)\n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask)\n\n        return outputs\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:58.470281Z","iopub.execute_input":"2024-10-24T11:39:58.470848Z","iopub.status.idle":"2024-10-24T11:39:58.519923Z","shell.execute_reply.started":"2024-10-24T11:39:58.470785Z","shell.execute_reply":"2024-10-24T11:39:58.519099Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\"\n)\n\n# Wrap PHI 2 with QLoRA\n# phi_lora_model = LoraModel(model, lora_config,\"default\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:09.590480Z","iopub.execute_input":"2024-10-24T11:40:09.591273Z","iopub.status.idle":"2024-10-24T11:40:09.739884Z","shell.execute_reply.started":"2024-10-24T11:40:09.591231Z","shell.execute_reply":"2024-10-24T11:40:09.739068Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"phi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:12.481425Z","iopub.execute_input":"2024-10-24T11:40:12.481874Z","iopub.status.idle":"2024-10-24T11:40:12.619782Z","shell.execute_reply.started":"2024-10-24T11:40:12.481817Z","shell.execute_reply":"2024-10-24T11:40:12.618889Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:14.135238Z","iopub.execute_input":"2024-10-24T11:40:14.136243Z","iopub.status.idle":"2024-10-24T11:40:14.147505Z","shell.execute_reply.started":"2024-10-24T11:40:14.136195Z","shell.execute_reply":"2024-10-24T11:40:14.146722Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:32.216512Z","iopub.execute_input":"2024-10-24T11:40:32.216960Z","iopub.status.idle":"2024-10-24T11:40:32.224258Z","shell.execute_reply.started":"2024-10-24T11:40:32.216920Z","shell.execute_reply":"2024-10-24T11:40:32.223476Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:33.388388Z","iopub.execute_input":"2024-10-24T11:40:33.389281Z","iopub.status.idle":"2024-10-24T11:40:33.396268Z","shell.execute_reply.started":"2024-10-24T11:40:33.389235Z","shell.execute_reply":"2024-10-24T11:40:33.395418Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n\ntrainable params: 2,179,072 || all params: 1,547,205,888 || trainable%: 0.1408\n\n\r\n","metadata":{}},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:36.407026Z","iopub.execute_input":"2024-10-24T11:40:36.407437Z","iopub.status.idle":"2024-10-24T11:40:36.414776Z","shell.execute_reply.started":"2024-10-24T11:40:36.407400Z","shell.execute_reply":"2024-10-24T11:40:36.413863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:37.608719Z","iopub.execute_input":"2024-10-24T11:40:37.609432Z","iopub.status.idle":"2024-10-24T11:40:37.617487Z","shell.execute_reply.started":"2024-10-24T11:40:37.609380Z","shell.execute_reply":"2024-10-24T11:40:37.616595Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPModel\n\n# Load CLIP and PHI2\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:48.301175Z","iopub.execute_input":"2024-10-24T11:40:48.302013Z","iopub.status.idle":"2024-10-24T11:40:52.170644Z","shell.execute_reply.started":"2024-10-24T11:40:48.301969Z","shell.execute_reply":"2024-10-24T11:40:52.169877Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe33f49a375c49e681a0594f85837a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b077863cbfa2493082ae3dee4f77808a"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Now the model can be trained, and the optimizer only updates LoRA and projection\noptimizer = torch.optim.AdamW(\n    [p for p in combined_model.parameters() if p.requires_grad], lr=1e-4\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:52.172392Z","iopub.execute_input":"2024-10-24T11:40:52.172741Z","iopub.status.idle":"2024-10-24T11:40:52.781413Z","shell.execute_reply.started":"2024-10-24T11:40:52.172706Z","shell.execute_reply":"2024-10-24T11:40:52.780602Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n# # Training loop\n# def train_model(combined_model, data_loader, optimizer, num_epochs=1, device=\"cuda\"):\n#     combined_model.train()\n#     combined_model = combined_model.to(device)\n    \n#     for epoch in range(num_epochs):\n#         total_loss = 0\n#         for batch in data_loader:\n#             image_embeddings = batch['image_embeddings'].to(device)\n#             input_ids = batch['input_ids'].to(device)\n#             labels = batch['labels'].to(device)\n            \n#             # Forward pass\n#             optimizer.zero_grad()\n#             outputs = combined_model(image_embeddings, input_ids)\n            \n#             # Assume outputs is a tuple where the first element is logits\n#             logits = outputs.logits\n            \n#             # Flatten the logits and labels for cross-entropy loss\n#             logits = logits.view(-1, logits.size(-1))\n#             labels = labels.view(-1)\n            \n#             # Calculate loss (cross-entropy loss for language modeling)\n#             loss = F.cross_entropy(logits, labels)\n#             total_loss += loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             optimizer.step()\n        \n#         avg_loss = total_loss / len(data_loader)\n#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# # Usage\n# data_loader = get_data_loader(batch_size=16)  # Adjust the batch size as needed\n# train_model(combined_model, data_loader, optimizer, num_epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:40:53.207278Z","iopub.execute_input":"2024-10-24T11:40:53.207891Z","iopub.status.idle":"2024-10-24T11:40:53.321309Z","shell.execute_reply.started":"2024-10-24T11:40:53.207849Z","shell.execute_reply":"2024-10-24T11:40:53.320243Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_loader\u001b[49m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)  \u001b[38;5;66;03m# Adjust the batch size as needed\u001b[39;00m\n\u001b[1;32m     37\u001b[0m train_model(combined_model, data_loader, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'get_data_loader' is not defined"],"ename":"NameError","evalue":"name 'get_data_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# for batch in data_loader:\n#     image_embeddings = batch['image_embeddings'].to(device)  # Assuming pre-extracted embeddings\n#     input_ids = batch['input_ids'].to(device)  # Tokenized text input\n#     labels = batch['labels'].to(device)  # Labels for training\n    \n#     # Forward pass through the model\n#     optimizer.zero_grad()\n#     outputs = combined_model(image_embeddings, input_ids)\n    \n#     # Get logits and calculate loss\n#     logits = outputs.logits.view(-1, logits.size(-1))\n#     labels = labels.view(-1)\n#     loss = F.cross_entropy(logits, labels)\n    \n#     # Backward pass and optimization\n#     loss.backward()\n#     optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:41:02.678949Z","iopub.execute_input":"2024-10-24T11:41:02.679738Z","iopub.status.idle":"2024-10-24T11:41:02.683554Z","shell.execute_reply.started":"2024-10-24T11:41:02.679695Z","shell.execute_reply":"2024-10-24T11:41:02.682889Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 2048\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        inputs = clip_processor(images=image, return_tensors=\"pt\")\n        image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n        image_features = feature_select(image_forward_outs)\n        image_embedding = image_features.squeeze(0).to(device)\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n        \n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n            'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\ntext_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:01:03.124281Z","iopub.execute_input":"2024-10-24T12:01:03.124726Z","iopub.status.idle":"2024-10-24T12:01:05.124967Z","shell.execute_reply.started":"2024-10-24T12:01:03.124686Z","shell.execute_reply":"2024-10-24T12:01:05.124075Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:01:05.126984Z","iopub.execute_input":"2024-10-24T12:01:05.127859Z","iopub.status.idle":"2024-10-24T12:01:05.155604Z","shell.execute_reply.started":"2024-10-24T12:01:05.127788Z","shell.execute_reply":"2024-10-24T12:01:05.154838Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d43d6338910>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollator\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        # Extract image embeddings\n        image_embeddings = torch.stack([item['image_embeddings'] for item in batch])\n        # Extract input_ids and labels\n        input_ids = [item['input_ids'] for item in batch]\n        labels = [item['labels'] for item in batch]\n\n        # Pad the input_ids and labels\n        padded_input_ids = self.tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt')['input_ids']\n        padded_labels = self.tokenizer.pad({'input_ids': labels}, padding=True, return_tensors='pt')['input_ids']\n\n        # Create attention masks\n        input_attention_mask = (padded_input_ids != self.tokenizer.pad_token_id).type(torch.float)\n        label_attention_mask = (padded_labels != self.tokenizer.pad_token_id).type(torch.float)\n        \n        # Prepare collated inputs\n        collated_inputs = {\n            'input_ids': padded_input_ids,\n            'labels': padded_labels,\n            'image_embeddings': image_embeddings,\n            'attention_mask': input_attention_mask\n        }\n        \n        return collated_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:01:07.204539Z","iopub.execute_input":"2024-10-24T12:01:07.205558Z","iopub.status.idle":"2024-10-24T12:01:07.214768Z","shell.execute_reply.started":"2024-10-24T12:01:07.205506Z","shell.execute_reply":"2024-10-24T12:01:07.213907Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n# mlp_projection = MLPProjection(input_dim=512, output_dim=768, hidden_dim=1024, num_layers=2)\n# model = PHI2WithMLP(mlp_projection,phi_lora_model = phi_lora_model)\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:01:09.748115Z","iopub.execute_input":"2024-10-24T12:01:09.749103Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_embedding = clip_model.get_image_features(**inputs)\n    images.append(image_embedding)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=4096  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:43.029324Z","iopub.execute_input":"2024-10-24T11:54:43.029765Z","iopub.status.idle":"2024-10-24T11:54:43.193232Z","shell.execute_reply.started":"2024-10-24T11:54:43.029723Z","shell.execute_reply":"2024-10-24T11:54:43.192376Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:45.309200Z","iopub.execute_input":"2024-10-24T11:54:45.309629Z","iopub.status.idle":"2024-10-24T11:54:45.316535Z","shell.execute_reply.started":"2024-10-24T11:54:45.309589Z","shell.execute_reply":"2024-10-24T11:54:45.315745Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"torch.Size([4096])"},"metadata":{}}]},{"cell_type":"code","source":"input_ids.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:46.339087Z","iopub.execute_input":"2024-10-24T11:54:46.339625Z","iopub.status.idle":"2024-10-24T11:54:46.345334Z","shell.execute_reply.started":"2024-10-24T11:54:46.339568Z","shell.execute_reply":"2024-10-24T11:54:46.344374Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"torch.Size([4096])"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:47.270149Z","iopub.execute_input":"2024-10-24T11:54:47.270561Z","iopub.status.idle":"2024-10-24T11:54:47.282186Z","shell.execute_reply.started":"2024-10-24T11:54:47.270518Z","shell.execute_reply":"2024-10-24T11:54:47.281413Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=8, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=8, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:48.599966Z","iopub.execute_input":"2024-10-24T11:54:48.600379Z","iopub.status.idle":"2024-10-24T11:54:48.616074Z","shell.execute_reply.started":"2024-10-24T11:54:48.600343Z","shell.execute_reply":"2024-10-24T11:54:48.615105Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"select_feature = 'cls_patch'","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:49.356909Z","iopub.execute_input":"2024-10-24T11:54:49.357868Z","iopub.status.idle":"2024-10-24T11:54:49.361324Z","shell.execute_reply.started":"2024-10-24T11:54:49.357810Z","shell.execute_reply":"2024-10-24T11:54:49.360582Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:50.706916Z","iopub.execute_input":"2024-10-24T11:54:50.707325Z","iopub.status.idle":"2024-10-24T11:54:50.712766Z","shell.execute_reply.started":"2024-10-24T11:54:50.707282Z","shell.execute_reply":"2024-10-24T11:54:50.711861Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\nimage_features = feature_select(image_forward_outs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:51.725201Z","iopub.execute_input":"2024-10-24T11:54:51.726142Z","iopub.status.idle":"2024-10-24T11:54:51.852456Z","shell.execute_reply.started":"2024-10-24T11:54:51.726090Z","shell.execute_reply":"2024-10-24T11:54:51.851708Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"image_features = image_features.squeeze(0).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:52.787384Z","iopub.execute_input":"2024-10-24T11:54:52.787905Z","iopub.status.idle":"2024-10-24T11:54:52.793007Z","shell.execute_reply.started":"2024-10-24T11:54:52.787852Z","shell.execute_reply":"2024-10-24T11:54:52.792041Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:54:54.402100Z","iopub.execute_input":"2024-10-24T11:54:54.402527Z","iopub.status.idle":"2024-10-24T11:54:54.409156Z","shell.execute_reply.started":"2024-10-24T11:54:54.402485Z","shell.execute_reply":"2024-10-24T11:54:54.408311Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:00.598178Z","iopub.execute_input":"2024-10-24T11:55:00.598575Z","iopub.status.idle":"2024-10-24T11:55:00.636708Z","shell.execute_reply.started":"2024-10-24T11:55:00.598540Z","shell.execute_reply":"2024-10-24T11:55:00.635794Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape,image_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:01.330812Z","iopub.execute_input":"2024-10-24T11:55:01.331856Z","iopub.status.idle":"2024-10-24T11:55:01.338040Z","shell.execute_reply.started":"2024-10-24T11:55:01.331789Z","shell.execute_reply":"2024-10-24T11:55:01.337213Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"(torch.Size([512]),\n torch.Size([4096, 2048]),\n torch.Size([50, 2048]),\n torch.Size([50, 768]))"},"metadata":{}}]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:02.648669Z","iopub.execute_input":"2024-10-24T11:55:02.649121Z","iopub.status.idle":"2024-10-24T11:55:02.688899Z","shell.execute_reply.started":"2024-10-24T11:55:02.649083Z","shell.execute_reply":"2024-10-24T11:55:02.688062Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"torch.Size([4146, 2048])"},"metadata":{}}]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:03.327397Z","iopub.execute_input":"2024-10-24T11:55:03.328444Z","iopub.status.idle":"2024-10-24T11:55:03.427610Z","shell.execute_reply.started":"2024-10-24T11:55:03.328393Z","shell.execute_reply":"2024-10-24T11:55:03.426808Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LlamaSdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): LlamaRotaryEmbedding()\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=False)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=2048, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_features.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:13.168052Z","iopub.execute_input":"2024-10-24T11:55:13.169055Z","iopub.status.idle":"2024-10-24T11:57:03.581937Z","shell.execute_reply.started":"2024-10-24T11:55:13.169009Z","shell.execute_reply":"2024-10-24T11:57:03.581054Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:57:03.584072Z","iopub.execute_input":"2024-10-24T11:57:03.584469Z","iopub.status.idle":"2024-10-24T11:57:03.590743Z","shell.execute_reply.started":"2024-10-24T11:57:03.584430Z","shell.execute_reply":"2024-10-24T11:57:03.589794Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 4146, 128256])"},"metadata":{}}]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=False)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:57:03.592229Z","iopub.execute_input":"2024-10-24T11:57:03.592572Z","iopub.status.idle":"2024-10-24T11:57:04.230206Z","shell.execute_reply.started":"2024-10-24T11:57:03.592538Z","shell.execute_reply":"2024-10-24T11:57:04.228923Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:00:08.201576Z","iopub.execute_input":"2024-10-24T12:00:08.202803Z","iopub.status.idle":"2024-10-24T12:00:08.209002Z","shell.execute_reply.started":"2024-10-24T12:00:08.202756Z","shell.execute_reply":"2024-10-24T12:00:08.208186Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"'Tags is the value of New?\\n Paris<|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|>?\\n?\\n?\\n(((((((((((((((((terterter\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n:\\n\\n::::::::::::::::::::::::::::::::::: : :：：：：：::::：：：：：：：：：：：：：：：： : : : : : :：：：：： : :：：：：：：：：：：：：：：：： : : : :：：：：：：：：： : : : ::： : : ::::: : : : : : : :：：：： : : : : : : : : : : :：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：：： : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：：： :\\xa0 : :   \\xa0\\xa0：： - - : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0   ：：：：：：：：：：：：：： : : : : : : : : : :：：： : : : : : : : : :   ：：：：：：： : : :：：：： : : : : : : : : : :：:::::：：：：：：：：：\\xa0\\xa0\\xa0\\xa0\\xa0：：：：：：：\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：：：：：：：：：：：：：\\xa0\\xa0\\xa0：：：\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：：：：：：：：：：： : : : : : : : : :： : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0 : : : : : : : : : : :\\xa0：： : :：：：： : : :：\\xa0：：： : : : : : : : :\\xa0\\xa0：：：：：：：：：：：：：： : : :\\xa0\\xa0： : : : : : : : : : :：： : : : : : : : : : : :：：：：：： : : : : : : : : : : : : : : : : : : : : : : :：：：： : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 to\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：： : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : : : : : : : : : : : : : : : : : : : : :     : : :    : : : :  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :：： : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0    \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : : :\\xa0\\xa0\\xa0\\xa0 : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : : : : : :：：：： : : : : : : : : : : : : :：：：：： : : :：：：：：：：：：：：：：： : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0 : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0 : : : : : : : : : : : : :： :： : : : : : : : : : : : : : : : : : : : : : : : : : : :：：：：： : : : :：：：：：：：：：：：：：：：：：：：：：：：：： : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :：：：：： : : :：：：：：：：：： : : : : : : : : : : : : : : : : : : : : : : :：： : : : :... : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :    : : : :             : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :   : : : : : : : : : : : : : : :：：： ：：： : : : : : : :：：：：：： : : : : : : : : : : : : : : : : : : : :   : : : :   : : : : : : : : :\\xa0\\xa0\\xa0 : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0201\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：：\\xa0 : : : : : :    : : : : : : : : :  ： : : :    : : : : : : : : : :   : : : : :    : : : :  : : : : : : : : : :\\xa0\\xa0\\xa0 : : : : : : : : : : : : ::：：   : : : : : : : : : : : : : : : : : : : : : : :                 ：      : : : :          : : :          : : : : : : : : : : : :-- : : : :-- : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 :\\xa0\\xa0 : : :\\xa0\\xa0\\xa0\\xa0\\xa0：：：： : : : : : : : : : : : :     : : : :\\xa0 : : : : : : : : :\\xa0\\xa0\\xa0\\xa0 : : :\\xa0\\xa0\\xa0 : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : : : : : :\\xa0\\xa0      : :  : : : : : : : : : : : : : : : : : : :111111111 :    : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :      : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0 : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0        : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0：：：：：：：：：：：：：：： : : : : : : : : :   : : : : : : : : : : : : : : : : : : : : : :：：：：：：：：：：：：：：：：： : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :：：：：：：：：：：：：：：：：：：：：：：：：：：： : : : : :：：：：：：：：：： : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\\xa0 : : :\\xa0\\xa0\\xa0 : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0        \\xa0\\xa0\\xa0\\xa0\\xa0    ：\\xa0\\xa0\\xa0                                                       :    \\xa0\\xa0\\xa0::::\\xa0    ：\\xa0：                                                       : :\\xa0         : : : :   : : : : : : : : : : : : : :\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0::::::::::: :\\xa0\\xa0\\xa0:::::::: :::::::::: :::::：：: :::::::：：：：：::::::::::::::：：：：::::::: : : : ::::::::::::::::::::::::::：：:::::::::::::: : : ::::::::::: :：:::  as as--.-to.. is:- is..--,- is--- is is---------- the- is is- are-- is- is the:::'"},"metadata":{}}]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = torch.randn(7484, 1, 1)\n\n# works as we are expanding singleton dimensions\nb = a.expand(-1, 100, 200)\nprint(b.shape)\n# torch.Size([7484, 100, 200])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fails\nb = a.expand(19, 100, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}