{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442},{"sourceId":9732867,"sourceType":"datasetVersion","datasetId":5945412},{"sourceId":9734757,"sourceType":"datasetVersion","datasetId":5957788}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-10-29T17:39:22.460622Z","iopub.execute_input":"2024-10-29T17:39:22.461024Z","iopub.status.idle":"2024-10-29T17:39:22.471596Z","shell.execute_reply.started":"2024-10-29T17:39:22.460992Z","shell.execute_reply":"2024-10-29T17:39:22.470615Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl\n/kaggle/input/sub-capstone/turns_60k_sample.csv\n/kaggle/input/sample-cap/new_clip_embeddings_part_0.pkl\n/kaggle/input/sample-cap/sample.csv\n/kaggle/input/sample-cap/turns_50_sample.csv\n/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:40:37.842987Z","iopub.execute_input":"2024-10-29T17:40:37.844252Z","iopub.status.idle":"2024-10-29T17:40:37.850824Z","shell.execute_reply.started":"2024-10-29T17:40:37.844193Z","shell.execute_reply":"2024-10-29T17:40:37.849824Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/working/mlp_projection.pth\n/kaggle/working/state.db\n/kaggle/working/my_phi_lora_model/adapter_model.safetensors\n/kaggle/working/my_phi_lora_model/README.md\n/kaggle/working/my_phi_lora_model/adapter_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip -q install -U bitsandbytes peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install --upgrade huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load preprocessed image embedding","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your .pkl file\nfile_path = '/kaggle/input/sub-capstone/sub_clip_embeddings_0.pkl'\n\n# Load the embeddings from the .pkl file\nwith open(file_path, 'rb') as file:\n    embeddings = pickle.load(file)\n\nlen(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:35:59.144017Z","iopub.execute_input":"2024-10-29T17:35:59.144905Z","iopub.status.idle":"2024-10-29T17:36:11.571609Z","shell.execute_reply.started":"2024-10-29T17:35:59.144861Z","shell.execute_reply":"2024-10-29T17:36:11.570634Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"30946"},"metadata":{}}]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:35:55.803286Z","iopub.execute_input":"2024-10-29T17:35:55.804375Z","iopub.status.idle":"2024-10-29T17:35:55.810373Z","shell.execute_reply.started":"2024-10-29T17:35:55.804326Z","shell.execute_reply":"2024-10-29T17:35:55.809177Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:35:52.366496Z","iopub.execute_input":"2024-10-29T17:35:52.366904Z","iopub.status.idle":"2024-10-29T17:35:52.374629Z","shell.execute_reply.started":"2024-10-29T17:35:52.366867Z","shell.execute_reply":"2024-10-29T17:35:52.373508Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# pip install -U bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load model using llama 1b instead of phi because of limited resources","metadata":{}},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig,AutoTokenizer\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n)\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bnb_config,\n    torch_dtype = torch.float32,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:33:54.370663Z","iopub.execute_input":"2024-10-29T17:33:54.371463Z","iopub.status.idle":"2024-10-29T17:35:01.104908Z","shell.execute_reply.started":"2024-10-29T17:33:54.371417Z","shell.execute_reply":"2024-10-29T17:35:01.104102Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22150d2e48794bd9801481e8bd9045b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716c5320e41942d5ba37ff9d4de875bc"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:35:21.921612Z","iopub.execute_input":"2024-10-29T17:35:21.922177Z","iopub.status.idle":"2024-10-29T17:35:23.133891Z","shell.execute_reply.started":"2024-10-29T17:35:21.922116Z","shell.execute_reply":"2024-10-29T17:35:23.133063Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf033ef41b774b98825f627bb235ece8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b8ab4e19184204b35e0c9415104c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4ab630eba242de8c0944cc37f01197"}},"metadata":{}}]},{"cell_type":"markdown","source":"### check inference","metadata":{}},{"cell_type":"code","source":"# Example prompt\nprompt = \"What types of food can be seen in the image?\"\n\n# Tokenize input\ninputs = tokenizer(prompt, return_tensors=\"pt\",truncation = True)\n\n# Generate response\noutputs = model.generate(**inputs, max_new_tokens=128, temperature=0.01, top_p=1)\n\n# Decode output to text\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:35:23.595748Z","iopub.execute_input":"2024-10-29T17:35:23.596615Z","iopub.status.idle":"2024-10-29T17:35:40.748269Z","shell.execute_reply.started":"2024-10-29T17:35:23.596575Z","shell.execute_reply":"2024-10-29T17:35:40.747199Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1935: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Model response: What types of food can be seen in the image? \nUnfortunately, I am unable to see the image you are referring to.  However, I can tell you that the image is likely a photograph of a restaurant or a food establishment.  Based on the fact that it is a restaurant, I would guess that the types of food that can be seen in the image are likely to be a variety of dishes, such as appetizers, entrees, and desserts.  The image may also show a variety of beverages, such as drinks and cocktails.  It is also possible that the image shows a variety of food presentation, such as a buffet or a food display.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## define custom Model with added mlp projection as suggested in llava 1.5","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=768, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n        \n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n        self.config = phi2_model.config\n        \n    def forward(self, image_embeddings=None,\n                inputs_embeds=None,\n                input_ids=None,\n                attention_mask=None,\n                labels=None,\n                output_attentions=False, \n        output_hidden_states=False, \n        **kwargs):  # Catch any additional arguments):\n        \n        if input_ids is not None:\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        elif inputs_embeds is not None:\n            token_embeddings = inputs_embeds\n        else:\n            raise ValueError(\"You must provide either input_ids or inputs_embeds.\")\n\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings).to(device = token_embeddings.device)\n            \n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [torch.ones((batch_size,image_embedding_length), device=attention_mask.device),attention_mask ], dim=1\n            )\n            \n            # Combine image and token embeddings\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            \n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), -100, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([label_padding,labels], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels, output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            **kwargs)\n\n        return outputs\n    \n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        # Generate inputs with projections where necessary\n        if image_embeddings is not None:\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n            projected_image_embeddings = projected_image_embeddings.unsqueeze(0)\n            \n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n           \n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [torch.ones((batch_size,image_embedding_length), device=attention_mask.device),attention_mask ], dim=1\n            )\n            \n           \n        else:\n            combined_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            new_attention_mask = attention_mask\n\n        return {\n            \"inputs_embeds\": combined_embeddings,\n            \"attention_mask\": new_attention_mask,\n            \n            **kwargs\n        }\n    \n    def generate(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        self.eval()  # Set to evaluation mode\n        # Prepare inputs for generation\n        inputs = self.prepare_inputs_for_generation(input_ids, attention_mask, image_embeddings, **kwargs)\n        # Use the model's built-in generate method\n        return self.phi2_model.generate(**inputs)\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2).to(device)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n\n\nfrom peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\n\nphi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:37:50.832216Z","iopub.execute_input":"2024-10-29T17:37:50.832633Z","iopub.status.idle":"2024-10-29T17:37:51.135290Z","shell.execute_reply.started":"2024-10-29T17:37:50.832600Z","shell.execute_reply":"2024-10-29T17:37:51.134619Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### add mlp_layer params for training along with lora params","metadata":{}},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True\n\nphi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:37:51.626544Z","iopub.execute_input":"2024-10-29T17:37:51.626978Z","iopub.status.idle":"2024-10-29T17:37:51.639414Z","shell.execute_reply.started":"2024-10-29T17:37:51.626941Z","shell.execute_reply":"2024-10-29T17:37:51.638491Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"trainable params: 4,587,520 || all params: 1,240,401,920 || trainable%: 0.3698\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### check inference on custom model","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Create a new GenerationConfig with desired settings\ngeneration_config = GenerationConfig(max_new_tokens=128, temperature=0.01, top_p=1)\nphi_lora_model.generation_config = generation_config\n\noutputs = phi_lora_model.generate(**inputs,image_embeddings = None, max_new_tokens=128, temperature=0.01, top_p=1)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:37:52.624801Z","iopub.execute_input":"2024-10-29T17:37:52.625421Z","iopub.status.idle":"2024-10-29T17:37:59.880328Z","shell.execute_reply.started":"2024-10-29T17:37:52.625377Z","shell.execute_reply":"2024-10-29T17:37:59.879305Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response:  \nUnfortunately, I am unable to see the image you are referring to.  However, I can tell you that the image is likely a photograph of a restaurant or a food establishment.  Based on the fact that it is a restaurant, I would guess that the types of food that can be seen in the image are likely to be a variety of dishes, such as appetizers, entrees, and desserts.  The image may also show a variety of beverages, such as drinks and cocktails.  It is also possible that the image shows a variety of food presentation, such as a buffet or a food display.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### load processed data from llava_150k (subset)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sub-capstone/turns_60k_sample.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:41:23.062752Z","iopub.execute_input":"2024-10-29T17:41:23.063221Z","iopub.status.idle":"2024-10-29T17:41:24.664643Z","shell.execute_reply.started":"2024-10-29T17:41:23.063182Z","shell.execute_reply":"2024-10-29T17:41:24.663601Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### define dataloader and collator","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n# clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n# clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 256 - 49\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image_embedding = embeddings[self.image_paths[idx]]\n        \n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx].replace('<image>','')+self.text_labels[idx],\n            return_tensors='pt',\n            truncation = True,\n            padding='max_length',  # Ensures padding to a consistent length\n            max_length=self.max_length\n        )\n        \n        \n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = input_ids.clone()\n        label_ids[label_ids == tokenizer.pad_token_id] = -100\n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:41:26.036880Z","iopub.execute_input":"2024-10-29T17:41:26.037289Z","iopub.status.idle":"2024-10-29T17:41:29.616771Z","shell.execute_reply.started":"2024-10-29T17:41:26.037252Z","shell.execute_reply":"2024-10-29T17:41:29.615675Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:41:29.618997Z","iopub.execute_input":"2024-10-29T17:41:29.620125Z","iopub.status.idle":"2024-10-29T17:41:31.225531Z","shell.execute_reply.started":"2024-10-29T17:41:29.620074Z","shell.execute_reply":"2024-10-29T17:41:31.224676Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78c7377127d0>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.default_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, features):\n        # Extract input features (image embeddings, text inputs, etc.)\n\n        input_ids = [f['input_ids'] for f in features]\n        attention_mask = [f['attention_mask'] for f in features]\n        image_embeddings = [f['image_embeddings'] for f in features if 'image_embeddings' in f]\n        labels = [f['labels'] for f in features if 'labels' in f]\n\n        # Collate the text inputs using the default collator\n        batch = self.default_collator(features)\n\n        # Add image embeddings if they exist\n        if image_embeddings:\n            batch['image_embeddings'] = torch.stack(image_embeddings)\n\n        # Add labels to the batch\n        if labels:\n            batch['labels'] = torch.stack(labels)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:41:31.226688Z","iopub.execute_input":"2024-10-29T17:41:31.227339Z","iopub.status.idle":"2024-10-29T17:41:31.235940Z","shell.execute_reply.started":"2024-10-29T17:41:31.227300Z","shell.execute_reply":"2024-10-29T17:41:31.235058Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### training 10000 steps","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\noutput_dir = \"./results_coupled\"\nper_device_train_batch_size = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 1000\nlogging_steps = 100\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nwarmup_ratio = 0.03\nlr_scheduler_type = \"cosine\"\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results1\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=1,\n    max_steps = 10000,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    lr_scheduler_type=lr_scheduler_type,\n    fp16=False,\n    weight_decay=0.01,\n    remove_unused_columns=False\n)\n\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,\n    # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T17:41:50.268587Z","iopub.execute_input":"2024-10-29T17:41:50.268994Z","iopub.status.idle":"2024-10-30T03:33:40.791016Z","shell.execute_reply.started":"2024-10-29T17:41:50.268959Z","shell.execute_reply":"2024-10-30T03:33:40.789957Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10000/10000 9:51:45, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.904600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.710400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.669900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.653100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.619700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.615500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.610500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.584300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.597800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.569200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.582700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.579300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.562800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.540400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.570700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.536100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.533400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.552500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.559500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.532500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.539600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.516800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.518600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.529400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.511700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.523700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.522200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.503300</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>1.488900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.522900</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>1.504600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.491500</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.488700</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.486500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.514700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.503600</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>1.509900</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>1.490000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>1.510300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.494600</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>1.498300</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.478700</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>1.492100</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>1.491600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.464100</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>1.506200</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>1.480900</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>1.472500</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>1.495000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.468600</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>1.483500</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>1.477600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>1.460500</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>1.481000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.483900</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>1.483900</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>1.465300</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>1.467500</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>1.463900</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.455300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>1.436100</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>1.462600</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>1.453200</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>1.450100</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.451100</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>1.465600</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>1.474300</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>1.458800</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>1.457100</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.449200</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>1.440300</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>1.457500</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>1.418300</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>1.462900</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.456700</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>1.458700</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>1.463400</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>1.450300</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>1.430400</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.440300</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>1.453900</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>1.463400</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>1.460700</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>1.448600</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.426900</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>1.448000</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>1.459000</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>1.457500</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>1.448500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.451600</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>1.470700</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>1.451300</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>1.466000</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>1.452200</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>1.468300</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>1.449200</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>1.460500</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>1.478200</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>1.447200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.453700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10000, training_loss=1.4997929794311524, metrics={'train_runtime': 35509.5138, 'train_samples_per_second': 2.253, 'train_steps_per_second': 0.282, 'total_flos': 0.0, 'train_loss': 1.4997929794311524, 'epoch': 0.5799454851243983})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:33:40.792798Z","iopub.execute_input":"2024-10-30T03:33:40.793203Z","iopub.status.idle":"2024-10-30T03:33:40.863834Z","shell.execute_reply.started":"2024-10-30T03:33:40.793104Z","shell.execute_reply":"2024-10-30T03:33:40.862866Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"hf_adapter_repo=\"Kartheekb7/peft_llava_llama_2\"","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:33:40.865396Z","iopub.execute_input":"2024-10-30T03:33:40.865698Z","iopub.status.idle":"2024-10-30T03:33:40.869678Z","shell.execute_reply.started":"2024-10-30T03:33:40.865668Z","shell.execute_reply":"2024-10-30T03:33:40.868718Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.push_to_hub(hf_adapter_repo)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:33:40.872507Z","iopub.execute_input":"2024-10-30T03:33:40.873290Z","iopub.status.idle":"2024-10-30T03:33:42.512606Z","shell.execute_reply.started":"2024-10-30T03:33:40.873243Z","shell.execute_reply":"2024-10-30T03:33:42.511624Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f00cd8f30d941d8a049a0ba95106f67"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Kartheekb7/peft_llava_llama_2/commit/a65f68ce2809da647eff38791ac465e8fa014772', commit_message='Upload model', commit_description='', oid='a65f68ce2809da647eff38791ac465e8fa014772', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Kartheekb7/peft_llava_llama_2', endpoint='https://huggingface.co', repo_type='model', repo_id='Kartheekb7/peft_llava_llama_2'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(hf_adapter_repo)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:33:42.515019Z","iopub.execute_input":"2024-10-30T03:33:42.515464Z","iopub.status.idle":"2024-10-30T03:33:44.185373Z","shell.execute_reply.started":"2024-10-30T03:33:42.515420Z","shell.execute_reply":"2024-10-30T03:33:44.184410Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ecb4083863a4c77b196e455d3dc02c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d59ac9659d4950bdfd499ecf2e8fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1730223711.4be4fe3b6de8.115.0:   0%|          | 0.00/27.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dc40e89df641b6a492c3d8f1089914"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1543cd2f9c745a19af295b409970642"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Kartheekb7/results1/commit/8bfeb9e37ebb9b595ba4574b794e973094c06c5a', commit_message='Kartheekb7/peft_llava_llama_2', commit_description='', oid='8bfeb9e37ebb9b595ba4574b794e973094c06c5a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Kartheekb7/results1', endpoint='https://huggingface.co', repo_type='model', repo_id='Kartheekb7/results1'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Specify the directory where you want to save the model\nmodel_save_directory = \"my_phi_lora_model\"\n\n# Save the combined model with LoRA\nphi_lora_model.save_pretrained(model_save_directory)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:33:44.186582Z","iopub.execute_input":"2024-10-30T03:33:44.186902Z","iopub.status.idle":"2024-10-30T03:33:44.249229Z","shell.execute_reply.started":"2024-10-30T03:33:44.186869Z","shell.execute_reply":"2024-10-30T03:33:44.248460Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"base_config = phi_lora_model.base_model.config","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:24:39.474371Z","iopub.execute_input":"2024-10-30T04:24:39.475118Z","iopub.status.idle":"2024-10-30T04:24:39.479620Z","shell.execute_reply.started":"2024-10-30T04:24:39.475075Z","shell.execute_reply":"2024-10-30T04:24:39.478586Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.config","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:18:05.204217Z","iopub.execute_input":"2024-10-30T04:18:05.204902Z","iopub.status.idle":"2024-10-30T04:18:05.212689Z","shell.execute_reply.started":"2024-10-30T04:18:05.204860Z","shell.execute_reply":"2024-10-30T04:18:05.211717Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"float32\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}"},"metadata":{}}]},{"cell_type":"code","source":"base_config","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:17:01.261105Z","iopub.execute_input":"2024-10-30T04:17:01.262027Z","iopub.status.idle":"2024-10-30T04:17:01.270251Z","shell.execute_reply.started":"2024-10-30T04:17:01.261981Z","shell.execute_reply":"2024-10-30T04:17:01.269187Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"float32\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}"},"metadata":{}}]},{"cell_type":"code","source":"image_paths = df['image'].tolist()\ntext_inputs = df['input'].tolist()\ntext_labels = df['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:53:49.554766Z","iopub.execute_input":"2024-10-30T03:53:49.555187Z","iopub.status.idle":"2024-10-30T03:53:49.569554Z","shell.execute_reply.started":"2024-10-30T03:53:49.555148Z","shell.execute_reply":"2024-10-30T03:53:49.568633Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"text_labels[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:54:37.444303Z","iopub.execute_input":"2024-10-30T03:54:37.445319Z","iopub.status.idle":"2024-10-30T03:54:37.452412Z","shell.execute_reply.started":"2024-10-30T03:54:37.445265Z","shell.execute_reply":"2024-10-30T03:54:37.451404Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'In the image, you can see various types of food, including broccoli, bread, meat, vegetables, and fruit. These foods are presented in colorful dishes or containers.'"},"metadata":{}}]},{"cell_type":"code","source":"image_path,text_input,text_label # Example text label (if needed for comparison)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:56:32.111564Z","iopub.execute_input":"2024-10-30T03:56:32.112375Z","iopub.status.idle":"2024-10-30T03:56:32.119065Z","shell.execute_reply.started":"2024-10-30T03:56:32.112331Z","shell.execute_reply":"2024-10-30T03:56:32.118042Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"('000000000009.jpg',\n 'What types of food can be seen in the image?\\n',\n 'In the image, you can see various types of food, including broccoli, bread, meat, vegetables, and fruit. These foods are presented in colorful dishes or containers.')"},"metadata":{}}]},{"cell_type":"markdown","source":"### inference on trained model","metadata":{}},{"cell_type":"code","source":"import torch\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_path = image_paths[0]\ntext_input = text_inputs[0]  # Example text input\ntext_label = text_labels[0]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimage_embedding = embeddings[image_path]\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256-49  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:50:40.846626Z","iopub.execute_input":"2024-10-30T03:50:40.847036Z","iopub.status.idle":"2024-10-30T03:50:40.855756Z","shell.execute_reply.started":"2024-10-30T03:50:40.846999Z","shell.execute_reply":"2024-10-30T03:50:40.854910Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"image_path = image_paths[0]\nimage_embedding = embeddings[image_path]\nimage_embedding = image_embedding.squeeze(0).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:50:49.383753Z","iopub.execute_input":"2024-10-30T03:50:49.384686Z","iopub.status.idle":"2024-10-30T03:50:49.389662Z","shell.execute_reply.started":"2024-10-30T03:50:49.384641Z","shell.execute_reply":"2024-10-30T03:50:49.388597Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# # Create a new GenerationConfig with desired settings\n# generation_config = GenerationConfig(max_new_tokens=128, temperature=0.01, top_p=1)\n# phi_lora_model.generation_config = generation_config\n\noutputs = phi_lora_model.generate(**input_encoding,image_embeddings = image_embedding, max_new_tokens=128, temperature=0.01, top_p=1)\n# Decode output to text\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:51:01.413960Z","iopub.execute_input":"2024-10-30T03:51:01.414922Z","iopub.status.idle":"2024-10-30T03:51:09.222031Z","shell.execute_reply.started":"2024-10-30T03:51:01.414875Z","shell.execute_reply":"2024-10-30T03:51:09.221061Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response: \nThe image shows a variety of food items, including meat, vegetables, and fruits. The food is contained in a plastic container or a plastic bag, which is open and partially filled. The contents include meat, broccoli, carrots, and other vegetables. There are also fruits, such as apples and oranges, visible in the image. The food is arranged in a colorful and visually appealing way, making it an attractive and appetizing meal. The food is likely to be served together in a meal or a snack. The image also includes a bowl, which could be used to serve the food. The overall presentation suggests a well-organized and\n","output_type":"stream"}]},{"cell_type":"code","source":"text_label","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:55:19.999572Z","iopub.execute_input":"2024-10-30T03:55:20.000487Z","iopub.status.idle":"2024-10-30T03:55:20.006005Z","shell.execute_reply.started":"2024-10-30T03:55:20.000444Z","shell.execute_reply":"2024-10-30T03:55:20.005079Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'In the image, you can see various types of food, including broccoli, bread, meat, vegetables, and fruit. These foods are presented in colorful dishes or containers.'"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.base_model.mlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:58:12.978475Z","iopub.execute_input":"2024-10-30T03:58:12.978899Z","iopub.status.idle":"2024-10-30T03:58:12.985880Z","shell.execute_reply.started":"2024-10-30T03:58:12.978857Z","shell.execute_reply":"2024-10-30T03:58:12.984851Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.base_model.model.mlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:02:40.457451Z","iopub.execute_input":"2024-10-30T04:02:40.457860Z","iopub.status.idle":"2024-10-30T04:02:40.465083Z","shell.execute_reply.started":"2024-10-30T04:02:40.457822Z","shell.execute_reply":"2024-10-30T04:02:40.463898Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Access the MLP layer\nmlp_layer = phi_lora_model.base_model.mlp_projection\ntorch.save(mlp_layer.state_dict(), \"mlp_projection_weights.pth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:03:21.313777Z","iopub.execute_input":"2024-10-30T04:03:21.314737Z","iopub.status.idle":"2024-10-30T04:03:21.337614Z","shell.execute_reply.started":"2024-10-30T04:03:21.314694Z","shell.execute_reply":"2024-10-30T04:03:21.336763Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Reload the saved weights\nloaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n\n# Check if reloaded weights match the original weights\nis_same = True\nfor key in loaded_mlp_weights:\n    if not torch.equal(loaded_mlp_weights[key], mlp_layer.state_dict()[key]):\n        is_same = False\n        break\n\nif is_same:\n    print(\"The loaded MLP layer is identical to the original model's MLP layer.\")\nelse:\n    print(\"The loaded MLP layer differs from the original model's MLP layer.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:03:40.321572Z","iopub.execute_input":"2024-10-30T04:03:40.321983Z","iopub.status.idle":"2024-10-30T04:03:40.341555Z","shell.execute_reply.started":"2024-10-30T04:03:40.321943Z","shell.execute_reply":"2024-10-30T04:03:40.340252Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"The loaded MLP layer is identical to the original model's MLP layer.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/2565097332.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n","output_type":"stream"}]},{"cell_type":"code","source":"phi_lora_model.base_model","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:35:45.959176Z","iopub.execute_input":"2024-10-30T04:35:45.960113Z","iopub.status.idle":"2024-10-30T04:35:45.973035Z","shell.execute_reply.started":"2024-10-30T04:35:45.960071Z","shell.execute_reply":"2024-10-30T04:35:45.972091Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"LoraModel(\n  (model): PHI2WithMLP(\n    (phi2_model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 2048)\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n    )\n    (mlp_projection): MLPProjection(\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1024, bias=False)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1024, out_features=2048, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig,AutoTokenizer\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n)\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\ninfer_model = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config=bnb_config,\n    torch_dtype = torch.float32,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:31:03.153391Z","iopub.execute_input":"2024-10-30T04:31:03.153765Z","iopub.status.idle":"2024-10-30T04:31:07.709956Z","shell.execute_reply.started":"2024-10-30T04:31:03.153734Z","shell.execute_reply":"2024-10-30T04:31:07.709004Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"}]},{"cell_type":"code","source":"# load saved model\nmlp_inf = MLPProjection(input_dim, output_dim, hidden_dim, depth=2).to(device)  # Customize MLP\ninference_base = create_phi2_model_with_lora(mlp_inf, infer_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:32:35.219720Z","iopub.execute_input":"2024-10-30T04:32:35.220635Z","iopub.status.idle":"2024-10-30T04:32:35.249764Z","shell.execute_reply.started":"2024-10-30T04:32:35.220565Z","shell.execute_reply":"2024-10-30T04:32:35.248964Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Reload the saved weights\nloaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n\n# Check if reloaded weights match the original weights\nis_same = True\nfor key in loaded_mlp_weights:\n    if not torch.equal(loaded_mlp_weights[key], mlp_inf.state_dict()[key]):\n        is_same = False\n        break\n\nif is_same:\n    print(\"The loaded MLP layer is identical to the original model's MLP layer.\")\nelse:\n    print(\"The loaded MLP layer differs from the original model's MLP layer.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:32:43.549984Z","iopub.execute_input":"2024-10-30T04:32:43.550878Z","iopub.status.idle":"2024-10-30T04:32:43.567100Z","shell.execute_reply.started":"2024-10-30T04:32:43.550832Z","shell.execute_reply":"2024-10-30T04:32:43.566121Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"The loaded MLP layer differs from the original model's MLP layer.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/798896664.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:37:32.682564Z","iopub.execute_input":"2024-10-30T04:37:32.683334Z","iopub.status.idle":"2024-10-30T04:37:32.687310Z","shell.execute_reply.started":"2024-10-30T04:37:32.683284Z","shell.execute_reply":"2024-10-30T04:37:32.686370Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"peft_model_id = \"Kartheekb7/results1\"\nloaded_model = PeftModel.from_pretrained(inference_base, peft_model_id,is_trainable=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:39:11.876849Z","iopub.execute_input":"2024-10-30T04:39:11.877669Z","iopub.status.idle":"2024-10-30T04:39:12.177147Z","shell.execute_reply.started":"2024-10-30T04:39:11.877628Z","shell.execute_reply":"2024-10-30T04:39:12.176350Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"loaded_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:39:18.551126Z","iopub.execute_input":"2024-10-30T04:39:18.551670Z","iopub.status.idle":"2024-10-30T04:39:18.564477Z","shell.execute_reply.started":"2024-10-30T04:39:18.551621Z","shell.execute_reply":"2024-10-30T04:39:18.563014Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 1,240,401,920 || trainable%: 0.1374\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in loaded_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True\n\nloaded_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:40:03.328785Z","iopub.execute_input":"2024-10-30T04:40:03.329781Z","iopub.status.idle":"2024-10-30T04:40:03.343559Z","shell.execute_reply.started":"2024-10-30T04:40:03.329735Z","shell.execute_reply":"2024-10-30T04:40:03.342492Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"trainable params: 4,587,520 || all params: 1,240,401,920 || trainable%: 0.3698\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=128, temperature=0.01, top_p=1)\nloaded_model.generation_config = generation_config\n\noutputs = loaded_model.generate(**input_encoding,image_embeddings = image_embedding, max_new_tokens=128, temperature=0.01, top_p=1)\n# Decode output to text\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:41:52.711849Z","iopub.execute_input":"2024-10-30T04:41:52.712894Z","iopub.status.idle":"2024-10-30T04:42:00.502041Z","shell.execute_reply.started":"2024-10-30T04:41:52.712847Z","shell.execute_reply":"2024-10-30T04:42:00.500946Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response: \nThe image shows a close-up view of the person eating the food. The image is taken from a close-up angle, so we can see the person eating the food. The image is taken from a close-up angle, which gives us a better idea of the person eating the food. The image is taken from a close-up angle, which allows us to see the person eating the food. The image is taken from a close-up angle, which gives us a better idea of the person eating the food. The image is taken from a close-up angle, which allows us to see the person eating the food. The image is taken from\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Access the MLP layer\nmlp_layer = loaded_model.base_model.mlp_projection\n# Reload the saved weights\nloaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n\n# Check if reloaded weights match the original weights\nis_same = True\nfor key in loaded_mlp_weights:\n    if not torch.equal(loaded_mlp_weights[key], mlp_inf.state_dict()[key]):\n        is_same = False\n        break\n\nif is_same:\n    print(\"The loaded MLP layer is identical to the original model's MLP layer.\")\nelse:\n    print(\"The loaded MLP layer differs from the original model's MLP layer.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:44:45.673879Z","iopub.execute_input":"2024-10-30T04:44:45.674860Z","iopub.status.idle":"2024-10-30T04:44:45.691168Z","shell.execute_reply.started":"2024-10-30T04:44:45.674814Z","shell.execute_reply":"2024-10-30T04:44:45.690212Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"The loaded MLP layer differs from the original model's MLP layer.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/2718849335.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n","output_type":"stream"}]},{"cell_type":"code","source":"loaded_model.base_model.model.mlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:46:57.744341Z","iopub.execute_input":"2024-10-30T04:46:57.745193Z","iopub.status.idle":"2024-10-30T04:46:57.751800Z","shell.execute_reply.started":"2024-10-30T04:46:57.745145Z","shell.execute_reply":"2024-10-30T04:46:57.750887Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=False)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=2048, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"loaded_model.base_model.model.mlp_projection.load_state_dict(loaded_mlp_weights)\nprint(\"Projection layer weights loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:48:14.269563Z","iopub.execute_input":"2024-10-30T04:48:14.270016Z","iopub.status.idle":"2024-10-30T04:48:14.276148Z","shell.execute_reply.started":"2024-10-30T04:48:14.269974Z","shell.execute_reply":"2024-10-30T04:48:14.275212Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Projection layer weights loaded successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Access the MLP layer\nmlp_layer = loaded_model.base_model.mlp_projection\n# Reload the saved weights\nloaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n\n# Check if reloaded weights match the original weights\nis_same = True\nfor key in loaded_mlp_weights:\n    if not torch.equal(loaded_mlp_weights[key], mlp_inf.state_dict()[key]):\n        is_same = False\n        break\n\nif is_same:\n    print(\"The loaded MLP layer is identical to the original model's MLP layer.\")\nelse:\n    print(\"The loaded MLP layer differs from the original model's MLP layer.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:48:16.923461Z","iopub.execute_input":"2024-10-30T04:48:16.923890Z","iopub.status.idle":"2024-10-30T04:48:16.941306Z","shell.execute_reply.started":"2024-10-30T04:48:16.923850Z","shell.execute_reply":"2024-10-30T04:48:16.940167Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"The loaded MLP layer is identical to the original model's MLP layer.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/2718849335.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_mlp_weights = torch.load(\"mlp_projection_weights.pth\")\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=128, temperature=0.01, top_p=1)\nloaded_model.generation_config = generation_config\n\noutputs = loaded_model.generate(**input_encoding,image_embeddings = image_embedding, max_new_tokens=128, temperature=0.01, top_p=1)\n# Decode output to text\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:48:31.188649Z","iopub.execute_input":"2024-10-30T04:48:31.189041Z","iopub.status.idle":"2024-10-30T04:48:38.949964Z","shell.execute_reply.started":"2024-10-30T04:48:31.189005Z","shell.execute_reply":"2024-10-30T04:48:38.948867Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response: \nThe image shows a variety of food items, including meat, vegetables, and fruits. The food is contained in a plastic container or a plastic bag, which is open and partially filled. The contents include meat, broccoli, carrots, and other vegetables. There are also fruits, such as apples and oranges, visible in the image. The food is arranged in a colorful and visually appealing way, making it an attractive and appetizing meal. The food is likely to be served together in a meal or a snack. The image also includes a bowl, which could be used to serve the food. The overall presentation suggests a well-organized and\n","output_type":"stream"}]},{"cell_type":"code","source":"loaded_projection_w","metadata":{},"execution_count":null,"outputs":[]}]}