{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9760205,"sourceType":"datasetVersion","datasetId":5976696},{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-30T05:44:12.443953Z","iopub.execute_input":"2024-10-30T05:44:12.444379Z","iopub.status.idle":"2024-10-30T05:44:13.965001Z","shell.execute_reply.started":"2024-10-30T05:44:12.444335Z","shell.execute_reply":"2024-10-30T05:44:13.963829Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip -q install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:47:11.353907Z","iopub.execute_input":"2024-10-30T05:47:11.354390Z","iopub.status.idle":"2024-10-30T05:47:34.156843Z","shell.execute_reply.started":"2024-10-30T05:47:11.354348Z","shell.execute_reply":"2024-10-30T05:47:34.155027Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip -q install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:47:34.159892Z","iopub.execute_input":"2024-10-30T05:47:34.160331Z","iopub.status.idle":"2024-10-30T05:47:50.399597Z","shell.execute_reply.started":"2024-10-30T05:47:34.160286Z","shell.execute_reply":"2024-10-30T05:47:50.397994Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:49:13.655360Z","iopub.execute_input":"2024-10-30T05:49:13.655858Z","iopub.status.idle":"2024-10-30T05:49:14.025959Z","shell.execute_reply.started":"2024-10-30T05:49:13.655798Z","shell.execute_reply":"2024-10-30T05:49:14.024542Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7364ebd53ec40f8a280054dc7d4d2fd"}},"metadata":{}}]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig,AutoTokenizer\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n# #     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    torch_dtype = torch.float32,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:50:21.512862Z","iopub.execute_input":"2024-10-30T05:50:21.513371Z","iopub.status.idle":"2024-10-30T05:51:30.957829Z","shell.execute_reply.started":"2024-10-30T05:50:21.513325Z","shell.execute_reply":"2024-10-30T05:51:30.956537Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53bb5c47f85541ccaf2035c0b4761c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f752252b56ef4754bf5daf8ebd243979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d00a3c81364f39ba5feced0f4b3ef4"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:51:30.970509Z","iopub.execute_input":"2024-10-30T05:51:30.971007Z","iopub.status.idle":"2024-10-30T05:51:32.474664Z","shell.execute_reply.started":"2024-10-30T05:51:30.970961Z","shell.execute_reply":"2024-10-30T05:51:32.473285Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2ac8daa26144668e7d027d63123c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43c03239f65494db2676a6ac0e15c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b03dcb589243a1b47f85dfe35e63ea"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:53:12.208143Z","iopub.execute_input":"2024-10-30T05:53:12.208650Z","iopub.status.idle":"2024-10-30T05:53:12.219190Z","shell.execute_reply.started":"2024-10-30T05:53:12.208603Z","shell.execute_reply":"2024-10-30T05:53:12.217636Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"select_feature = 'patch'\ndef feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:51:30.960454Z","iopub.execute_input":"2024-10-30T05:51:30.961128Z","iopub.status.idle":"2024-10-30T05:51:30.969166Z","shell.execute_reply.started":"2024-10-30T05:51:30.961080Z","shell.execute_reply":"2024-10-30T05:51:30.967419Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=768, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim,bias = False))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim,bias=False))\n        \n        self.mlp = nn.Sequential(*modules)\n        \n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n        self.config = phi2_model.config\n        \n    def forward(self, image_embeddings=None,\n                inputs_embeds=None,\n                input_ids=None,\n                attention_mask=None,\n                labels=None,\n                output_attentions=False, \n        output_hidden_states=False, \n        **kwargs):  # Catch any additional arguments):\n        \n        if input_ids is not None:\n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        elif inputs_embeds is not None:\n            token_embeddings = inputs_embeds\n        else:\n            raise ValueError(\"You must provide either input_ids or inputs_embeds.\")\n\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings).to(device = token_embeddings.device)\n            \n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [torch.ones((batch_size,image_embedding_length), device=attention_mask.device),attention_mask ], dim=1\n            )\n            \n            # Combine image and token embeddings\n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            \n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n        if labels is not None:\n            # Labels should match the sequence length of combined embeddings\n            # If labels correspond only to text tokens, pad them to match the new sequence length\n            if image_embeddings is not None:\n                label_padding = torch.full(\n                    (batch_size, image_embedding_length), -100, device=labels.device  # Use -100 for ignore index\n                )\n                new_labels = torch.cat([label_padding,labels], dim=1)\n            else:\n                new_labels = labels\n        else:\n            new_labels = labels\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask,labels = new_labels, output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            **kwargs)\n\n        return outputs\n    \n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        # Generate inputs with projections where necessary\n        if image_embeddings is not None:\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n            projected_image_embeddings = projected_image_embeddings.unsqueeze(0)\n            \n            token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n           \n            combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            \n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [torch.ones((batch_size,image_embedding_length), device=attention_mask.device),attention_mask ], dim=1\n            )\n            \n           \n        else:\n            combined_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n            new_attention_mask = attention_mask\n\n        return {\n            \"inputs_embeds\": combined_embeddings,\n            \"attention_mask\": new_attention_mask,\n            \n            **kwargs\n        }\n    \n    def generate(self, input_ids, attention_mask=None, image_embeddings=None, **kwargs):\n        self.eval()  # Set to evaluation mode\n        # Prepare inputs for generation\n        inputs = self.prepare_inputs_for_generation(input_ids, attention_mask, image_embeddings, **kwargs)\n        # Use the model's built-in generate method\n        return self.phi2_model.generate(**inputs)\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2).to(device)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:53:40.233972Z","iopub.execute_input":"2024-10-30T05:53:40.234454Z","iopub.status.idle":"2024-10-30T05:53:40.308746Z","shell.execute_reply.started":"2024-10-30T05:53:40.234409Z","shell.execute_reply":"2024-10-30T05:53:40.307223Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\npeft_model_id = \"Kartheekb7/results1\"\nloaded_model = PeftModel.from_pretrained(combined_model, peft_model_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T05:58:37.509419Z","iopub.execute_input":"2024-10-30T05:58:37.512630Z","iopub.status.idle":"2024-10-30T05:58:39.783104Z","shell.execute_reply.started":"2024-10-30T05:58:37.512526Z","shell.execute_reply":"2024-10-30T05:58:39.781478Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87165f52f3e145f5a2e68b6140923074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264630fb365e41b2acf45020b6883706"}},"metadata":{}}]},{"cell_type":"code","source":"loaded_mlp_weights = torch.load(\"/kaggle/input/projection-weights/mlp_projection_weights.pth\",map_location=torch.device('cpu'))","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:02:51.595334Z","iopub.execute_input":"2024-10-30T06:02:51.596568Z","iopub.status.idle":"2024-10-30T06:02:51.614347Z","shell.execute_reply.started":"2024-10-30T06:02:51.596513Z","shell.execute_reply":"2024-10-30T06:02:51.612624Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2731402148.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_mlp_weights = torch.load(\"/kaggle/input/projection-weights/mlp_projection_weights.pth\",map_location=torch.device('cpu'))\n","output_type":"stream"}]},{"cell_type":"code","source":"loaded_model.base_model.model.mlp_projection.load_state_dict(loaded_mlp_weights)\nprint(\"Projection layer weights loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:02:59.707678Z","iopub.execute_input":"2024-10-30T06:02:59.708259Z","iopub.status.idle":"2024-10-30T06:02:59.719499Z","shell.execute_reply.started":"2024-10-30T06:02:59.708210Z","shell.execute_reply":"2024-10-30T06:02:59.717860Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Projection layer weights loaded successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Create a new GenerationConfig with desired settings\ngeneration_config = GenerationConfig(max_new_tokens=128, temperature=0.01, top_p=1)\nloaded_model.generation_config = generation_config","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:03:16.478859Z","iopub.execute_input":"2024-10-30T06:03:16.479985Z","iopub.status.idle":"2024-10-30T06:03:16.489048Z","shell.execute_reply.started":"2024-10-30T06:03:16.479922Z","shell.execute_reply":"2024-10-30T06:03:16.487649Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n# Load CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:09:48.816032Z","iopub.execute_input":"2024-10-30T06:09:48.816552Z","iopub.status.idle":"2024-10-30T06:10:18.212042Z","shell.execute_reply.started":"2024-10-30T06:09:48.816504Z","shell.execute_reply":"2024-10-30T06:10:18.210629Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47fda7137f340868fb51dc1187c2920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62703349a0824220b15ec1f63d54a854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22adaf918acd45d28183373b80f39f80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57526abc57084340a332dcbde04361d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48134111ed9f45848d42c4fe43625658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9ee894ae234f24a610256fffd68ea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c56ba464a0a4dc080535c3b12103ff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab8076dbc4b4e0a88c6a9b814e34795"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_clip_embedding(image_path):\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        image_features = clip_model.get_image_features(**inputs)\n        \n        image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n        image_features = feature_select(image_forward_outs)\n        image_embedding = image_features.squeeze(0)\n    return image_embedding","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:10:18.214496Z","iopub.execute_input":"2024-10-30T06:10:18.215400Z","iopub.status.idle":"2024-10-30T06:10:18.224199Z","shell.execute_reply.started":"2024-10-30T06:10:18.215351Z","shell.execute_reply":"2024-10-30T06:10:18.222420Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\n\n\n# Example input data\nimage_path = \"/kaggle/input/sample/000000000009.jpg\"\ntext_input = \"What types of food can be seen in the image?\"  # Example text input\n\n# Prepare inputs\nimage_embedding = get_clip_embedding(image_path)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_input,\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=256-49  # Set this to match your model's input size\n)\n\nimage_embedding = image_embedding.squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:41:21.827449Z","iopub.execute_input":"2024-10-30T06:41:21.829157Z","iopub.status.idle":"2024-10-30T06:41:22.188407Z","shell.execute_reply.started":"2024-10-30T06:41:21.829088Z","shell.execute_reply":"2024-10-30T06:41:22.187104Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\noutputs = loaded_model.generate(**input_encoding,image_embeddings = image_embedding, max_new_tokens=128, temperature=0.01, top_p=1)\n# Decode output to text\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Model response:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T06:41:31.410357Z","iopub.execute_input":"2024-10-30T06:41:31.410920Z","iopub.status.idle":"2024-10-30T06:42:23.843257Z","shell.execute_reply.started":"2024-10-30T06:41:31.410862Z","shell.execute_reply":"2024-10-30T06:42:23.841901Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response: \nThe image shows a variety of food items, including a salad, a bowl of rice, a plate of meat, and a bowl of vegetables. There are also some fruits visible in the image. The food items are arranged in a colorful and visually appealing way, making it easy to identify and appreciate the different types of food. The salad and rice are likely to be a healthy and balanced meal option, while the meat and vegetables provide protein and essential nutrients. The fruits add natural sweetness and freshness to the dish. Overall, the image showcases a diverse and appetizing selection of food items.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchaudio\nimport torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nimport ipywidgets as widgets\nfrom IPython.display import display, Audio\n\n# Load the Whisper model and processor\naudio_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\naudio_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\naudio_model.config.forced_decoder_ids = None\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:04:04.274837Z","iopub.execute_input":"2024-10-30T07:04:04.275371Z","iopub.status.idle":"2024-10-30T07:04:05.672353Z","shell.execute_reply.started":"2024-10-30T07:04:04.275325Z","shell.execute_reply":"2024-10-30T07:04:05.670948Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\n  \"automatic-speech-recognition\",\n  model=\"openai/whisper-small\",\n  chunk_length_s=30,\n  device=device,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:04:07.419101Z","iopub.execute_input":"2024-10-30T07:04:07.419679Z","iopub.status.idle":"2024-10-30T07:04:08.091178Z","shell.execute_reply.started":"2024-10-30T07:04:07.419625Z","shell.execute_reply":"2024-10-30T07:04:08.089855Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"result = pipe(\"/kaggle/working/temp_audio.wav\",generate_kwargs={\"language\": \"english\"})\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:02:27.472345Z","iopub.execute_input":"2024-10-30T07:02:27.472816Z","iopub.status.idle":"2024-10-30T07:02:32.286224Z","shell.execute_reply.started":"2024-10-30T07:02:27.472757Z","shell.execute_reply":"2024-10-30T07:02:32.284867Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nYou have passed language=english, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=english.\n","output_type":"stream"}]},{"cell_type":"code","source":"result['text']","metadata":{"execution":{"iopub.status.busy":"2024-10-30T07:02:32.800368Z","iopub.execute_input":"2024-10-30T07:02:32.800947Z","iopub.status.idle":"2024-10-30T07:02:32.810454Z","shell.execute_reply.started":"2024-10-30T07:02:32.800890Z","shell.execute_reply":"2024-10-30T07:02:32.808978Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"\" Hi, I'm going to miscarry.\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}