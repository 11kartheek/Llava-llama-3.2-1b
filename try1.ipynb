{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9682357,"sourceType":"datasetVersion","datasetId":5918442}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T08:20:48.373718Z","iopub.execute_input":"2024-10-24T08:20:48.374181Z","iopub.status.idle":"2024-10-24T08:20:49.520699Z","shell.execute_reply.started":"2024-10-24T08:20:48.374124Z","shell.execute_reply":"2024-10-24T08:20:49.519547Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sample/000000000009.jpg\n/kaggle/input/sample/000000000025.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes peft","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:22:49.863032Z","iopub.execute_input":"2024-10-24T08:22:49.863488Z","iopub.status.idle":"2024-10-24T08:23:11.024532Z","shell.execute_reply.started":"2024-10-24T08:22:49.863446Z","shell.execute_reply":"2024-10-24T08:23:11.023032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n# notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:11.027036Z","iopub.execute_input":"2024-10-24T08:23:11.027480Z","iopub.status.idle":"2024-10-24T08:23:11.032678Z","shell.execute_reply.started":"2024-10-24T08:23:11.027435Z","shell.execute_reply":"2024-10-24T08:23:11.031540Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# meta-llama/Llama-3.2-1B-Instruct\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# Load PHI 2 model with 4-bit quantization for efficient fine-tuning\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.float16\n# )\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n#     quantization_config=bnb_config,\n    trust_remote_code=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:11.034031Z","iopub.execute_input":"2024-10-24T08:23:11.034402Z","iopub.status.idle":"2024-10-24T08:23:14.857049Z","shell.execute_reply.started":"2024-10-24T08:23:11.034365Z","shell.execute_reply":"2024-10-24T08:23:14.855888Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:21.248796Z","iopub.execute_input":"2024-10-24T08:23:21.249977Z","iopub.status.idle":"2024-10-24T08:23:21.261089Z","shell.execute_reply.started":"2024-10-24T08:23:21.249907Z","shell.execute_reply":"2024-10-24T08:23:21.259746Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport random\nclass MLPProjection(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=1024, depth=2):\n        super(MLPProjection, self).__init__()\n        modules = []\n        modules.append(nn.Linear(input_dim, hidden_dim))\n        \n        for _ in range(1, depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(hidden_dim, output_dim))\n        \n        self.mlp = nn.Sequential(*modules)\n    \n    def forward(self, x):\n        return self.mlp(x)\n\nclass PHI2WithMLP(nn.Module):\n    def __init__(self, phi2_model, mlp_projection):\n        super(PHI2WithMLP, self).__init__()\n        self.phi2_model = phi2_model\n        self.mlp_projection = mlp_projection\n\n    def forward(self, image_embeddings=None, input_ids=None, attention_mask=None, labels=None):\n       \n        # Get token embeddings from PHI2 model\n        token_embeddings = self.phi2_model.get_input_embeddings()(input_ids)\n        \n        if image_embeddings is not None:\n            # Apply MLP to image embeddings to map to text embedding space\n            projected_image_embeddings = self.mlp_projection(image_embeddings)\n\n            # Get the sequence length for the image embeddings\n            image_embedding_length = projected_image_embeddings.size(1)\n            \n            batch_size, text_sequence_length = attention_mask.shape\n            print(attention_mask.shape)\n            print(batch_size, text_sequence_length)\n            # Extend attention mask for image embeddings (ones for image embedding positions)\n            new_attention_mask = torch.cat(\n                [attention_mask, torch.ones((batch_size,image_embedding_length), device=attention_mask.device)], dim=1\n            )\n            print(new_attention_mask.shape)\n            print(projected_image_embeddings.shape,token_embeddings.shape)\n            # Combine image and token embeddings\n            if random.random() < 0.5:\n                combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=1)  # Concatenating along sequence length\n            else:\n                combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=1)\n        else:\n            # No image embeddings: Use only token embeddings and the original attention mask\n            combined_embeddings = token_embeddings\n            new_attention_mask = attention_mask\n\n        # Pass the combined embeddings through the PHI2 model with the (updated or original) attention mask\n        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=new_attention_mask)\n\n        return outputs\n\ndef create_phi2_model_with_lora(mlp_projection,lan_model):\n    \n    for param in mlp_projection.parameters():\n        param.requires_grad = True\n\n    # Return PHI2 model with MLP projection\n    return PHI2WithMLP(lan_model, mlp_projection)\n    \nmodel_embedding_dim = model.config.hidden_size  # This might change based on your model architecture\n\n# Example usage\ninput_dim = 768  # Input dimension of image embeddings\noutput_dim = model_embedding_dim  # Target dimension of text embeddings\nhidden_dim = 1024  # Hidden layer dimension of the MLP\n\nmlp_projection = MLPProjection(input_dim, output_dim, hidden_dim, depth=2)  # Customize MLP\ncombined_model = create_phi2_model_with_lora(mlp_projection, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:42:24.231973Z","iopub.execute_input":"2024-10-24T08:42:24.232904Z","iopub.status.idle":"2024-10-24T08:42:24.279175Z","shell.execute_reply.started":"2024-10-24T08:42:24.232851Z","shell.execute_reply":"2024-10-24T08:42:24.278026Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraModel, LoraConfig,get_peft_model\n\n# Set up the QLoRA configuration for attention layers in PHI 2\nlora_config = LoraConfig(\n    r=8,  # Low-rank dimension\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply QLoRA only to these layers\n    lora_dropout=0.05,\n    bias=\"none\"\n)\n\n# Wrap PHI 2 with QLoRA\n# phi_lora_model = LoraModel(model, lora_config,\"default\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:42:25.815769Z","iopub.execute_input":"2024-10-24T08:42:25.816270Z","iopub.status.idle":"2024-10-24T08:42:25.823229Z","shell.execute_reply.started":"2024-10-24T08:42:25.816226Z","shell.execute_reply":"2024-10-24T08:42:25.821894Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"phi_lora_model = get_peft_model(combined_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:42:26.336843Z","iopub.execute_input":"2024-10-24T08:42:26.337311Z","iopub.status.idle":"2024-10-24T08:42:26.480985Z","shell.execute_reply.started":"2024-10-24T08:42:26.337269Z","shell.execute_reply":"2024-10-24T08:42:26.479845Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"phi_lora_model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:42:26.723680Z","iopub.execute_input":"2024-10-24T08:42:26.724162Z","iopub.status.idle":"2024-10-24T08:42:26.745109Z","shell.execute_reply.started":"2024-10-24T08:42:26.724116Z","shell.execute_reply":"2024-10-24T08:42:26.743910Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): Qwen2ForCausalLM(\n        (model): Qwen2Model(\n          (embed_tokens): Embedding(151936, 1536)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen2DecoderLayer(\n              (self_attn): Qwen2SdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=256, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=256, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): Qwen2RotaryEmbedding()\n              )\n              (mlp): Qwen2MLP(\n                (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n                (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n                (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n              (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n            )\n          )\n          (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=1536, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:26.208295Z","iopub.execute_input":"2024-10-24T08:23:26.209014Z","iopub.status.idle":"2024-10-24T08:23:26.239067Z","shell.execute_reply.started":"2024-10-24T08:23:26.208963Z","shell.execute_reply":"2024-10-24T08:23:26.230217Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"trainable params: 2,179,072 || all params: 1,548,255,232 || trainable%: 0.1407\n","output_type":"stream"}]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:27.781147Z","iopub.execute_input":"2024-10-24T08:23:27.781639Z","iopub.status.idle":"2024-10-24T08:23:27.796166Z","shell.execute_reply.started":"2024-10-24T08:23:27.781597Z","shell.execute_reply":"2024-10-24T08:23:27.794729Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"trainable params: 2,179,072 || all params: 1,548,255,232 || trainable%: 0.1407\n","output_type":"stream"}]},{"cell_type":"markdown","source":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n\ntrainable params: 2,179,072 || all params: 1,547,205,888 || trainable%: 0.1408\n\n\r\n","metadata":{}},{"cell_type":"code","source":"for name, param in phi_lora_model.named_parameters():\n    if 'mlp_projection' in name :\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:29.439709Z","iopub.execute_input":"2024-10-24T08:23:29.440638Z","iopub.status.idle":"2024-10-24T08:23:29.452810Z","shell.execute_reply.started":"2024-10-24T08:23:29.440590Z","shell.execute_reply":"2024-10-24T08:23:29.451610Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"phi_lora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:30.241892Z","iopub.execute_input":"2024-10-24T08:23:30.243085Z","iopub.status.idle":"2024-10-24T08:23:30.256250Z","shell.execute_reply.started":"2024-10-24T08:23:30.243030Z","shell.execute_reply":"2024-10-24T08:23:30.254974Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"trainable params: 4,540,928 || all params: 1,548,255,232 || trainable%: 0.2933\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPModel\n\n# Load CLIP and PHI2\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:31.783404Z","iopub.execute_input":"2024-10-24T08:23:31.783859Z","iopub.status.idle":"2024-10-24T08:23:35.961952Z","shell.execute_reply.started":"2024-10-24T08:23:31.783816Z","shell.execute_reply":"2024-10-24T08:23:35.960795Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e2d877da9ac4b86bbaa90651a809b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e178b703e856448cb305812cdb8a050f"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Now the model can be trained, and the optimizer only updates LoRA and projection\noptimizer = torch.optim.AdamW(\n    [p for p in combined_model.parameters() if p.requires_grad], lr=1e-4\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:39.262020Z","iopub.execute_input":"2024-10-24T08:23:39.263055Z","iopub.status.idle":"2024-10-24T08:23:39.938091Z","shell.execute_reply.started":"2024-10-24T08:23:39.262996Z","shell.execute_reply":"2024-10-24T08:23:39.937095Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n# # Training loop\n# def train_model(combined_model, data_loader, optimizer, num_epochs=1, device=\"cuda\"):\n#     combined_model.train()\n#     combined_model = combined_model.to(device)\n    \n#     for epoch in range(num_epochs):\n#         total_loss = 0\n#         for batch in data_loader:\n#             image_embeddings = batch['image_embeddings'].to(device)\n#             input_ids = batch['input_ids'].to(device)\n#             labels = batch['labels'].to(device)\n            \n#             # Forward pass\n#             optimizer.zero_grad()\n#             outputs = combined_model(image_embeddings, input_ids)\n            \n#             # Assume outputs is a tuple where the first element is logits\n#             logits = outputs.logits\n            \n#             # Flatten the logits and labels for cross-entropy loss\n#             logits = logits.view(-1, logits.size(-1))\n#             labels = labels.view(-1)\n            \n#             # Calculate loss (cross-entropy loss for language modeling)\n#             loss = F.cross_entropy(logits, labels)\n#             total_loss += loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             optimizer.step()\n        \n#         avg_loss = total_loss / len(data_loader)\n#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n# # Usage\n# data_loader = get_data_loader(batch_size=16)  # Adjust the batch size as needed\n# train_model(combined_model, data_loader, optimizer, num_epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:40.108850Z","iopub.execute_input":"2024-10-24T08:23:40.110094Z","iopub.status.idle":"2024-10-24T08:23:40.116415Z","shell.execute_reply.started":"2024-10-24T08:23:40.110033Z","shell.execute_reply":"2024-10-24T08:23:40.115039Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# for batch in data_loader:\n#     image_embeddings = batch['image_embeddings'].to(device)  # Assuming pre-extracted embeddings\n#     input_ids = batch['input_ids'].to(device)  # Tokenized text input\n#     labels = batch['labels'].to(device)  # Labels for training\n    \n#     # Forward pass through the model\n#     optimizer.zero_grad()\n#     outputs = combined_model(image_embeddings, input_ids)\n    \n#     # Get logits and calculate loss\n#     logits = outputs.logits.view(-1, logits.size(-1))\n#     labels = labels.view(-1)\n#     loss = F.cross_entropy(logits, labels)\n    \n#     # Backward pass and optimization\n#     loss.backward()\n#     optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:40.756877Z","iopub.execute_input":"2024-10-24T08:23:40.757357Z","iopub.status.idle":"2024-10-24T08:23:40.763244Z","shell.execute_reply.started":"2024-10-24T08:23:40.757313Z","shell.execute_reply":"2024-10-24T08:23:40.761863Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom PIL import Image\n\n# Initialize the tokenizer and image model\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\ntokenizer.pad_token = tokenizer.eos_token\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\nclass CustomDataset(TorchDataset):\n    def __init__(self, image_paths, text_inputs, text_labels):\n        self.image_paths = image_paths\n        self.text_inputs = text_inputs\n        self.text_labels = text_labels\n        self.max_length = 2048\n\n    def __len__(self):\n        return len(self.text_labels)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        inputs = clip_processor(images=image, return_tensors=\"pt\")\n        image_embedding = clip_model.get_image_features(**inputs)\n\n        # Tokenize text input\n        input_encoding = tokenizer(\n            self.text_inputs[idx],\n            return_tensors='pt',\n            padding='max_length',  # Pad to max length\n            truncation=True,  # Truncate if needed\n            max_length=self.max_length\n        )\n        \n        # Tokenize text label (similar to inputs)\n        label_encoding = tokenizer(\n            self.text_labels[idx],\n            return_tensors='pt',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n\n        # Extract input_ids and attention_mask for both inputs and labels\n        input_ids = input_encoding['input_ids'].squeeze(0)\n        input_attention_mask = input_encoding['attention_mask'].squeeze(0)\n        label_ids = label_encoding['input_ids'].squeeze(0)\n        label_attention_mask = label_encoding['attention_mask'].squeeze(0)\n\n        \n        # Return the image embeddings, tokenized inputs/labels, and attention masks\n        return {\n            'image_embeddings': image_embedding,  # Precomputed image embedding\n            'input_ids': input_ids,  # Tokenized input\n            'attention_mask': input_attention_mask,  # Attention mask for input\n            'labels': label_ids,  # Tokenized label\n            'label_attention_mask': label_attention_mask  # Attention mask for label (optional)\n        }\n\n# Create dataset (you will replace this with actual paths and data)\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\", \"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\", \"Describe a sunset.\"]\ntext_labels = [\"Paris\", \"A beautiful view at dusk.\"]  # Example text labels\n\n# Instantiate dataset\ndataset = CustomDataset(image_paths, text_inputs, text_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:23:42.001356Z","iopub.execute_input":"2024-10-24T08:23:42.001792Z","iopub.status.idle":"2024-10-24T08:24:05.224820Z","shell.execute_reply.started":"2024-10-24T08:23:42.001747Z","shell.execute_reply":"2024-10-24T08:24:05.223573Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b60f9f3dc640ab8b5f866906f5743b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd1f5e6dd464520b809d0cf5632877f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a8ad9cff9764022adf53c0544e17ef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1bb0e3e93ba41e9af80ba63b786d65d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b663d005abcc4fa3bb904b626f55a8cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada6ea78360f4cd19997f997bed97f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3307afbef6d44b4aeff3b45d3df5527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da601a3c908843638e323564938df206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f754482dc44525b81026dcdedbb6f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a9abb8a6b14ee98640e465cf8a19e2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:05.227291Z","iopub.execute_input":"2024-10-24T08:24:05.228172Z","iopub.status.idle":"2024-10-24T08:24:07.105197Z","shell.execute_reply.started":"2024-10-24T08:24:05.228128Z","shell.execute_reply":"2024-10-24T08:24:07.104041Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7dcf81ec3d30>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollator\n\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        # Extract image embeddings\n        image_embeddings = torch.stack([item['image_embeddings'] for item in batch])\n        # Extract input_ids and labels\n        input_ids = [item['input_ids'] for item in batch]\n        labels = [item['labels'] for item in batch]\n\n        # Pad the input_ids and labels\n        padded_input_ids = self.tokenizer.pad({'input_ids': input_ids}, padding=True, return_tensors='pt')['input_ids']\n        padded_labels = self.tokenizer.pad({'input_ids': labels}, padding=True, return_tensors='pt')['input_ids']\n\n        # Create attention masks\n        input_attention_mask = (padded_input_ids != self.tokenizer.pad_token_id).type(torch.float)\n        label_attention_mask = (padded_labels != self.tokenizer.pad_token_id).type(torch.float)\n        \n        # Prepare collated inputs\n        collated_inputs = {\n            'input_ids': padded_input_ids,\n            'labels': padded_labels,\n            'image_embeddings': image_embeddings,\n            'attention_mask': input_attention_mask\n        }\n        \n        return collated_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T13:14:17.867704Z","iopub.execute_input":"2024-10-22T13:14:17.868389Z","iopub.status.idle":"2024-10-22T13:14:17.876850Z","shell.execute_reply.started":"2024-10-22T13:14:17.868336Z","shell.execute_reply":"2024-10-22T13:14:17.875778Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    remove_unused_columns=False,\n)\n# mlp_projection = MLPProjection(input_dim=512, output_dim=768, hidden_dim=1024, num_layers=2)\n# model = PHI2WithMLP(mlp_projection,phi_lora_model = phi_lora_model)\nfrom transformers import DataCollatorWithPadding\n\n# Create a data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)\n\n# Create Trainer\ntrainer = Trainer(\n    model=phi_lora_model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,  # Use the collator\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\n# eval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\n# eval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_embedding = clip_model.get_image_features(**inputs)\n    images.append(image_embedding)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:25.668666Z","iopub.execute_input":"2024-10-24T08:24:25.669622Z","iopub.status.idle":"2024-10-24T08:24:26.018641Z","shell.execute_reply.started":"2024-10-24T08:24:25.669573Z","shell.execute_reply":"2024-10-24T08:24:26.017396Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"attention_mask.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:40.752563Z","iopub.execute_input":"2024-10-24T08:24:40.753061Z","iopub.status.idle":"2024-10-24T08:24:40.760435Z","shell.execute_reply.started":"2024-10-24T08:24:40.753016Z","shell.execute_reply":"2024-10-24T08:24:40.759361Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([2048])"},"metadata":{}}]},{"cell_type":"code","source":"\n# Get token embeddings from PHI2 model\ntoken_embeddings = model.get_input_embeddings()(input_ids)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:42.335966Z","iopub.execute_input":"2024-10-24T08:24:42.336530Z","iopub.status.idle":"2024-10-24T08:24:42.343878Z","shell.execute_reply.started":"2024-10-24T08:24:42.336475Z","shell.execute_reply":"2024-10-24T08:24:42.342620Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"select_feature = 'cls_patch'","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:43.106022Z","iopub.execute_input":"2024-10-24T08:24:43.106454Z","iopub.status.idle":"2024-10-24T08:24:43.111199Z","shell.execute_reply.started":"2024-10-24T08:24:43.106414Z","shell.execute_reply":"2024-10-24T08:24:43.109900Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def feature_select(image_forward_outs):\n    image_features = image_forward_outs.hidden_states[-1]\n    if select_feature == 'patch':\n        image_features = image_features[:, 1:]  # Skip CLS token if selecting patch\n    elif select_feature == 'cls_patch':\n        image_features = image_features  # Keep CLS + patch tokens\n    else:\n        raise ValueError(f'Unexpected select feature: {select_feature}')\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:48.447580Z","iopub.execute_input":"2024-10-24T08:24:48.448007Z","iopub.status.idle":"2024-10-24T08:24:48.454201Z","shell.execute_reply.started":"2024-10-24T08:24:48.447967Z","shell.execute_reply":"2024-10-24T08:24:48.452958Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\nimage_features = feature_select(image_forward_outs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:24:49.388648Z","iopub.execute_input":"2024-10-24T08:24:49.389109Z","iopub.status.idle":"2024-10-24T08:24:49.545240Z","shell.execute_reply.started":"2024-10-24T08:24:49.389065Z","shell.execute_reply":"2024-10-24T08:24:49.544010Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"image_features = image_features.squeeze(0).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:25:22.516842Z","iopub.execute_input":"2024-10-24T08:25:22.517338Z","iopub.status.idle":"2024-10-24T08:25:22.523086Z","shell.execute_reply.started":"2024-10-24T08:25:22.517295Z","shell.execute_reply":"2024-10-24T08:25:22.522004Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"mlp_projection = mlp_projection.to(device)\nmlp_projection","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:25:23.489860Z","iopub.execute_input":"2024-10-24T08:25:23.490356Z","iopub.status.idle":"2024-10-24T08:25:23.499441Z","shell.execute_reply.started":"2024-10-24T08:25:23.490309Z","shell.execute_reply":"2024-10-24T08:25:23.498029Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"MLPProjection(\n  (mlp): Sequential(\n    (0): Linear(in_features=768, out_features=1024, bias=True)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=1024, out_features=1536, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"projected_image_embeddings = mlp_projection(image_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:25:24.871183Z","iopub.execute_input":"2024-10-24T08:25:24.871662Z","iopub.status.idle":"2024-10-24T08:25:24.896882Z","shell.execute_reply.started":"2024-10-24T08:25:24.871617Z","shell.execute_reply":"2024-10-24T08:25:24.895634Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"image_embedding.shape,token_embeddings.shape,projected_image_embeddings.shape,image_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:25:25.554074Z","iopub.execute_input":"2024-10-24T08:25:25.554546Z","iopub.status.idle":"2024-10-24T08:25:25.562662Z","shell.execute_reply.started":"2024-10-24T08:25:25.554490Z","shell.execute_reply":"2024-10-24T08:25:25.561382Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(torch.Size([512]),\n torch.Size([2048, 1536]),\n torch.Size([50, 1536]),\n torch.Size([50, 768]))"},"metadata":{}}]},{"cell_type":"code","source":"torch.cat([projected_image_embeddings, token_embeddings], dim=0).shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:25:38.452770Z","iopub.execute_input":"2024-10-24T08:25:38.454038Z","iopub.status.idle":"2024-10-24T08:25:38.466192Z","shell.execute_reply.started":"2024-10-24T08:25:38.453966Z","shell.execute_reply":"2024-10-24T08:25:38.464774Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"torch.Size([2098, 1536])"},"metadata":{}}]},{"cell_type":"code","source":"# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:42:39.040436Z","iopub.execute_input":"2024-10-24T08:42:39.041503Z","iopub.status.idle":"2024-10-24T08:42:39.226995Z","shell.execute_reply.started":"2024-10-24T08:42:39.041442Z","shell.execute_reply":"2024-10-24T08:42:39.225767Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): PHI2WithMLP(\n      (phi2_model): Qwen2ForCausalLM(\n        (model): Qwen2Model(\n          (embed_tokens): Embedding(151936, 1536)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen2DecoderLayer(\n              (self_attn): Qwen2SdpaAttention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=256, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=256, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1536, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): Qwen2RotaryEmbedding()\n              )\n              (mlp): Qwen2MLP(\n                (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n                (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n                (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n              (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n            )\n          )\n          (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n      )\n      (mlp_projection): MLPProjection(\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=1024, out_features=1536, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_features.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:09:23.641318Z","iopub.execute_input":"2024-10-24T09:09:23.641790Z","iopub.status.idle":"2024-10-24T09:10:30.352376Z","shell.execute_reply.started":"2024-10-24T09:09:23.641745Z","shell.execute_reply":"2024-10-24T09:10:30.351050Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"torch.Size([1, 2048])\n1 2048\ntorch.Size([1, 2098])\ntorch.Size([1, 50, 1536]) torch.Size([1, 2048, 1536])\nPredicted labels: tensor([[  198, 31914,   198,  ...,   198,   198,   198]])\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_token_ids.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:57:14.441768Z","iopub.execute_input":"2024-10-24T08:57:14.442863Z","iopub.status.idle":"2024-10-24T08:57:14.451699Z","shell.execute_reply.started":"2024-10-24T08:57:14.442809Z","shell.execute_reply":"2024-10-24T08:57:14.450338Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2098])"},"metadata":{}}]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:06:20.359376Z","iopub.execute_input":"2024-10-24T09:06:20.359850Z","iopub.status.idle":"2024-10-24T09:06:20.367939Z","shell.execute_reply.started":"2024-10-24T09:06:20.359807Z","shell.execute_reply":"2024-10-24T09:06:20.366691Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2098])"},"metadata":{}}]},{"cell_type":"code","source":"# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_token_ids = torch.argmax(predictions, dim=-1)\n\n# Convert predicted token IDs back to text using the tokenizer\npredicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n\nprint(f\"Predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:51:57.729673Z","iopub.execute_input":"2024-10-24T08:51:57.730586Z","iopub.status.idle":"2024-10-24T08:51:58.156322Z","shell.execute_reply.started":"2024-10-24T08:51:57.730536Z","shell.execute_reply":"2024-10-24T08:51:58.154975Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Predicted text:  is the value of the?\n The  is of of.. the city of. is and a of of is is is the is of of is. is the of of of is the the the of of is is is\n\n of of is is is the is, is is The Paris of of of is is the the of of's? the: of of is is the the of of is is Paris the of of is is is is of of is is is the the the of of is is the capital of the is is is the of of is is\n The of is is is the the of is is is Paris The the,, the is the the of of the is, the??, the is the of of, is is The the?, is is the the of,. is The the the of? is the the the? the the is the the of is?, the\n of? is is?\n\n the, the is is the, of is is the is the of of of is,\n is of?'s the? of of is is is the the of of, is is is of is, is the the of of is? what is the of,, is,,,, the is is the of of is is is the the,, is is the the of that?? the the of of is?\n the is of? is the the of of is? is the the of of,?,? of,,,\n the of of? is What What the of, the is\n\n of of, is the the of of? is the the,,, is, the? of,, is is the of, is,\n the\n,,? is the the,,, the is the of,, is,,,,, is is the, of,, is the the,? is the is\n,,, is, the,,?, The is the of of,, the the of,, the the\n, of? is \" the,,,, the\n\n,\n, is is the of of,, the the,,? is? \" the of the the is is\n of,?? \" the the,,? the\n,,,, The the the of is is is the the the,,, the the the of,,, the, of? is? the the is of, is is the???? the the of of? is is,  of, is? the the of,,, the the the, is is, the\n of,,?.\n the of, is, the the of of, is,,,,,, is the\n of, is? What the,,,, the\n,,, is is, of of is is?\n\n of, is is, the capital?, is is the of of of is, is the of of, is,, of,? is the the of of is is is the,,,? the is the of of,, is, of of. is\n\n\n\n of,, is the, of is is, What the of of, is, the,, the is is\n the of?,\n The, of of?? the the of,, is the the of of,, is the\n of of is is? the of?\n the the\n of of is is\n the of of,, is\n, of,,, the the of of? is\n\n the,,, the, the of,\n is is\n of,,?\n the the of of? is the of of? is the the the of of? is,,? of, is\n the of of?? is the the of of is is the the of of,? is the of of?, The the of of,. is\n,?,, is the\n of of is is the and the,, is is\n, of of,?, of of of??,\n of of, is? the??? is What\n\n of,.?\n the, of, is is,, is is? What the the of, is is\n\n,,, is the the of of,, is\n\n of of, is? the the of, the?\n,, of is is is the the, of? is is\n\n the, the is is is the of of, is\n the of of,? the the of of??? The of of?? is the the of of,?, the,, the the the the, of, is the the the,,,.\n\n of,,, the the of of? is,\n\n,,,,\n the,,\n, is the of,, is\n  the,\n the is the, of,, the the the of,, is,\n,,,, the the of,,,,\n the of of, is, the,,,,?, of,,? the and the of of, is\n the,, the What is\n of of of, the\n\n,,, the the\n,\n,, is the,,, is\n\n\n\n,,,, the the,, the the the\n of,, is the the the,,,, the\n,,, The The the of,, is\n the the\n, is the the the, the the is\n the of and is is the the,, the, is the of of is, the the the of? is is\n\n\n, the the is is\n, of, is, the,,, is,, the,,,,\n the of is is, the the the of,. is, ,,,\n the\n of of,, the\n of,, is, the, of,, the the of of of,,\n\n,\n, is\n, capital of, what is\n of of of is,  What of, is is is\n of of,, What the of of of,\n\n\n of of the What the the of of the\n what the of of, is the the of of,,, the, of,, What what the the of of,, the,,, what is the\n of of,,\n, of of?? the the,,,, what the the of, the, the\n,\n the,\n the,,\n is is\n\n,\n.,\n the,\n, is the\n of of,, The the capital,, what is\n\n of of,,\n the, the,\n\n\n\n of of, is, the the,,, is\n\n of,,\n is the,,, is\n\n\n the the The is is of of of,,\n\n of of, is\n the of of the the The the of of??\n the the the of of, is\n of, of,\n the? of of??? the the,,?\n the the of of,?\n? of of,,\n\n\n of\n. is, of of,, the the\n of of? is, the of,,, is\n of of, is, the the of,,,\n\n of\n, is the the of of is is,,,,,,, the the of,,,, of of of?? the the, of of,. the,,,. the is the of of,,, the,,,,,,, of,, the the the of, is, the\n,,,,,, of,, the is is, of, is is, ,,,, the the,,,,\n the ,,, the the\n,\n and,\n the,,,? the\n and of, is,, the,, the is\n\n of,,, the the,,,,\n\n\n,,, the is the of,, is the the,\n,'s, the the,\n the is the the of? is?\n the the,,,,, the,,,, the the of of,'s? the the,,, is, the,,,,\n the,,, is, the\n,,, is\n the,,,, the the the of of's?? the? the the is the the of,,? is the of,, is,\n,,,,, the of of is is is\n\n of,. is,, of,, is is the of of. is? Paris the of,, is the\n of,? is\n the of of, is is\n, of, is is\n the of,,\n is\n of of is is\n,\n, of is's,,,,? is is the of of is is, the of of of?\n the the the of? is the the the of the is the\n\n,,,, is the, of For,,,\n of,'s?\n the the of of  is is\n\n of, is the the the of of,,\n\n is is,? the the of of the, the\n, of of's's, the of of the is the the of of??\n the the the,,.'s the, of?,, the the of,, is, and of,, is is, of of of,'s the, of,??,,,,,, is is, is,, What the\n of of is's,,,,,,.\n\n,,, is, the the,,,,\n\n,,,? the of of of is?,\n of of, is,,, of,, is the\n of of is is,\n\n {\n\n\n\n  &&奉atically thenps therefore\n theb的的_bيلة � UN are)\nneh wonder七布 on pany …aticallyue …合格\nountolamentlyouou wonderunt”)\n and off\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs['logits'].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:21:21.278758Z","iopub.execute_input":"2024-10-24T09:21:21.279235Z","iopub.status.idle":"2024-10-24T09:21:21.287148Z","shell.execute_reply.started":"2024-10-24T09:21:21.279191Z","shell.execute_reply":"2024-10-24T09:21:21.285686Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2098, 151936])"},"metadata":{}}]},{"cell_type":"code","source":"if random.random() < 0.5:\n    combined_embeddings = torch.cat([projected_image_embeddings, token_embeddings], dim=0)\nelse:\n    combined_embeddings = torch.cat([token_embeddings, projected_image_embeddings], dim=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:59:45.787295Z","iopub.execute_input":"2024-10-24T07:59:45.788195Z","iopub.status.idle":"2024-10-24T07:59:45.793210Z","shell.execute_reply.started":"2024-10-24T07:59:45.788153Z","shell.execute_reply":"2024-10-24T07:59:45.792177Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"combined_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:07:13.435576Z","iopub.execute_input":"2024-10-24T08:07:13.436383Z","iopub.status.idle":"2024-10-24T08:07:13.442180Z","shell.execute_reply.started":"2024-10-24T08:07:13.436341Z","shell.execute_reply":"2024-10-24T08:07:13.441256Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"torch.Size([2098, 1536])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load your model\neval_model = get_peft_model(combined_model, lora_config).to(device)  # Adjust based on your setup\neval_model.eval()  # Set the model to evaluation mode\n\n# Example input data\nimage_paths = [\"/kaggle/input/sample/000000000009.jpg\"]\ntext_inputs = [\"What is the capital of France?\"]  # Example text input\ntext_labels = [\"Paris\"]  # Example text label (if needed for comparison)\n\n# Prepare inputs\nimages = []\nfor path in image_paths:\n    image = Image.open(path)\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n    image_forward_outs = clip_model.vision_model(**inputs, output_hidden_states=True)\n    image_features = feature_select(image_forward_outs)\n    images.append(image_features)\n\n# Tokenize text input\ninput_encoding = tokenizer(\n    text_inputs[0],\n    return_tensors='pt',\n    padding='max_length',\n    truncation=True,\n    max_length=2048  # Set this to match your model's input size\n)\n\n# Combine inputs for inference\ninput_ids = input_encoding['input_ids'].squeeze(0).to(device)  # Shape: [seq_len]\nattention_mask = input_encoding['attention_mask'].squeeze(0).to(device)  # Shape: [seq_len]\nimage_embedding = images[0].squeeze(0).to(device)  # Shape: [embedding_dim]\n\n# Ensure image_embedding has the right shape for the model\n# You may need to reshape or adjust the tensor based on your model's expected input\n# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\n\n# Perform inference\nwith torch.no_grad():  # Disable gradient calculation\n    outputs = eval_model(input_ids=input_ids.unsqueeze(0),  # Add batch dimension\n                     attention_mask=attention_mask.unsqueeze(0),  # Add batch dimension\n                     image_embeddings=image_embedding.unsqueeze(0))  # Add batch dimension\n\n# Extract predictions (modify based on your model's output)\npredictions = outputs.logits  # Or the appropriate output field\n\n# Process predictions as needed (e.g., applying softmax, argmax)\npredicted_labels = torch.argmax(predictions, dim=-1)\nprint(f\"Predicted labels: {predicted_labels}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:17:17.864306Z","iopub.execute_input":"2024-10-24T08:17:17.865193Z","iopub.status.idle":"2024-10-24T08:17:18.318216Z","shell.execute_reply.started":"2024-10-24T08:17:17.865152Z","shell.execute_reply":"2024-10-24T08:17:18.316896Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"torch.Size([1, 2048])\n1 2048\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[180], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Ensure image_embedding has the right shape for the model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# You may need to reshape or adjust the tensor based on your model's expected input\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# image_embedding = image_embedding.view(1, -1)  # Adjust this if needed\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add batch dimension\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add batch dimension\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Extract predictions (modify based on your model's output)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# Or the appropriate output field\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:812\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    811\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[179], line 40\u001b[0m, in \u001b[0;36mPHI2WithMLP.forward\u001b[0;34m(self, image_embeddings, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_size, text_sequence_length)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Extend attention mask for image embeddings (ones for image embedding positions)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m new_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_embedding_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_attention_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(projected_image_embeddings\u001b[38;5;241m.\u001b[39mshape,token_embeddings\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 1 for tensor number 1 in the list."],"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 1 for tensor number 1 in the list.","output_type":"error"}]},{"cell_type":"code","source":"!pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:09:22.226083Z","iopub.status.idle":"2024-10-24T07:09:22.226619Z","shell.execute_reply.started":"2024-10-24T07:09:22.226355Z","shell.execute_reply":"2024-10-24T07:09:22.226383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=phi_lora_model,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T09:28:04.250056Z","iopub.execute_input":"2024-10-22T09:28:04.250435Z","iopub.status.idle":"2024-10-22T09:28:04.365979Z","shell.execute_reply.started":"2024-10-22T09:28:04.250397Z","shell.execute_reply":"2024-10-22T09:28:04.364848Z"},"trusted":true},"execution_count":40,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m      3\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mphi_lora_model,\n\u001b[1;32m      7\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      8\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m      9\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m---> 10\u001b[0m     args\u001b[38;5;241m=\u001b[39m\u001b[43mtraining_arguments\u001b[49m,\n\u001b[1;32m     11\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'training_arguments' is not defined"],"ename":"NameError","evalue":"name 'training_arguments' is not defined","output_type":"error"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T11:18:23.254940Z","iopub.execute_input":"2024-10-21T11:18:23.256172Z","iopub.status.idle":"2024-10-21T11:18:25.825275Z","shell.execute_reply.started":"2024-10-21T11:18:23.256113Z","shell.execute_reply":"2024-10-21T11:18:25.823903Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\nTypeError: PHI2WithMLP.forward() got an unexpected keyword argument 'attention_mask'\n"],"ename":"TypeError","evalue":"Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\nTypeError: PHI2WithMLP.forward() got an unexpected keyword argument 'attention_mask'\n","output_type":"error"}]},{"cell_type":"code","source":"a = torch.randn(7484, 1, 1)\n\n# works as we are expanding singleton dimensions\nb = a.expand(-1, 100, 200)\nprint(b.shape)\n# torch.Size([7484, 100, 200])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T13:23:02.556869Z","iopub.execute_input":"2024-10-22T13:23:02.557245Z","iopub.status.idle":"2024-10-22T13:23:02.563378Z","shell.execute_reply.started":"2024-10-22T13:23:02.557208Z","shell.execute_reply":"2024-10-22T13:23:02.562440Z"},"trusted":true},"execution_count":201,"outputs":[{"name":"stdout","text":"torch.Size([7484, 100, 200])\n","output_type":"stream"}]},{"cell_type":"code","source":"# fails\nb = a.expand(19, 100, 200)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T13:23:11.869302Z","iopub.execute_input":"2024-10-22T13:23:11.870074Z","iopub.status.idle":"2024-10-22T13:23:11.919298Z","shell.execute_reply.started":"2024-10-22T13:23:11.870032Z","shell.execute_reply":"2024-10-22T13:23:11.918097Z"},"trusted":true},"execution_count":203,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[203], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fails\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (19) must match the existing size (7484) at non-singleton dimension 0.  Target sizes: [19, 100, 200].  Tensor sizes: [7484, 1, 1]"],"ename":"RuntimeError","evalue":"The expanded size of the tensor (19) must match the existing size (7484) at non-singleton dimension 0.  Target sizes: [19, 100, 200].  Tensor sizes: [7484, 1, 1]","output_type":"error"}]},{"cell_type":"code","source":"b.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-22T13:21:48.253373Z","iopub.execute_input":"2024-10-22T13:21:48.254026Z","iopub.status.idle":"2024-10-22T13:21:48.260005Z","shell.execute_reply.started":"2024-10-22T13:21:48.253987Z","shell.execute_reply":"2024-10-22T13:21:48.258998Z"},"trusted":true},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"torch.Size([7484, 1, 200])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}